{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1Khwg0tu7g8B",
        "CpG-X8cN7efL",
        "YU1TsqVo79nZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# mnist 호출"
      ],
      "metadata": {
        "id": "1Khwg0tu7g8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('You should use Python 3.x')\n",
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from collections import OrderedDict\n",
        "\n",
        "# 원-핫 인코딩 함수\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "    return T\n",
        "\n",
        "def load_fashion_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    (x_train, t_train), (x_test, t_test) = fashion_mnist.load_data()\n",
        "\n",
        "    if normalize:\n",
        "        x_train = x_train.astype(np.float32) / 255.0\n",
        "        x_test = x_test.astype(np.float32) / 255.0\n",
        "\n",
        "    if one_hot_label:\n",
        "        t_train = _change_one_hot_label(t_train)\n",
        "        t_test = _change_one_hot_label(t_test)\n",
        "\n",
        "    if flatten:\n",
        "        x_train = x_train.reshape(-1, 28*28)\n",
        "        x_test = x_test.reshape(-1, 28*28)\n",
        "    else:\n",
        "        x_train = x_train.reshape(-1, 1, 28, 28)\n",
        "        x_test = x_test.reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (x_train, t_train), (x_test, t_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    (x_train, t_train), (x_test, t_test) = load_fashion_mnist()\n",
        "    print(f\"x_train shape: {x_train.shape}\")\n",
        "    print(f\"t_train shape: {t_train.shape}\")\n",
        "    print(f\"x_test shape: {x_test.shape}\")\n",
        "    print(f\"t_test shape: {t_test.shape}\")\n"
      ],
      "metadata": {
        "id": "193HcsSk7TJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2ad523-370c-47b9-e3c2-91a7b0f600f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 784)\n",
            "t_train shape: (60000,)\n",
            "x_test shape: (10000, 784)\n",
            "t_test shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_fashion_mnist(flatten=False)\n"
      ],
      "metadata": {
        "id": "wBNJFwpqEHdD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "CpG-X8cN7efL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SGD:\n",
        "\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "\n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n",
        "\n",
        "def he_initialization(shape):\n",
        "    stddev = np.sqrt(2.0 / np.prod(shape[:-1]))\n",
        "    return stddev * np.random.randn(*shape)"
      ],
      "metadata": {
        "id": "KKzMcleY7dWk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer"
      ],
      "metadata": {
        "id": "YU1TsqVo79nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "  # coding: utf-8\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n"
      ],
      "metadata": {
        "id": "inkRWPa378rH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기본 틀"
      ],
      "metadata": {
        "id": "v7xOC3GLFMYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "      def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01},\n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))"
      ],
      "metadata": {
        "id": "jHBOGkMFF1BS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet2:\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param_1={'filter_num':16, 'filter_size':2, 'pad':0, 'stride':1},\n",
        "                 conv_param_2={'filter_num':32, 'filter_size':2, 'pad':0, 'stride':1},\n",
        "                 hidden_size=256, output_size=10, weight_init_std=0.01):\n",
        "        filter_num1 = conv_param_1['filter_num']\n",
        "        filter_size1 = conv_param_1['filter_size']\n",
        "        filter_pad1 = conv_param_1['pad']\n",
        "        filter_stride1 = conv_param_1['stride']\n",
        "        filter_num2 = conv_param_2['filter_num']\n",
        "        filter_size2 = conv_param_2['filter_size']\n",
        "        filter_pad2 = conv_param_2['pad']\n",
        "        filter_stride2 = conv_param_2['stride']\n",
        "\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size1 = (input_size - filter_size1 + 2*filter_pad1) / filter_stride1 + 1\n",
        "        conv_output_size2 = (conv_output_size1 - filter_size2 + 2*filter_pad2) / filter_stride2 + 1\n",
        "        pool_output_size = int(filter_num2 * conv_output_size2 * conv_output_size2)\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num1, input_dim[0], filter_size1, filter_size1)\n",
        "        self.params['b1'] = np.zeros(filter_num1)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num2, filter_num1, filter_size2, filter_size2)\n",
        "        self.params['b2'] = np.zeros(filter_num2)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b3'] = np.zeros(hidden_size)\n",
        "        self.params['W4'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b4'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param_1['stride'], conv_param_1['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'],\n",
        "                                           conv_param_2['stride'], conv_param_2['pad'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine1'] = Affine(self.params['W3'], self.params['b3'])\n",
        "        self.layers['Relu3'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W4'], self.params['b4'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3, 4):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W4'], grads['b4'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "metadata": {
        "id": "eoj4yo9fTEGq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet2(input_dim=(1,28,28),\n",
        "                        conv_param_1={'filter_num': 16, 'filter_size': 2, 'pad': 0, 'stride': 1},\n",
        "                        conv_param_2={'filter_num': 32, 'filter_size': 2, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=256, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HmSG9rZMTN50",
        "outputId": "c0025567-9194-4ab4-b28c-3936c3bf0074"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.07275347682060111\n",
            "train loss:0.09285095640695636\n",
            "train loss:0.08431769729744301\n",
            "train loss:0.08284066020845517\n",
            "train loss:0.07670160907388807\n",
            "train loss:0.1045671537683101\n",
            "train loss:0.0692053659895673\n",
            "train loss:0.0652782272788405\n",
            "train loss:0.06572040689562965\n",
            "train loss:0.04661610737339462\n",
            "train loss:0.04097602455958233\n",
            "train loss:0.06192533949125239\n",
            "train loss:0.05008490166111893\n",
            "train loss:0.05426651126530257\n",
            "train loss:0.07690140035326104\n",
            "train loss:0.042744209050869744\n",
            "train loss:0.08867114455838772\n",
            "train loss:0.040848546572496175\n",
            "train loss:0.08758981587894347\n",
            "train loss:0.02840405667480641\n",
            "train loss:0.10787211297954126\n",
            "train loss:0.04992168410116669\n",
            "train loss:0.07152483004915741\n",
            "train loss:0.12458171565236274\n",
            "train loss:0.13602698040971528\n",
            "train loss:0.09836099266724543\n",
            "train loss:0.0458740725918743\n",
            "train loss:0.0560740807837072\n",
            "train loss:0.05926483396194782\n",
            "train loss:0.07315595360872405\n",
            "train loss:0.047086178220684685\n",
            "train loss:0.06757314559827825\n",
            "train loss:0.035042684009153995\n",
            "train loss:0.08143773246295663\n",
            "train loss:0.03071166741844641\n",
            "train loss:0.1369894077956803\n",
            "train loss:0.10767558631930615\n",
            "train loss:0.03156036420702255\n",
            "train loss:0.03972943275369283\n",
            "train loss:0.07762804706438009\n",
            "train loss:0.12613816854690824\n",
            "train loss:0.11681923311317349\n",
            "train loss:0.13367052995514667\n",
            "train loss:0.09080334815572608\n",
            "train loss:0.0907281882899138\n",
            "train loss:0.04688633996772114\n",
            "train loss:0.15615145486174367\n",
            "train loss:0.05333846800027715\n",
            "train loss:0.08726029288408486\n",
            "train loss:0.20732736853550146\n",
            "train loss:0.0627533798760927\n",
            "train loss:0.07478443278120925\n",
            "train loss:0.05340788188111117\n",
            "train loss:0.08999200614522054\n",
            "train loss:0.043099655713953224\n",
            "train loss:0.10844707577943971\n",
            "train loss:0.07369750388122441\n",
            "train loss:0.09307234935502771\n",
            "train loss:0.12978846270749161\n",
            "train loss:0.07186935479780338\n",
            "train loss:0.09804137221474787\n",
            "train loss:0.045986662345448376\n",
            "train loss:0.09164162605633482\n",
            "train loss:0.18148225885292843\n",
            "train loss:0.06258578134197473\n",
            "train loss:0.04977917280255517\n",
            "train loss:0.10473509888575813\n",
            "train loss:0.08193759356176851\n",
            "train loss:0.04245923785716396\n",
            "train loss:0.06890103762121265\n",
            "train loss:0.06270020949085724\n",
            "train loss:0.13798676893608813\n",
            "train loss:0.07207185512417523\n",
            "train loss:0.08959193468780823\n",
            "train loss:0.05682699663199121\n",
            "train loss:0.14125161615262102\n",
            "train loss:0.07651755227450656\n",
            "train loss:0.053915993817998144\n",
            "train loss:0.04194632538428848\n",
            "train loss:0.03333553731933524\n",
            "train loss:0.03696967926811872\n",
            "train loss:0.08996609518537242\n",
            "train loss:0.0638502634248745\n",
            "train loss:0.04952299406752436\n",
            "train loss:0.06355549940801432\n",
            "train loss:0.027429915544470468\n",
            "train loss:0.11189996977039028\n",
            "train loss:0.12051524673745381\n",
            "train loss:0.050437311245989026\n",
            "train loss:0.04857808515373911\n",
            "train loss:0.04509096410168678\n",
            "train loss:0.12261849676282814\n",
            "train loss:0.12496930712709244\n",
            "train loss:0.05546924614769921\n",
            "train loss:0.020630591305218795\n",
            "train loss:0.053010620593922725\n",
            "train loss:0.0376441397707363\n",
            "train loss:0.05687728678699795\n",
            "train loss:0.057606093904378944\n",
            "train loss:0.11276960756581657\n",
            "train loss:0.11757071766508587\n",
            "train loss:0.03632083978983234\n",
            "train loss:0.03538105252243955\n",
            "train loss:0.0325599742292578\n",
            "train loss:0.01738149610102012\n",
            "train loss:0.06282467021357702\n",
            "train loss:0.037336021511302456\n",
            "train loss:0.05242552249758255\n",
            "train loss:0.047375510182155266\n",
            "train loss:0.0665447199692877\n",
            "train loss:0.04697446500886288\n",
            "train loss:0.06246857964767598\n",
            "train loss:0.03686599196469908\n",
            "train loss:0.0556106345826839\n",
            "train loss:0.03453737716670479\n",
            "train loss:0.031383462775816696\n",
            "train loss:0.09322406625468678\n",
            "train loss:0.10775362303419839\n",
            "train loss:0.05321783297273195\n",
            "train loss:0.07173320328723758\n",
            "train loss:0.033078234496080106\n",
            "train loss:0.07059136527983471\n",
            "train loss:0.08518469019943216\n",
            "train loss:0.10274914513558014\n",
            "train loss:0.06749148234317182\n",
            "train loss:0.02744118220476537\n",
            "train loss:0.09222594679276246\n",
            "train loss:0.09785464929411408\n",
            "train loss:0.08145938634453001\n",
            "train loss:0.04164550485021616\n",
            "train loss:0.16599069014244075\n",
            "train loss:0.12786838479706847\n",
            "train loss:0.1024440283474922\n",
            "train loss:0.12361371657860774\n",
            "train loss:0.054632362279899586\n",
            "train loss:0.057662345784108865\n",
            "train loss:0.0406152420836911\n",
            "train loss:0.04769446757831393\n",
            "train loss:0.051532221064366876\n",
            "train loss:0.08029251652288111\n",
            "train loss:0.08750178389027781\n",
            "train loss:0.05941406692902828\n",
            "train loss:0.022516137158601263\n",
            "train loss:0.07927954193730936\n",
            "train loss:0.03352667220732395\n",
            "train loss:0.07925655189113463\n",
            "train loss:0.10119275154680843\n",
            "train loss:0.030977449619398856\n",
            "train loss:0.10684812984547909\n",
            "train loss:0.04088698397089872\n",
            "train loss:0.16325369540778825\n",
            "train loss:0.03284854759134926\n",
            "train loss:0.08033876484732605\n",
            "train loss:0.09656061895721811\n",
            "train loss:0.14996972912088227\n",
            "train loss:0.10349628106641243\n",
            "train loss:0.15760266411691656\n",
            "train loss:0.05202208427038987\n",
            "train loss:0.04157175679192207\n",
            "train loss:0.022537488496266658\n",
            "train loss:0.15144931848234428\n",
            "train loss:0.12767765906315806\n",
            "train loss:0.05127944597327939\n",
            "train loss:0.09398544015902044\n",
            "train loss:0.031647325169818295\n",
            "train loss:0.039982592930595834\n",
            "train loss:0.07144877735778865\n",
            "train loss:0.08361291093607535\n",
            "train loss:0.08615978360531962\n",
            "train loss:0.04544414559887494\n",
            "train loss:0.08064335531816019\n",
            "train loss:0.08576998586484132\n",
            "train loss:0.041634919051278925\n",
            "train loss:0.12921087883096175\n",
            "train loss:0.05040653480891174\n",
            "train loss:0.0774663550775379\n",
            "train loss:0.06182231301391941\n",
            "train loss:0.031628394662520654\n",
            "train loss:0.10098477533559348\n",
            "train loss:0.0681170426276661\n",
            "train loss:0.160867223808038\n",
            "train loss:0.04678806963800893\n",
            "train loss:0.05594783610561566\n",
            "train loss:0.08967921344198672\n",
            "train loss:0.12503798915163283\n",
            "train loss:0.039514330915694534\n",
            "train loss:0.05962715931751955\n",
            "train loss:0.030897768316660933\n",
            "train loss:0.03429810373214845\n",
            "train loss:0.1064266823712117\n",
            "train loss:0.059294294659773344\n",
            "=== epoch:13, train acc:0.976, test acc:0.901 ===\n",
            "train loss:0.054569261355285094\n",
            "train loss:0.08182362559527201\n",
            "train loss:0.09827101848738794\n",
            "train loss:0.04317869909343844\n",
            "train loss:0.06124015510042044\n",
            "train loss:0.11256376539142762\n",
            "train loss:0.05808616882385647\n",
            "train loss:0.04840301673135885\n",
            "train loss:0.05143108593533187\n",
            "train loss:0.07851121947459778\n",
            "train loss:0.07501955061395454\n",
            "train loss:0.051845007204742465\n",
            "train loss:0.0742929103991416\n",
            "train loss:0.096852485564424\n",
            "train loss:0.08724663992899134\n",
            "train loss:0.06735799310783377\n",
            "train loss:0.03083982180860681\n",
            "train loss:0.07851256476454126\n",
            "train loss:0.08919794236293452\n",
            "train loss:0.07263331034989255\n",
            "train loss:0.06592907275843979\n",
            "train loss:0.09490413055057965\n",
            "train loss:0.03585500300006774\n",
            "train loss:0.08271603488944615\n",
            "train loss:0.10271218469907321\n",
            "train loss:0.06710591181971144\n",
            "train loss:0.03415353811800839\n",
            "train loss:0.04548915033356384\n",
            "train loss:0.07629880998317538\n",
            "train loss:0.031317963528180816\n",
            "train loss:0.03857563757403719\n",
            "train loss:0.13469874037391752\n",
            "train loss:0.0711618396703284\n",
            "train loss:0.07154918603046158\n",
            "train loss:0.023882350712471204\n",
            "train loss:0.1351218371634225\n",
            "train loss:0.1511613242997114\n",
            "train loss:0.12256711928910134\n",
            "train loss:0.09095876756150809\n",
            "train loss:0.03358063750686499\n",
            "train loss:0.07468759930652498\n",
            "train loss:0.04745568487683682\n",
            "train loss:0.0907460827093907\n",
            "train loss:0.033014509658168406\n",
            "train loss:0.07259814743678888\n",
            "train loss:0.06315038107152922\n",
            "train loss:0.14311607241259927\n",
            "train loss:0.026228440810133657\n",
            "train loss:0.07384853306880054\n",
            "train loss:0.07053082099858632\n",
            "train loss:0.08912509481278902\n",
            "train loss:0.04426422922791074\n",
            "train loss:0.06455954165329052\n",
            "train loss:0.10463733081440654\n",
            "train loss:0.07657366247888717\n",
            "train loss:0.059395829210424286\n",
            "train loss:0.10672847369869345\n",
            "train loss:0.0841455146198193\n",
            "train loss:0.1140463931175065\n",
            "train loss:0.16343427472533423\n",
            "train loss:0.03422568351068019\n",
            "train loss:0.07142158262380861\n",
            "train loss:0.09510486233008848\n",
            "train loss:0.06319694598115273\n",
            "train loss:0.0953618783949664\n",
            "train loss:0.08067976590933994\n",
            "train loss:0.06805121460224207\n",
            "train loss:0.08138533609124549\n",
            "train loss:0.044994476985803174\n",
            "train loss:0.03747762131151017\n",
            "train loss:0.07018297173727411\n",
            "train loss:0.04531190028175125\n",
            "train loss:0.05340986731589872\n",
            "train loss:0.07807524008354776\n",
            "train loss:0.03769438118151907\n",
            "train loss:0.08526252241434223\n",
            "train loss:0.04539996114103592\n",
            "train loss:0.03681683906558823\n",
            "train loss:0.1109841111536371\n",
            "train loss:0.06731642226217717\n",
            "train loss:0.04562629109985502\n",
            "train loss:0.04737211579736193\n",
            "train loss:0.03300887362263436\n",
            "train loss:0.037055352356898004\n",
            "train loss:0.07691133079384926\n",
            "train loss:0.1278183770488122\n",
            "train loss:0.027141514853369308\n",
            "train loss:0.01565757188237515\n",
            "train loss:0.03851462308314764\n",
            "train loss:0.051039608188302894\n",
            "train loss:0.057280214254881126\n",
            "train loss:0.15278473514718477\n",
            "train loss:0.036783809552551144\n",
            "train loss:0.05677678542605086\n",
            "train loss:0.1024365261016534\n",
            "train loss:0.044833853100584624\n",
            "train loss:0.1063382469164552\n",
            "train loss:0.06980577892330851\n",
            "train loss:0.10354542316167349\n",
            "train loss:0.08023894565217254\n",
            "train loss:0.05744155044036472\n",
            "train loss:0.08125392400654949\n",
            "train loss:0.023418238046236228\n",
            "train loss:0.06524795460092547\n",
            "train loss:0.05670600283752881\n",
            "train loss:0.06803126011981021\n",
            "train loss:0.02281367706966033\n",
            "train loss:0.02238020715077623\n",
            "train loss:0.052143724600324494\n",
            "train loss:0.07552775752543885\n",
            "train loss:0.051802104853554985\n",
            "train loss:0.06475571364971226\n",
            "train loss:0.04285726287489507\n",
            "train loss:0.05914801864527468\n",
            "train loss:0.0571542654730987\n",
            "train loss:0.07423891465487215\n",
            "train loss:0.10014697193610107\n",
            "train loss:0.03345671122641935\n",
            "train loss:0.04508038255280358\n",
            "train loss:0.052874730025942326\n",
            "train loss:0.022982368519177187\n",
            "train loss:0.04519105696809109\n",
            "train loss:0.06546164271686748\n",
            "train loss:0.06144836424704603\n",
            "train loss:0.12772838857867141\n",
            "train loss:0.055590031514298026\n",
            "train loss:0.04471121867058518\n",
            "train loss:0.06310782954884146\n",
            "train loss:0.06784784070971807\n",
            "train loss:0.06859417742787764\n",
            "train loss:0.11374189784751979\n",
            "train loss:0.05127666966973685\n",
            "train loss:0.07049540520063279\n",
            "train loss:0.13153046071205957\n",
            "train loss:0.10793967287618661\n",
            "train loss:0.05831743211898537\n",
            "train loss:0.08199008751922694\n",
            "train loss:0.17341843529009338\n",
            "train loss:0.07190855281875583\n",
            "train loss:0.07611228284914479\n",
            "train loss:0.13004086888148964\n",
            "train loss:0.08146179811267346\n",
            "train loss:0.06248734059205651\n",
            "train loss:0.03996225614135936\n",
            "train loss:0.05824175989746012\n",
            "train loss:0.05325819246124644\n",
            "train loss:0.06363037489972644\n",
            "train loss:0.08082920757865207\n",
            "train loss:0.05536107306768037\n",
            "train loss:0.06625116151254297\n",
            "train loss:0.07566270559945812\n",
            "train loss:0.04211057454363663\n",
            "train loss:0.09030422995667059\n",
            "train loss:0.11590420483302254\n",
            "train loss:0.048654042781377596\n",
            "train loss:0.034793527883010766\n",
            "train loss:0.040602982039022885\n",
            "train loss:0.06436956347351978\n",
            "train loss:0.0886088436415646\n",
            "train loss:0.08873624508469341\n",
            "train loss:0.08031496164483114\n",
            "train loss:0.06604393865847068\n",
            "train loss:0.03614476027232461\n",
            "train loss:0.045814079762561306\n",
            "train loss:0.05697929738230461\n",
            "train loss:0.04506790495683749\n",
            "train loss:0.08380281668891516\n",
            "train loss:0.16852243796084912\n",
            "train loss:0.04467054734390666\n",
            "train loss:0.11425952756457143\n",
            "train loss:0.15201767539532415\n",
            "train loss:0.05376644397012249\n",
            "train loss:0.15173499933497409\n",
            "train loss:0.11100035789155051\n",
            "train loss:0.12458189668825134\n",
            "train loss:0.05898447786556114\n",
            "train loss:0.08921870560695046\n",
            "train loss:0.03815416722545276\n",
            "train loss:0.04063696348341127\n",
            "train loss:0.062281395512637984\n",
            "train loss:0.09843177855015836\n",
            "train loss:0.05548599071112483\n",
            "train loss:0.06110285783513213\n",
            "train loss:0.13797279308089885\n",
            "train loss:0.05041661743236184\n",
            "train loss:0.03794841790565154\n",
            "train loss:0.046505959709572185\n",
            "train loss:0.04894615386547527\n",
            "train loss:0.043806188910907984\n",
            "train loss:0.1094556668111648\n",
            "train loss:0.09833805720061856\n",
            "train loss:0.09660499981182386\n",
            "train loss:0.07258861569932223\n",
            "train loss:0.07906437776778791\n",
            "train loss:0.04124840812907717\n",
            "train loss:0.0492307144413376\n",
            "train loss:0.06396845613332218\n",
            "train loss:0.11393508605120126\n",
            "train loss:0.05101779768812155\n",
            "train loss:0.0550738726370332\n",
            "train loss:0.04958117486820709\n",
            "train loss:0.10507789467085507\n",
            "train loss:0.08256960903040919\n",
            "train loss:0.12776005516559746\n",
            "train loss:0.12431073371193557\n",
            "train loss:0.07900593930224463\n",
            "train loss:0.050726605797791514\n",
            "train loss:0.060288947410034155\n",
            "train loss:0.08644038171815784\n",
            "train loss:0.031449554201311604\n",
            "train loss:0.04971465004483754\n",
            "train loss:0.04524137198966611\n",
            "train loss:0.1252044427002213\n",
            "train loss:0.06788443311746617\n",
            "train loss:0.04844794540291128\n",
            "train loss:0.042210331490587887\n",
            "train loss:0.0376749362846515\n",
            "train loss:0.04772324211108022\n",
            "train loss:0.11527776131728942\n",
            "train loss:0.0601597045327819\n",
            "train loss:0.07696250305968153\n",
            "train loss:0.06153823602384547\n",
            "train loss:0.12409681601512385\n",
            "train loss:0.014686645442363588\n",
            "train loss:0.023196699793292308\n",
            "train loss:0.045955955464023895\n",
            "train loss:0.03430461161141713\n",
            "train loss:0.04820487264087192\n",
            "train loss:0.1316063720876084\n",
            "train loss:0.0645470143861172\n",
            "train loss:0.041369334683491556\n",
            "train loss:0.051538760528844305\n",
            "train loss:0.08255972635876224\n",
            "train loss:0.07418678298052372\n",
            "train loss:0.07540633164086459\n",
            "train loss:0.06715362049471868\n",
            "train loss:0.03601496223589234\n",
            "train loss:0.04202576335089813\n",
            "train loss:0.07444858041662579\n",
            "train loss:0.045367074737463646\n",
            "train loss:0.06397710183104365\n",
            "train loss:0.013872073725666045\n",
            "train loss:0.09314561879004037\n",
            "train loss:0.031124714863626526\n",
            "train loss:0.11425121884497899\n",
            "train loss:0.09478515999794242\n",
            "train loss:0.06658983261896716\n",
            "train loss:0.06813265421096257\n",
            "train loss:0.10521135115355387\n",
            "train loss:0.048433710390779955\n",
            "train loss:0.04140076962579334\n",
            "train loss:0.08762977973641682\n",
            "train loss:0.054999459073520135\n",
            "train loss:0.03309884004605515\n",
            "train loss:0.06833907888039298\n",
            "train loss:0.07216223547756476\n",
            "train loss:0.030636017395229777\n",
            "train loss:0.055568162109880276\n",
            "train loss:0.07410683903914875\n",
            "train loss:0.06502584105793871\n",
            "train loss:0.050959519536364156\n",
            "train loss:0.06191427783030357\n",
            "train loss:0.15667830753763926\n",
            "train loss:0.0822061424051113\n",
            "train loss:0.0577062390947283\n",
            "train loss:0.0834000947414673\n",
            "train loss:0.05501252740750723\n",
            "train loss:0.11505310382372601\n",
            "train loss:0.07951769564506045\n",
            "train loss:0.04057868258197858\n",
            "train loss:0.04713791433305574\n",
            "train loss:0.0599019244786756\n",
            "train loss:0.04088715476816151\n",
            "train loss:0.09900949547838309\n",
            "train loss:0.07626807663012236\n",
            "train loss:0.032806861637095175\n",
            "train loss:0.0649765882536312\n",
            "train loss:0.07986627856445348\n",
            "train loss:0.0529754311633662\n",
            "train loss:0.06656709076191056\n",
            "train loss:0.07458351238040561\n",
            "train loss:0.10620125103099783\n",
            "train loss:0.06332298602185449\n",
            "train loss:0.05633208451815899\n",
            "train loss:0.09469942677880588\n",
            "train loss:0.09629910899457556\n",
            "train loss:0.03679101111177429\n",
            "train loss:0.0630776390077646\n",
            "train loss:0.07309570186791971\n",
            "train loss:0.1022811313264539\n",
            "train loss:0.13792877357195532\n",
            "train loss:0.04864208714781564\n",
            "train loss:0.08425509793138358\n",
            "train loss:0.03544468675946031\n",
            "train loss:0.09087664910560106\n",
            "train loss:0.03448616551240382\n",
            "train loss:0.06779906199629304\n",
            "train loss:0.07446379228450142\n",
            "train loss:0.0566968517053198\n",
            "train loss:0.06297711969970222\n",
            "train loss:0.06293614417641707\n",
            "train loss:0.035280189352806544\n",
            "train loss:0.032489386798990105\n",
            "train loss:0.04355318182666756\n",
            "train loss:0.166529533787971\n",
            "train loss:0.03876633121405183\n",
            "train loss:0.028161162487778446\n",
            "train loss:0.04755876454377856\n",
            "train loss:0.055799361496000434\n",
            "train loss:0.04302975189173953\n",
            "train loss:0.05110500337698087\n",
            "train loss:0.06422692503659146\n",
            "train loss:0.03873636617156688\n",
            "train loss:0.05055602319573848\n",
            "train loss:0.09781220326469502\n",
            "train loss:0.046615978494768485\n",
            "train loss:0.03880702082899597\n",
            "train loss:0.028562381705429095\n",
            "train loss:0.028036194596315213\n",
            "train loss:0.047925424814265094\n",
            "train loss:0.0568102687417185\n",
            "train loss:0.06822165510323462\n",
            "train loss:0.11780537872285271\n",
            "train loss:0.08144804149470167\n",
            "train loss:0.03354659281243278\n",
            "train loss:0.13138837510774862\n",
            "train loss:0.04525048322056822\n",
            "train loss:0.12683122738753846\n",
            "train loss:0.1077678872149044\n",
            "train loss:0.14788407965942585\n",
            "train loss:0.035464819061583046\n",
            "train loss:0.06668127532075163\n",
            "train loss:0.08152036797757453\n",
            "train loss:0.04093195315644294\n",
            "train loss:0.02835877481679101\n",
            "train loss:0.03829144694090409\n",
            "train loss:0.06290621005504379\n",
            "train loss:0.035543134463001974\n",
            "train loss:0.04924907986875914\n",
            "train loss:0.08896733091672417\n",
            "train loss:0.08513259611032246\n",
            "train loss:0.0758956150286551\n",
            "train loss:0.10223410612611504\n",
            "train loss:0.12408839059802323\n",
            "train loss:0.07661076176307087\n",
            "train loss:0.06766448883831853\n",
            "train loss:0.04576959156166352\n",
            "train loss:0.07833624509252596\n",
            "train loss:0.05043048026147314\n",
            "train loss:0.05782059123501972\n",
            "train loss:0.05287809747409789\n",
            "train loss:0.05499728303286181\n",
            "train loss:0.03644517044103905\n",
            "train loss:0.03277585866185861\n",
            "train loss:0.047275144259969586\n",
            "train loss:0.16327370576858244\n",
            "train loss:0.0702207205013894\n",
            "train loss:0.05178162364158153\n",
            "train loss:0.07628171829044543\n",
            "train loss:0.05438849148666819\n",
            "train loss:0.03897015514980734\n",
            "train loss:0.03748832328855492\n",
            "train loss:0.04333517672599292\n",
            "train loss:0.027319317052971956\n",
            "train loss:0.04979464363126253\n",
            "train loss:0.03482434975022555\n",
            "train loss:0.05103010726571888\n",
            "train loss:0.10590969566341439\n",
            "train loss:0.09081307942546965\n",
            "train loss:0.061464467785075444\n",
            "train loss:0.06373879976088137\n",
            "train loss:0.025299238556089013\n",
            "train loss:0.10186459096558396\n",
            "train loss:0.08354994570797367\n",
            "train loss:0.08980677671145348\n",
            "train loss:0.09893737908118638\n",
            "train loss:0.0766269231192648\n",
            "train loss:0.015423886619881512\n",
            "train loss:0.08175279751166126\n",
            "train loss:0.05054630933778832\n",
            "train loss:0.06083976060767223\n",
            "train loss:0.03926938337616901\n",
            "train loss:0.1013478642914656\n",
            "train loss:0.08187403922588096\n",
            "train loss:0.12673658038178104\n",
            "train loss:0.024556489733135674\n",
            "train loss:0.1096200623643446\n",
            "train loss:0.04775280138728766\n",
            "train loss:0.14109611095632546\n",
            "train loss:0.048334544994105465\n",
            "train loss:0.054580444536650986\n",
            "train loss:0.04171922174226319\n",
            "train loss:0.064000030491925\n",
            "train loss:0.09218438478324606\n",
            "train loss:0.07540969229447694\n",
            "train loss:0.04321001881922986\n",
            "train loss:0.09836935999724694\n",
            "train loss:0.0439666806583039\n",
            "train loss:0.03394726676647377\n",
            "train loss:0.03693068091014474\n",
            "train loss:0.027817926799815238\n",
            "train loss:0.059465053148285374\n",
            "train loss:0.06394164525272505\n",
            "train loss:0.07929651625985434\n",
            "train loss:0.08037137795930584\n",
            "train loss:0.056995510145002155\n",
            "train loss:0.0685433194013816\n",
            "train loss:0.06248514143207296\n",
            "train loss:0.06281205568086853\n",
            "train loss:0.0940720578264216\n",
            "train loss:0.09641709962486912\n",
            "train loss:0.05760465049077184\n",
            "train loss:0.05934037322190537\n",
            "train loss:0.09579627294561772\n",
            "train loss:0.036985806746477785\n",
            "train loss:0.053627228953875145\n",
            "train loss:0.06095734881520294\n",
            "train loss:0.055011272602602766\n",
            "train loss:0.051292882130923347\n",
            "train loss:0.09820256878351015\n",
            "train loss:0.0550211663978134\n",
            "train loss:0.05315635770698452\n",
            "train loss:0.0627249316195611\n",
            "train loss:0.0503521402080347\n",
            "train loss:0.06191130017745999\n",
            "train loss:0.05700050854222211\n",
            "train loss:0.10112366687387081\n",
            "train loss:0.08197872074943714\n",
            "train loss:0.17028297626895494\n",
            "train loss:0.060873024654130845\n",
            "train loss:0.060440836578450785\n",
            "train loss:0.06167619027728003\n",
            "train loss:0.0652711682668591\n",
            "train loss:0.09585503790715817\n",
            "train loss:0.07711960177429054\n",
            "train loss:0.049871656989277244\n",
            "train loss:0.03835783758457305\n",
            "train loss:0.08990993321626521\n",
            "train loss:0.04763089151349054\n",
            "train loss:0.07881971169691437\n",
            "train loss:0.06471952353785675\n",
            "train loss:0.04262211835519407\n",
            "train loss:0.04311182076740398\n",
            "train loss:0.1049711344587568\n",
            "train loss:0.10984593742660696\n",
            "train loss:0.07528202691181676\n",
            "train loss:0.060079730774465545\n",
            "train loss:0.1497518548723843\n",
            "train loss:0.07637390362544527\n",
            "train loss:0.05699793407392824\n",
            "train loss:0.12287575813181659\n",
            "train loss:0.14596612348796334\n",
            "train loss:0.10864487078061298\n",
            "train loss:0.083287655067175\n",
            "train loss:0.04754850976964154\n",
            "train loss:0.038217218978743715\n",
            "train loss:0.06904356928977609\n",
            "train loss:0.05809840782929036\n",
            "train loss:0.05686214802835921\n",
            "train loss:0.032455100842177405\n",
            "train loss:0.042358367041417155\n",
            "train loss:0.04897320662786339\n",
            "train loss:0.02584876051842445\n",
            "train loss:0.02936767375737733\n",
            "train loss:0.07282862076703657\n",
            "train loss:0.16051451524587446\n",
            "train loss:0.06244696746635067\n",
            "train loss:0.039122819677847025\n",
            "train loss:0.04816409317277847\n",
            "train loss:0.06624149079240539\n",
            "train loss:0.1327433838285574\n",
            "train loss:0.07062269990268413\n",
            "train loss:0.0567141616011065\n",
            "train loss:0.0670648554493207\n",
            "train loss:0.07434704889933892\n",
            "train loss:0.016128509840586867\n",
            "train loss:0.052207299893016146\n",
            "train loss:0.08820590236185359\n",
            "train loss:0.0892248364742962\n",
            "train loss:0.026033886488230754\n",
            "train loss:0.07728544838384628\n",
            "train loss:0.14239707887201986\n",
            "train loss:0.046991988224277914\n",
            "train loss:0.02484839315249603\n",
            "train loss:0.04511295314641204\n",
            "train loss:0.037165137833668795\n",
            "train loss:0.04702881055368246\n",
            "train loss:0.0451976016266548\n",
            "train loss:0.1248228888202108\n",
            "train loss:0.05076972081855293\n",
            "train loss:0.03323862456091666\n",
            "train loss:0.09776539629282203\n",
            "train loss:0.11167223917467338\n",
            "train loss:0.12730105775835607\n",
            "train loss:0.135277486043831\n",
            "train loss:0.03837244215946257\n",
            "train loss:0.06282270287329547\n",
            "train loss:0.05274968815599666\n",
            "train loss:0.07193829997919358\n",
            "train loss:0.07486246413219877\n",
            "train loss:0.06815204788447682\n",
            "train loss:0.09511534176083464\n",
            "train loss:0.09356947902799567\n",
            "train loss:0.046187142512927826\n",
            "train loss:0.09049492918919402\n",
            "train loss:0.03683983817445099\n",
            "train loss:0.08386212497427215\n",
            "train loss:0.05181019039196385\n",
            "train loss:0.06071779086139834\n",
            "train loss:0.06800219032052468\n",
            "train loss:0.021341574666074038\n",
            "train loss:0.04363286724740492\n",
            "train loss:0.06611532211979655\n",
            "train loss:0.05303529891606058\n",
            "train loss:0.046774406545858266\n",
            "train loss:0.05701418197709729\n",
            "train loss:0.05127133195204402\n",
            "train loss:0.08151306286968499\n",
            "train loss:0.016312844008866526\n",
            "train loss:0.038634445900331965\n",
            "train loss:0.08435005190087062\n",
            "train loss:0.07026718691786125\n",
            "train loss:0.04380904855474718\n",
            "train loss:0.022758826825113182\n",
            "train loss:0.058283050163959675\n",
            "train loss:0.07118075445644964\n",
            "train loss:0.07933656962632146\n",
            "train loss:0.09206201031891997\n",
            "train loss:0.04829170237040848\n",
            "train loss:0.07952964073947129\n",
            "train loss:0.13954481609367733\n",
            "train loss:0.045963406060375654\n",
            "train loss:0.031144293862302358\n",
            "train loss:0.03492908765110513\n",
            "train loss:0.07676856215425736\n",
            "train loss:0.060626110822164715\n",
            "train loss:0.07124944391522017\n",
            "train loss:0.02313650854145425\n",
            "train loss:0.19281614670180638\n",
            "train loss:0.048727059724674705\n",
            "train loss:0.03728594440211579\n",
            "train loss:0.08714702136579397\n",
            "train loss:0.03862158785000002\n",
            "train loss:0.03164783934180424\n",
            "train loss:0.08742748791411083\n",
            "train loss:0.05747508904129546\n",
            "train loss:0.07743335193734642\n",
            "train loss:0.0480776399878145\n",
            "train loss:0.06071520522689889\n",
            "train loss:0.08387566065073514\n",
            "train loss:0.04739964495870501\n",
            "train loss:0.05864125754641967\n",
            "train loss:0.04820060156450113\n",
            "train loss:0.06886185577694526\n",
            "train loss:0.12031547872194454\n",
            "train loss:0.07440026775777135\n",
            "train loss:0.0926307372202822\n",
            "train loss:0.05906824297142516\n",
            "train loss:0.041663791919986995\n",
            "train loss:0.07015033195991605\n",
            "train loss:0.10816917184477673\n",
            "train loss:0.04999434814430415\n",
            "train loss:0.03330880137619725\n",
            "train loss:0.060563871545174675\n",
            "train loss:0.030622067543082162\n",
            "train loss:0.06347262854323912\n",
            "train loss:0.05321682870207146\n",
            "train loss:0.08740612807866827\n",
            "train loss:0.04908874250078475\n",
            "train loss:0.06160555482574183\n",
            "train loss:0.018376778282209575\n",
            "train loss:0.03315246064488842\n",
            "train loss:0.04292470849381319\n",
            "train loss:0.05794874611579366\n",
            "train loss:0.045863037714309704\n",
            "train loss:0.01770922154634146\n",
            "train loss:0.03561753182418001\n",
            "train loss:0.01834208699676867\n",
            "train loss:0.05322572702682388\n",
            "train loss:0.022230537240585085\n",
            "train loss:0.14109150393213413\n",
            "train loss:0.023868915658266508\n",
            "train loss:0.04995538709660646\n",
            "train loss:0.0704018658273137\n",
            "train loss:0.03306372903053763\n",
            "train loss:0.049244437950954595\n",
            "train loss:0.02545165825124561\n",
            "train loss:0.04018397730837599\n",
            "train loss:0.05141995552295955\n",
            "train loss:0.046365706374629265\n",
            "train loss:0.05363896445073694\n",
            "train loss:0.023276646068442233\n",
            "train loss:0.03769259196054589\n",
            "train loss:0.011828328002642375\n",
            "train loss:0.03867080649526824\n",
            "train loss:0.09084783407331684\n",
            "train loss:0.03682309255609597\n",
            "train loss:0.09996331864896808\n",
            "train loss:0.05049862944012662\n",
            "train loss:0.05835629933354037\n",
            "=== epoch:14, train acc:0.977, test acc:0.906 ===\n",
            "train loss:0.11714367191326965\n",
            "train loss:0.03612067321388815\n",
            "train loss:0.05644458266037097\n",
            "train loss:0.10432869395453405\n",
            "train loss:0.046912839220195596\n",
            "train loss:0.06631629350359446\n",
            "train loss:0.03383491549850043\n",
            "train loss:0.022190017087954605\n",
            "train loss:0.17612729931853552\n",
            "train loss:0.038117387479215115\n",
            "train loss:0.022967154309246694\n",
            "train loss:0.0503591415694359\n",
            "train loss:0.12619968673023585\n",
            "train loss:0.050213516043139304\n",
            "train loss:0.1595011581633089\n",
            "train loss:0.06557525676651171\n",
            "train loss:0.05432889066243184\n",
            "train loss:0.034649265667006504\n",
            "train loss:0.10603644804257635\n",
            "train loss:0.09187419508175763\n",
            "train loss:0.07189326426815257\n",
            "train loss:0.08175808368373266\n",
            "train loss:0.0521581915574989\n",
            "train loss:0.03954541275939496\n",
            "train loss:0.11808541300961126\n",
            "train loss:0.06726000810479803\n",
            "train loss:0.038965435277785954\n",
            "train loss:0.09237261750149715\n",
            "train loss:0.1086891865344178\n",
            "train loss:0.033241441576821275\n",
            "train loss:0.04468246976345032\n",
            "train loss:0.09234630404804758\n",
            "train loss:0.04730968714173908\n",
            "train loss:0.1520902303321883\n",
            "train loss:0.04291914408605621\n",
            "train loss:0.08580644911905354\n",
            "train loss:0.11086599675836668\n",
            "train loss:0.09037328231895764\n",
            "train loss:0.029310395929749652\n",
            "train loss:0.026169990929105356\n",
            "train loss:0.034286294096743164\n",
            "train loss:0.0766874111707717\n",
            "train loss:0.036705533556896125\n",
            "train loss:0.07732779857128856\n",
            "train loss:0.034745679124620545\n",
            "train loss:0.08732105763512843\n",
            "train loss:0.05705554599012049\n",
            "train loss:0.04586140890354807\n",
            "train loss:0.06177975594797034\n",
            "train loss:0.1084470165086685\n",
            "train loss:0.06995960697166986\n",
            "train loss:0.03934717563923862\n",
            "train loss:0.07753526910001503\n",
            "train loss:0.09047317930655849\n",
            "train loss:0.03065048121391918\n",
            "train loss:0.040086379408540436\n",
            "train loss:0.06284198055393618\n",
            "train loss:0.041723333403883546\n",
            "train loss:0.04455254594574508\n",
            "train loss:0.05649140961606185\n",
            "train loss:0.06500426067510075\n",
            "train loss:0.03864789910043659\n",
            "train loss:0.062019183182635114\n",
            "train loss:0.043371203785831834\n",
            "train loss:0.041039263120195464\n",
            "train loss:0.07888046076712274\n",
            "train loss:0.038039849323648504\n",
            "train loss:0.07049956291164688\n",
            "train loss:0.06793181886460822\n",
            "train loss:0.04503718733281953\n",
            "train loss:0.04972896808725817\n",
            "train loss:0.01787325285078236\n",
            "train loss:0.09159765120509868\n",
            "train loss:0.06814919564709367\n",
            "train loss:0.04872149644485755\n",
            "train loss:0.02353099798052686\n",
            "train loss:0.10698319594357533\n",
            "train loss:0.09237617025863248\n",
            "train loss:0.08789567568704788\n",
            "train loss:0.06884007446577456\n",
            "train loss:0.04853143416040719\n",
            "train loss:0.03241036756370657\n",
            "train loss:0.031053873111623026\n",
            "train loss:0.023844814051970233\n",
            "train loss:0.09326741099379877\n",
            "train loss:0.02554266181108211\n",
            "train loss:0.09622901908087622\n",
            "train loss:0.03843413540725468\n",
            "train loss:0.0383832113653289\n",
            "train loss:0.05750340002363906\n",
            "train loss:0.06222169916876636\n",
            "train loss:0.06818866970611948\n",
            "train loss:0.10131509149009942\n",
            "train loss:0.07902443142557919\n",
            "train loss:0.13576068542001976\n",
            "train loss:0.042207628356366\n",
            "train loss:0.04618414971044657\n",
            "train loss:0.05608462809863817\n",
            "train loss:0.02177439519689075\n",
            "train loss:0.07210252630992624\n",
            "train loss:0.05770008656495754\n",
            "train loss:0.05032408160405021\n",
            "train loss:0.08100047141741135\n",
            "train loss:0.059928691568975276\n",
            "train loss:0.057014286927840205\n",
            "train loss:0.061371671933501\n",
            "train loss:0.026310593256816842\n",
            "train loss:0.04688789962766828\n",
            "train loss:0.023296845600478965\n",
            "train loss:0.04720478981240116\n",
            "train loss:0.08099081589472229\n",
            "train loss:0.09212764876484635\n",
            "train loss:0.08568537264554626\n",
            "train loss:0.04310131048065706\n",
            "train loss:0.05539022442452591\n",
            "train loss:0.08556371784532121\n",
            "train loss:0.11100276724555154\n",
            "train loss:0.03590429230079693\n",
            "train loss:0.06054682596839252\n",
            "train loss:0.04540150865471679\n",
            "train loss:0.02473084589641291\n",
            "train loss:0.06303619236782917\n",
            "train loss:0.03452603526390665\n",
            "train loss:0.05326454815111663\n",
            "train loss:0.07303143015626506\n",
            "train loss:0.07701903499114679\n",
            "train loss:0.055142333227436016\n",
            "train loss:0.05300568420473344\n",
            "train loss:0.06855153777710123\n",
            "train loss:0.04045193797405575\n",
            "train loss:0.03701440280254081\n",
            "train loss:0.05605927211107547\n",
            "train loss:0.03328866619377166\n",
            "train loss:0.04126554413405364\n",
            "train loss:0.08253341716041303\n",
            "train loss:0.10071916601846659\n",
            "train loss:0.018871242527605265\n",
            "train loss:0.05338890377336015\n",
            "train loss:0.05204496303949834\n",
            "train loss:0.059035707318803324\n",
            "train loss:0.03832588695649242\n",
            "train loss:0.04211867476408608\n",
            "train loss:0.046998113823521495\n",
            "train loss:0.036732547688282255\n",
            "train loss:0.035992606885835776\n",
            "train loss:0.06856106721253612\n",
            "train loss:0.02661486867350688\n",
            "train loss:0.017398646896724587\n",
            "train loss:0.07150575766835846\n",
            "train loss:0.07109419006371934\n",
            "train loss:0.02212763921878038\n",
            "train loss:0.10242494440836333\n",
            "train loss:0.04739124776481488\n",
            "train loss:0.10728409790037141\n",
            "train loss:0.07176722267559608\n",
            "train loss:0.08211250620913219\n",
            "train loss:0.023332575025876535\n",
            "train loss:0.06713147107136638\n",
            "train loss:0.03816873823987642\n",
            "train loss:0.029584366295792645\n",
            "train loss:0.07247060861881705\n",
            "train loss:0.04548439852925525\n",
            "train loss:0.03730080682052135\n",
            "train loss:0.0798443299817959\n",
            "train loss:0.023897817092794097\n",
            "train loss:0.07750500785087283\n",
            "train loss:0.0712760068720413\n",
            "train loss:0.07447161829305653\n",
            "train loss:0.03399266206033382\n",
            "train loss:0.03692781394837154\n",
            "train loss:0.03403591176861384\n",
            "train loss:0.046229041753899774\n",
            "train loss:0.04296509284548125\n",
            "train loss:0.014674560379224266\n",
            "train loss:0.03486086554765274\n",
            "train loss:0.1157530537918892\n",
            "train loss:0.0133713435872324\n",
            "train loss:0.05708461934648488\n",
            "train loss:0.09532213810331916\n",
            "train loss:0.06636518214835649\n",
            "train loss:0.08165002596661058\n",
            "train loss:0.09607602812616549\n",
            "train loss:0.13694113487961537\n",
            "train loss:0.031711801632554916\n",
            "train loss:0.04483690290044162\n",
            "train loss:0.06548515480100822\n",
            "train loss:0.16862166432184236\n",
            "train loss:0.08607811358422111\n",
            "train loss:0.05877256687991915\n",
            "train loss:0.08185601682556444\n",
            "train loss:0.04309071360837668\n",
            "train loss:0.08326160570244381\n",
            "train loss:0.07179808839252723\n",
            "train loss:0.06892063354082428\n",
            "train loss:0.08438375893618649\n",
            "train loss:0.05900465034246862\n",
            "train loss:0.04720038960723622\n",
            "train loss:0.0879992664458863\n",
            "train loss:0.022167841276374844\n",
            "train loss:0.08808369954796424\n",
            "train loss:0.0688051076118985\n",
            "train loss:0.05303101509342447\n",
            "train loss:0.036058059052648\n",
            "train loss:0.02511823090084778\n",
            "train loss:0.049160861821071995\n",
            "train loss:0.022942250265193233\n",
            "train loss:0.04078540814613895\n",
            "train loss:0.037251783047886435\n",
            "train loss:0.03510990032291911\n",
            "train loss:0.0548277734601311\n",
            "train loss:0.06751450396473815\n",
            "train loss:0.019027923110349593\n",
            "train loss:0.05565155757545127\n",
            "train loss:0.03737960820668298\n",
            "train loss:0.06751961233859582\n",
            "train loss:0.08833281226129365\n",
            "train loss:0.14079364811731834\n",
            "train loss:0.036148955061824965\n",
            "train loss:0.07517015834650477\n",
            "train loss:0.06646058445324556\n",
            "train loss:0.05227507316215221\n",
            "train loss:0.025633076695825163\n",
            "train loss:0.04459435791212035\n",
            "train loss:0.03717282918177179\n",
            "train loss:0.023470256766727756\n",
            "train loss:0.06955194359855695\n",
            "train loss:0.052498407456796944\n",
            "train loss:0.07399226825803931\n",
            "train loss:0.0136417879585667\n",
            "train loss:0.04486518038310341\n",
            "train loss:0.07963612265223678\n",
            "train loss:0.07071432532443825\n",
            "train loss:0.0503686821612966\n",
            "train loss:0.05934360657191612\n",
            "train loss:0.04122551594758469\n",
            "train loss:0.03167075821748639\n",
            "train loss:0.07678751968810917\n",
            "train loss:0.04266822331833974\n",
            "train loss:0.041171464832865705\n",
            "train loss:0.07026448136234179\n",
            "train loss:0.09975516510688172\n",
            "train loss:0.054497125228534504\n",
            "train loss:0.0306656318605811\n",
            "train loss:0.03376167453789747\n",
            "train loss:0.04676598038683202\n",
            "train loss:0.030989174274894036\n",
            "train loss:0.08195514022670775\n",
            "train loss:0.04736587889968901\n",
            "train loss:0.16622200580200824\n",
            "train loss:0.06518119613445049\n",
            "train loss:0.02135182641317406\n",
            "train loss:0.037513058439022244\n",
            "train loss:0.05372918748786592\n",
            "train loss:0.031184330811765647\n",
            "train loss:0.04063389740734203\n",
            "train loss:0.04042196657683062\n",
            "train loss:0.06005393980998948\n",
            "train loss:0.06872421428011248\n",
            "train loss:0.03696220895301974\n",
            "train loss:0.02285356274197062\n",
            "train loss:0.02257589576140234\n",
            "train loss:0.06337494164769769\n",
            "train loss:0.045578213629307356\n",
            "train loss:0.08401406423015018\n",
            "train loss:0.046083345098197\n",
            "train loss:0.1105843902911633\n",
            "train loss:0.09100776031551336\n",
            "train loss:0.04704218253063727\n",
            "train loss:0.03405490039119029\n",
            "train loss:0.08795389770780604\n",
            "train loss:0.03501139117311621\n",
            "train loss:0.05044041720037036\n",
            "train loss:0.009004701556871225\n",
            "train loss:0.04287287605557422\n",
            "train loss:0.0424447407725768\n",
            "train loss:0.060871101009731615\n",
            "train loss:0.02799063911728681\n",
            "train loss:0.027505623480115884\n",
            "train loss:0.0333105887120674\n",
            "train loss:0.07622662890397672\n",
            "train loss:0.05891000543219623\n",
            "train loss:0.030338803630549145\n",
            "train loss:0.0548830406857617\n",
            "train loss:0.09046033731533566\n",
            "train loss:0.023446109459623676\n",
            "train loss:0.037489506880675795\n",
            "train loss:0.014653949536067966\n",
            "train loss:0.03557261530975017\n",
            "train loss:0.054566689550038454\n",
            "train loss:0.053911757415314704\n",
            "train loss:0.03608710237028163\n",
            "train loss:0.08462161635018782\n",
            "train loss:0.06549837926115241\n",
            "train loss:0.03094665701265591\n",
            "train loss:0.03878029947898261\n",
            "train loss:0.09970311608812495\n",
            "train loss:0.042527887742707986\n",
            "train loss:0.07396799899400008\n",
            "train loss:0.04991964236573015\n",
            "train loss:0.0656849499366274\n",
            "train loss:0.019504429885840356\n",
            "train loss:0.014262116779953517\n",
            "train loss:0.04677384125140937\n",
            "train loss:0.10481678898931626\n",
            "train loss:0.04024915087522076\n",
            "train loss:0.05307087049884726\n",
            "train loss:0.034334665624816456\n",
            "train loss:0.05517832284442268\n",
            "train loss:0.01688396788264788\n",
            "train loss:0.03751405083998288\n",
            "train loss:0.036045337902768156\n",
            "train loss:0.045017167181253426\n",
            "train loss:0.021628129534664885\n",
            "train loss:0.13229381073057933\n",
            "train loss:0.05641742208337451\n",
            "train loss:0.09723225626419542\n",
            "train loss:0.05465364634127172\n",
            "train loss:0.048489267586479\n",
            "train loss:0.06570136292653997\n",
            "train loss:0.1190437734599603\n",
            "train loss:0.020716199669222633\n",
            "train loss:0.03692910233053825\n",
            "train loss:0.05468087282755023\n",
            "train loss:0.0553845473927633\n",
            "train loss:0.06724285114787791\n",
            "train loss:0.03627666724294634\n",
            "train loss:0.014763962689888579\n",
            "train loss:0.03165115105255011\n",
            "train loss:0.11503471091472024\n",
            "train loss:0.04896341612461431\n",
            "train loss:0.05581021583741769\n",
            "train loss:0.037629110978113396\n",
            "train loss:0.020984057993970424\n",
            "train loss:0.02629565701447167\n",
            "train loss:0.03311215316287633\n",
            "train loss:0.013886916901777407\n",
            "train loss:0.02289189382578793\n",
            "train loss:0.06503967024243273\n",
            "train loss:0.0691160872173464\n",
            "train loss:0.03808954842973735\n",
            "train loss:0.09527307143049187\n",
            "train loss:0.03205436823562019\n",
            "train loss:0.022870922918337314\n",
            "train loss:0.05875877392553688\n",
            "train loss:0.07197319019876998\n",
            "train loss:0.03941642286528127\n",
            "train loss:0.08202282260345055\n",
            "train loss:0.0400021218273698\n",
            "train loss:0.021010677209308836\n",
            "train loss:0.016146224060865483\n",
            "train loss:0.05013683430515746\n",
            "train loss:0.020616059594345582\n",
            "train loss:0.06252498978728062\n",
            "train loss:0.04083660180699113\n",
            "train loss:0.0367280737305337\n",
            "train loss:0.02734826020524829\n",
            "train loss:0.04142038620017999\n",
            "train loss:0.10732705150978519\n",
            "train loss:0.021232037543496937\n",
            "train loss:0.02846065291943018\n",
            "train loss:0.0480394589188634\n",
            "train loss:0.030354448184714812\n",
            "train loss:0.07082163431223355\n",
            "train loss:0.10123104806630927\n",
            "train loss:0.06222602174285624\n",
            "train loss:0.03318126182849794\n",
            "train loss:0.03166595658448524\n",
            "train loss:0.06268381598025073\n",
            "train loss:0.05827623001541389\n",
            "train loss:0.014627687350455271\n",
            "train loss:0.06363295820171716\n",
            "train loss:0.07863095608363352\n",
            "train loss:0.05127969838428825\n",
            "train loss:0.03874123890650447\n",
            "train loss:0.07439627114514293\n",
            "train loss:0.0582696828621408\n",
            "train loss:0.09616500746141127\n",
            "train loss:0.04312616984544253\n",
            "train loss:0.030762627852938675\n",
            "train loss:0.038625318500099974\n",
            "train loss:0.04546212309516323\n",
            "train loss:0.029335522707175977\n",
            "train loss:0.0693434397062214\n",
            "train loss:0.0673363534762281\n",
            "train loss:0.03445495785437302\n",
            "train loss:0.06137071978817997\n",
            "train loss:0.02691848759595275\n",
            "train loss:0.05412044599465484\n",
            "train loss:0.046589579999204184\n",
            "train loss:0.05262843948308737\n",
            "train loss:0.04540251790779247\n",
            "train loss:0.03818712852792423\n",
            "train loss:0.04166373258913169\n",
            "train loss:0.012270564139585494\n",
            "train loss:0.029429304302162774\n",
            "train loss:0.06087667282293159\n",
            "train loss:0.02167395058456906\n",
            "train loss:0.038431575455476416\n",
            "train loss:0.07565947126618934\n",
            "train loss:0.028144155985101657\n",
            "train loss:0.05511293407861949\n",
            "train loss:0.08315113258069905\n",
            "train loss:0.0183195882932405\n",
            "train loss:0.04508307442038225\n",
            "train loss:0.09129493052601428\n",
            "train loss:0.03837585843842141\n",
            "train loss:0.03375126899509015\n",
            "train loss:0.05505003256187686\n",
            "train loss:0.030989050558684242\n",
            "train loss:0.05583036544424633\n",
            "train loss:0.027753150450675897\n",
            "train loss:0.04107211779552083\n",
            "train loss:0.029210730581792764\n",
            "train loss:0.01573237749506441\n",
            "train loss:0.039406293154263465\n",
            "train loss:0.06444929027274346\n",
            "train loss:0.13480923137307133\n",
            "train loss:0.041223075749776975\n",
            "train loss:0.044692728659377706\n",
            "train loss:0.06617568316062743\n",
            "train loss:0.10283171579221427\n",
            "train loss:0.07854049576928201\n",
            "train loss:0.06374159536333614\n",
            "train loss:0.04845491780558629\n",
            "train loss:0.031858518481015834\n",
            "train loss:0.05407831649600036\n",
            "train loss:0.20062826657021066\n",
            "train loss:0.0194828858208923\n",
            "train loss:0.023138447863736325\n",
            "train loss:0.09141101011314177\n",
            "train loss:0.03401225023120023\n",
            "train loss:0.04924648827533172\n",
            "train loss:0.03249784534116222\n",
            "train loss:0.07895618306710099\n",
            "train loss:0.020185194997509214\n",
            "train loss:0.03284172902760235\n",
            "train loss:0.03918636728105123\n",
            "train loss:0.10981375852042202\n",
            "train loss:0.031992900755100376\n",
            "train loss:0.03958677872781133\n",
            "train loss:0.019433805879588716\n",
            "train loss:0.0832131169466404\n",
            "train loss:0.031325697023394154\n",
            "train loss:0.06281177927815774\n",
            "train loss:0.048637139991325296\n",
            "train loss:0.07798419818732723\n",
            "train loss:0.05025285394927491\n",
            "train loss:0.05264725591433568\n",
            "train loss:0.04494838224841531\n",
            "train loss:0.04949603121090498\n",
            "train loss:0.04261787844680095\n",
            "train loss:0.049182863472496924\n",
            "train loss:0.05662430214766168\n",
            "train loss:0.05328851943166863\n",
            "train loss:0.0663546970862549\n",
            "train loss:0.03497481122756098\n",
            "train loss:0.031574869774594075\n",
            "train loss:0.013985203161288035\n",
            "train loss:0.028158110071577983\n",
            "train loss:0.06980181818410557\n",
            "train loss:0.09249601184307712\n",
            "train loss:0.04129522819459858\n",
            "train loss:0.10564429702116133\n",
            "train loss:0.14509056881746535\n",
            "train loss:0.05563815417531396\n",
            "train loss:0.047524064082906305\n",
            "train loss:0.07145769483741382\n",
            "train loss:0.08276710826683577\n",
            "train loss:0.040024223967724364\n",
            "train loss:0.051497118562842295\n",
            "train loss:0.023731470520102226\n",
            "train loss:0.027109807616613787\n",
            "train loss:0.03320507577249214\n",
            "train loss:0.06571643256304725\n",
            "train loss:0.022423096209953534\n",
            "train loss:0.03454303600740908\n",
            "train loss:0.06418355214975438\n",
            "train loss:0.03787447433118818\n",
            "train loss:0.009591243828223796\n",
            "train loss:0.048962091516742065\n",
            "train loss:0.04475894385528893\n",
            "train loss:0.10267173182930427\n",
            "train loss:0.016466877389816156\n",
            "train loss:0.01794929260630654\n",
            "train loss:0.072942883133016\n",
            "train loss:0.08530180931336645\n",
            "train loss:0.037501451341841886\n",
            "train loss:0.03978217042671803\n",
            "train loss:0.09735376655410467\n",
            "train loss:0.056333788639056505\n",
            "train loss:0.03371023526206839\n",
            "train loss:0.038182475475852885\n",
            "train loss:0.06672384672673451\n",
            "train loss:0.09089766154804123\n",
            "train loss:0.039473507066293906\n",
            "train loss:0.12967767275189424\n",
            "train loss:0.03289191527148631\n",
            "train loss:0.11043739054795873\n",
            "train loss:0.09807350715720081\n",
            "train loss:0.020850966513269717\n",
            "train loss:0.051007750558246065\n",
            "train loss:0.02889619644842676\n",
            "train loss:0.07769399191037324\n",
            "train loss:0.05859556232145678\n",
            "train loss:0.09239048388368755\n",
            "train loss:0.03651518212915054\n",
            "train loss:0.0707543437849001\n",
            "train loss:0.04982807254724417\n",
            "train loss:0.02207986446410161\n",
            "train loss:0.054557007596315224\n",
            "train loss:0.052232110985829444\n",
            "train loss:0.020840516269238787\n",
            "train loss:0.03805384378978733\n",
            "train loss:0.048122407852184204\n",
            "train loss:0.0688680998644983\n",
            "train loss:0.015433136436383102\n",
            "train loss:0.02013151340993894\n",
            "train loss:0.06316469624079488\n",
            "train loss:0.056697205675810866\n",
            "train loss:0.02933670594783206\n",
            "train loss:0.0628344717839683\n",
            "train loss:0.0396064068689105\n",
            "train loss:0.06431970255256005\n",
            "train loss:0.12680346362890924\n",
            "train loss:0.054735816018718735\n",
            "train loss:0.030014671522544497\n",
            "train loss:0.10792778160697036\n",
            "train loss:0.013162264305548977\n",
            "train loss:0.05269312420922428\n",
            "train loss:0.01804737327248824\n",
            "train loss:0.030294518365696757\n",
            "train loss:0.0064807778562616354\n",
            "train loss:0.08557666461948489\n",
            "train loss:0.0337165138670273\n",
            "train loss:0.08599555936324428\n",
            "train loss:0.07804893394905067\n",
            "train loss:0.02529640983645876\n",
            "train loss:0.06941451999428212\n",
            "train loss:0.040312475648084115\n",
            "train loss:0.06077766445450077\n",
            "train loss:0.027001894316106622\n",
            "train loss:0.06691403154864979\n",
            "train loss:0.061058762094993904\n",
            "train loss:0.06548735420332616\n",
            "train loss:0.05134385584765137\n",
            "train loss:0.04223058268248543\n",
            "train loss:0.04026766006363211\n",
            "train loss:0.02938100201763148\n",
            "train loss:0.066533392594366\n",
            "train loss:0.04820193275220598\n",
            "train loss:0.06153564525755467\n",
            "train loss:0.0691980996483042\n",
            "train loss:0.06870422678039322\n",
            "train loss:0.09605272755413306\n",
            "train loss:0.07458501082366699\n",
            "train loss:0.0715092215955791\n",
            "train loss:0.03861040455875298\n",
            "train loss:0.047158298668372026\n",
            "train loss:0.02654694375894733\n",
            "train loss:0.02565202880917521\n",
            "train loss:0.0662275626211566\n",
            "train loss:0.05012161737866561\n",
            "train loss:0.030589400781496474\n",
            "train loss:0.02936547337600716\n",
            "train loss:0.04659915629808338\n",
            "train loss:0.030754510423490235\n",
            "train loss:0.03179231720701911\n",
            "train loss:0.07779435694766265\n",
            "train loss:0.059201842198925304\n",
            "train loss:0.05509153245180834\n",
            "train loss:0.0774370326027003\n",
            "train loss:0.042832264819657434\n",
            "train loss:0.03780503852804951\n",
            "train loss:0.03165103861560022\n",
            "train loss:0.051307166653752\n",
            "train loss:0.025617031117153678\n",
            "train loss:0.06285567770223201\n",
            "train loss:0.0486866633112386\n",
            "train loss:0.07686216746335883\n",
            "train loss:0.07929067007697928\n",
            "train loss:0.023228661429972566\n",
            "train loss:0.06590768319599746\n",
            "train loss:0.03502509185469693\n",
            "train loss:0.06303014855541474\n",
            "train loss:0.09586427078485317\n",
            "train loss:0.04990899691196618\n",
            "train loss:0.035047717455205914\n",
            "train loss:0.07063537915522292\n",
            "train loss:0.06888707887646472\n",
            "train loss:0.04200918059110317\n",
            "train loss:0.04176782403923229\n",
            "train loss:0.029590088506299395\n",
            "train loss:0.05097251524326969\n",
            "train loss:0.0425474486287075\n",
            "train loss:0.060148050238573766\n",
            "train loss:0.030957216524390173\n",
            "train loss:0.07461485136851671\n",
            "train loss:0.04349478582746932\n",
            "train loss:0.10609659030894499\n",
            "train loss:0.06536566838517642\n",
            "=== epoch:15, train acc:0.977, test acc:0.895 ===\n",
            "train loss:0.02928789074593347\n",
            "train loss:0.07965131289716199\n",
            "train loss:0.06871847068101981\n",
            "train loss:0.050297269691868554\n",
            "train loss:0.032013886544502726\n",
            "train loss:0.01936488776928863\n",
            "train loss:0.06987291095912614\n",
            "train loss:0.06222552791250064\n",
            "train loss:0.05511790749033715\n",
            "train loss:0.03684212745636119\n",
            "train loss:0.03496067914939461\n",
            "train loss:0.05641350944026528\n",
            "train loss:0.07431565990502975\n",
            "train loss:0.057609080446520806\n",
            "train loss:0.047520766345241716\n",
            "train loss:0.0218299989088795\n",
            "train loss:0.09831863068708545\n",
            "train loss:0.03391767580595843\n",
            "train loss:0.09726412612650098\n",
            "train loss:0.044957889820489975\n",
            "train loss:0.01617293563266732\n",
            "train loss:0.08778339489746012\n",
            "train loss:0.04137109391436797\n",
            "train loss:0.05337681059166707\n",
            "train loss:0.047385716961988435\n",
            "train loss:0.024607333468549215\n",
            "train loss:0.025897963034798008\n",
            "train loss:0.029242470069854108\n",
            "train loss:0.040025179127173516\n",
            "train loss:0.019305246842871337\n",
            "train loss:0.07512180051808746\n",
            "train loss:0.03856458903555898\n",
            "train loss:0.03079265638355354\n",
            "train loss:0.06055196177896783\n",
            "train loss:0.05412257617494416\n",
            "train loss:0.05008761386552726\n",
            "train loss:0.04342794200068376\n",
            "train loss:0.04004296632297252\n",
            "train loss:0.06684518843994053\n",
            "train loss:0.16050638758590835\n",
            "train loss:0.06169321664949665\n",
            "train loss:0.11131817947976747\n",
            "train loss:0.11768482967098405\n",
            "train loss:0.0884967756510255\n",
            "train loss:0.07801631540809968\n",
            "train loss:0.03051821784914098\n",
            "train loss:0.02583131277563112\n",
            "train loss:0.06422963723155008\n",
            "train loss:0.06998660301938783\n",
            "train loss:0.018970215874567695\n",
            "train loss:0.02369598877475807\n",
            "train loss:0.0494454121968692\n",
            "train loss:0.03979667498027798\n",
            "train loss:0.042279651449871086\n",
            "train loss:0.08612819661014873\n",
            "train loss:0.011887305879746924\n",
            "train loss:0.06951154017785144\n",
            "train loss:0.07264094645764149\n",
            "train loss:0.04602950953569024\n",
            "train loss:0.03510828451471639\n",
            "train loss:0.02400760833005768\n",
            "train loss:0.01287936605798454\n",
            "train loss:0.09930893989758265\n",
            "train loss:0.035118547346113325\n",
            "train loss:0.0387545021521471\n",
            "train loss:0.02110015399552486\n",
            "train loss:0.03717307923326305\n",
            "train loss:0.04470423006525939\n",
            "train loss:0.0317323860654684\n",
            "train loss:0.029125425103242723\n",
            "train loss:0.018865039694398743\n",
            "train loss:0.03208886362861198\n",
            "train loss:0.07584596807455166\n",
            "train loss:0.05296899765163712\n",
            "train loss:0.07903883781804615\n",
            "train loss:0.05058770222383584\n",
            "train loss:0.011950671686824652\n",
            "train loss:0.051909204226964094\n",
            "train loss:0.015309495461177642\n",
            "train loss:0.07515769565773409\n",
            "train loss:0.056234928137062525\n",
            "train loss:0.11438599723937842\n",
            "train loss:0.16267529455046514\n",
            "train loss:0.05301797252341375\n",
            "train loss:0.030903122092032193\n",
            "train loss:0.056230031342346554\n",
            "train loss:0.022039741828577847\n",
            "train loss:0.059928899943395335\n",
            "train loss:0.0507455651845839\n",
            "train loss:0.09708363334865763\n",
            "train loss:0.07412385454057249\n",
            "train loss:0.059705224370371825\n",
            "train loss:0.0777327160403053\n",
            "train loss:0.023246837170322217\n",
            "train loss:0.026039966749294426\n",
            "train loss:0.024077617491755153\n",
            "train loss:0.07546366641633964\n",
            "train loss:0.058728803864678845\n",
            "train loss:0.05687221193954458\n",
            "train loss:0.05832704287049202\n",
            "train loss:0.05491549819696675\n",
            "train loss:0.025555648280307514\n",
            "train loss:0.0761758493384901\n",
            "train loss:0.052458710397747\n",
            "train loss:0.04367894721498659\n",
            "train loss:0.06202368023996416\n",
            "train loss:0.041465758393051166\n",
            "train loss:0.03148367866813296\n",
            "train loss:0.06475321442711576\n",
            "train loss:0.01982895950798349\n",
            "train loss:0.02513917743261941\n",
            "train loss:0.051071572914067304\n",
            "train loss:0.030803075589379267\n",
            "train loss:0.02359112382257535\n",
            "train loss:0.051575919647928445\n",
            "train loss:0.05444148651225594\n",
            "train loss:0.06666873952715853\n",
            "train loss:0.03185708000798293\n",
            "train loss:0.02092119059075156\n",
            "train loss:0.025478751851880782\n",
            "train loss:0.13557042047956044\n",
            "train loss:0.04022687775808038\n",
            "train loss:0.061284268088883334\n",
            "train loss:0.13579104871303038\n",
            "train loss:0.1265461162612952\n",
            "train loss:0.07919191651341248\n",
            "train loss:0.0430912666481392\n",
            "train loss:0.042285342975925025\n",
            "train loss:0.06933922725335019\n",
            "train loss:0.06534155073467329\n",
            "train loss:0.0421725617328234\n",
            "train loss:0.02650140047454632\n",
            "train loss:0.04876599943022278\n",
            "train loss:0.033779523010697365\n",
            "train loss:0.0275501981320835\n",
            "train loss:0.07988348757229885\n",
            "train loss:0.03469541650856874\n",
            "train loss:0.03215702541695097\n",
            "train loss:0.08422082354093818\n",
            "train loss:0.010107099758815637\n",
            "train loss:0.016681288707851457\n",
            "train loss:0.05088225299729759\n",
            "train loss:0.027849704818291694\n",
            "train loss:0.07936911541133874\n",
            "train loss:0.04138053643235132\n",
            "train loss:0.13671687426107887\n",
            "train loss:0.03972684620563321\n",
            "train loss:0.04188531630339774\n",
            "train loss:0.036300890257377046\n",
            "train loss:0.028000998633158386\n",
            "train loss:0.045368804938537274\n",
            "train loss:0.026902830779849826\n",
            "train loss:0.04244920225945165\n",
            "train loss:0.045616768897791184\n",
            "train loss:0.04054859940711761\n",
            "train loss:0.02762461813762913\n",
            "train loss:0.01658936921124303\n",
            "train loss:0.028406539803896597\n",
            "train loss:0.03217872832989568\n",
            "train loss:0.02946943263542794\n",
            "train loss:0.028810250702109762\n",
            "train loss:0.01992798024370644\n",
            "train loss:0.06376388190057022\n",
            "train loss:0.008962311315023707\n",
            "train loss:0.05830812478275272\n",
            "train loss:0.025652558545878286\n",
            "train loss:0.06403308433620612\n",
            "train loss:0.0683317535094258\n",
            "train loss:0.0273401712806654\n",
            "train loss:0.03641717319232852\n",
            "train loss:0.029422170833978675\n",
            "train loss:0.03307901357344729\n",
            "train loss:0.0329657094291469\n",
            "train loss:0.06329531534959848\n",
            "train loss:0.09665574983546911\n",
            "train loss:0.09805842053735696\n",
            "train loss:0.020663754315055037\n",
            "train loss:0.040301127226011244\n",
            "train loss:0.12928982950201506\n",
            "train loss:0.057038546750601125\n",
            "train loss:0.013492309492766101\n",
            "train loss:0.0571384212276627\n",
            "train loss:0.0381243273437645\n",
            "train loss:0.04049699681101205\n",
            "train loss:0.0675294815889196\n",
            "train loss:0.07270185931466122\n",
            "train loss:0.04042593277588839\n",
            "train loss:0.053789129586412285\n",
            "train loss:0.03670989656937282\n",
            "train loss:0.03430751570506669\n",
            "train loss:0.0911536627810679\n",
            "train loss:0.09496785230391433\n",
            "train loss:0.08426884937783966\n",
            "train loss:0.03606397448168718\n",
            "train loss:0.020314835175973052\n",
            "train loss:0.04034663498205163\n",
            "train loss:0.048780586757239244\n",
            "train loss:0.017476874916088275\n",
            "train loss:0.06057903879916598\n",
            "train loss:0.03119510604445317\n",
            "train loss:0.026861409934447793\n",
            "train loss:0.04160959020902047\n",
            "train loss:0.06046929332664089\n",
            "train loss:0.040957438040912585\n",
            "train loss:0.022281416754248932\n",
            "train loss:0.03652335801596664\n",
            "train loss:0.03052099575269794\n",
            "train loss:0.0752829101890472\n",
            "train loss:0.059049987807023106\n",
            "train loss:0.04511888370923188\n",
            "train loss:0.05508262303513409\n",
            "train loss:0.034050443428940785\n",
            "train loss:0.07305734870543144\n",
            "train loss:0.045234567353019936\n",
            "train loss:0.0207600054372284\n",
            "train loss:0.04312891298415379\n",
            "train loss:0.058137193326690866\n",
            "train loss:0.08237132825536085\n",
            "train loss:0.04166693276294636\n",
            "train loss:0.0430852390640608\n",
            "train loss:0.031347302458769336\n",
            "train loss:0.06026061699549714\n",
            "train loss:0.0382438999617327\n",
            "train loss:0.045837716788074516\n",
            "train loss:0.05781341814908766\n",
            "train loss:0.04779520421721568\n",
            "train loss:0.04087846377523169\n",
            "train loss:0.045001838711110284\n",
            "train loss:0.0458071359030207\n",
            "train loss:0.02251044999152514\n",
            "train loss:0.039647861044999316\n",
            "train loss:0.08349155425400705\n",
            "train loss:0.026398292492746916\n",
            "train loss:0.035451349307179594\n",
            "train loss:0.025395647521601495\n",
            "train loss:0.008011619445386404\n",
            "train loss:0.016739217414084653\n",
            "train loss:0.02182477794544045\n",
            "train loss:0.022307933451870935\n",
            "train loss:0.08201545700375881\n",
            "train loss:0.06396462838910373\n",
            "train loss:0.042742752671497304\n",
            "train loss:0.03198031149472026\n",
            "train loss:0.046279241029816676\n",
            "train loss:0.05537676039765858\n",
            "train loss:0.07705590113651263\n",
            "train loss:0.015876938338862966\n",
            "train loss:0.029213445869895417\n",
            "train loss:0.03801982518743217\n",
            "train loss:0.05597000313902682\n",
            "train loss:0.035077646478849346\n",
            "train loss:0.03163828569000958\n",
            "train loss:0.05586003547944175\n",
            "train loss:0.07404190449385217\n",
            "train loss:0.05526449452394202\n",
            "train loss:0.02377389710246187\n",
            "train loss:0.03776889377751688\n",
            "train loss:0.0798917093888032\n",
            "train loss:0.05115162927611638\n",
            "train loss:0.018679832171318158\n",
            "train loss:0.1066747465072843\n",
            "train loss:0.017180842401810448\n",
            "train loss:0.054349837920185225\n",
            "train loss:0.05835471397226136\n",
            "train loss:0.05355587963459289\n",
            "train loss:0.027579241988627438\n",
            "train loss:0.027027138327862254\n",
            "train loss:0.14783185612083008\n",
            "train loss:0.031650054424514795\n",
            "train loss:0.0528869466993352\n",
            "train loss:0.05022184563375559\n",
            "train loss:0.15038981658411296\n",
            "train loss:0.04586346873651861\n",
            "train loss:0.03240157454666079\n",
            "train loss:0.022078238161561767\n",
            "train loss:0.01608114216667271\n",
            "train loss:0.0491641632773653\n",
            "train loss:0.0192882610352035\n",
            "train loss:0.019109108799877785\n",
            "train loss:0.028086543811181797\n",
            "train loss:0.09282882832969051\n",
            "train loss:0.05566073369049938\n",
            "train loss:0.03437083956482222\n",
            "train loss:0.01820712213550396\n",
            "train loss:0.033227823504731926\n",
            "train loss:0.03466641851941496\n",
            "train loss:0.03907994674537206\n",
            "train loss:0.0561531832958392\n",
            "train loss:0.034284520370333714\n",
            "train loss:0.05179348822417369\n",
            "train loss:0.04424587690687991\n",
            "train loss:0.06137162063132779\n",
            "train loss:0.04439474440579362\n",
            "train loss:0.028910954882201394\n",
            "train loss:0.05124904971934319\n",
            "train loss:0.0270711589059322\n",
            "train loss:0.08154004771118137\n",
            "train loss:0.02521337368548009\n",
            "train loss:0.08066143951341215\n",
            "train loss:0.07655766541184153\n",
            "train loss:0.007308603589923778\n",
            "train loss:0.04436477852203715\n",
            "train loss:0.032451325603672865\n",
            "train loss:0.03682842771924958\n",
            "train loss:0.06619191498752183\n",
            "train loss:0.06511731399810669\n",
            "train loss:0.04789196338071125\n",
            "train loss:0.057349510560483444\n",
            "train loss:0.04382996352261842\n",
            "train loss:0.03402637363764452\n",
            "train loss:0.030547862201068553\n",
            "train loss:0.06710958248286629\n",
            "train loss:0.07138670530270465\n",
            "train loss:0.024125960689913618\n",
            "train loss:0.05923170100530773\n",
            "train loss:0.050564875640252055\n",
            "train loss:0.0941396793564218\n",
            "train loss:0.04031198365870361\n",
            "train loss:0.05606202925678493\n",
            "train loss:0.020014205107531\n",
            "train loss:0.031130741232082655\n",
            "train loss:0.04552513546025522\n",
            "train loss:0.0743112783639399\n",
            "train loss:0.018867979592038703\n",
            "train loss:0.03858464625347669\n",
            "train loss:0.07343604311129118\n",
            "train loss:0.0424808451012337\n",
            "train loss:0.027879021351235277\n",
            "train loss:0.05906023710332347\n",
            "train loss:0.011950702589067694\n",
            "train loss:0.02937294063415184\n",
            "train loss:0.014426437446420684\n",
            "train loss:0.03774661688409074\n",
            "train loss:0.072965159450232\n",
            "train loss:0.05229430025427443\n",
            "train loss:0.08558414103096337\n",
            "train loss:0.04781378821267153\n",
            "train loss:0.041757941673217\n",
            "train loss:0.0376005998071563\n",
            "train loss:0.039747507365521934\n",
            "train loss:0.03993667464672448\n",
            "train loss:0.06705591368854863\n",
            "train loss:0.02296606738266682\n",
            "train loss:0.017949562536174312\n",
            "train loss:0.024888003639397503\n",
            "train loss:0.02233374781221131\n",
            "train loss:0.035716853597587306\n",
            "train loss:0.05096235519583165\n",
            "train loss:0.019500450838079483\n",
            "train loss:0.0222386859023322\n",
            "train loss:0.0518382731833878\n",
            "train loss:0.04625973101242757\n",
            "train loss:0.055546380857723926\n",
            "train loss:0.03635112958924139\n",
            "train loss:0.030780450762397903\n",
            "train loss:0.029830516817570915\n",
            "train loss:0.03674782532977724\n",
            "train loss:0.019936817052047816\n",
            "train loss:0.009229784284597245\n",
            "train loss:0.027522234775959248\n",
            "train loss:0.03739254249742008\n",
            "train loss:0.07936091232892988\n",
            "train loss:0.06929674444549358\n",
            "train loss:0.08450241757889461\n",
            "train loss:0.024035387388770656\n",
            "train loss:0.024585772315982177\n",
            "train loss:0.07166983190548365\n",
            "train loss:0.024685875083935886\n",
            "train loss:0.04475385104984821\n",
            "train loss:0.04203838123663358\n",
            "train loss:0.019961816699377296\n",
            "train loss:0.060845045086056045\n",
            "train loss:0.04071484112014199\n",
            "train loss:0.038030518202600255\n",
            "train loss:0.051227265020967624\n",
            "train loss:0.0425836672444105\n",
            "train loss:0.09514285962666574\n",
            "train loss:0.019356731159778394\n",
            "train loss:0.03234596211091251\n",
            "train loss:0.02603975485481679\n",
            "train loss:0.0356332200078539\n",
            "train loss:0.059849659507126604\n",
            "train loss:0.04613746931871033\n",
            "train loss:0.06773459001426221\n",
            "train loss:0.05804078130736141\n",
            "train loss:0.019844185007484444\n",
            "train loss:0.10786261091108117\n",
            "train loss:0.03750480360465823\n",
            "train loss:0.05386667165721543\n",
            "train loss:0.06587232072210553\n",
            "train loss:0.030490425950705915\n",
            "train loss:0.036884120644231695\n",
            "train loss:0.040869541684435774\n",
            "train loss:0.08686207313943871\n",
            "train loss:0.07186999491900554\n",
            "train loss:0.04871147307404328\n",
            "train loss:0.05443211085448736\n",
            "train loss:0.02930148706280816\n",
            "train loss:0.03458130520693419\n",
            "train loss:0.013982264155474253\n",
            "train loss:0.035710306100032035\n",
            "train loss:0.05032442401163486\n",
            "train loss:0.06776050602650044\n",
            "train loss:0.04696617887873703\n",
            "train loss:0.06354845169023689\n",
            "train loss:0.06476563028337484\n",
            "train loss:0.0329155688277122\n",
            "train loss:0.045717463442594974\n",
            "train loss:0.019135591516754126\n",
            "train loss:0.06259129579288163\n",
            "train loss:0.08928273162474203\n",
            "train loss:0.0254887440157067\n",
            "train loss:0.08838125032519795\n",
            "train loss:0.07020767228169633\n",
            "train loss:0.024666818452492828\n",
            "train loss:0.04149808481205088\n",
            "train loss:0.007157679134129315\n",
            "train loss:0.02610753289906123\n",
            "train loss:0.05181819384308443\n",
            "train loss:0.04766945827211552\n",
            "train loss:0.017954634661073537\n",
            "train loss:0.23498288349121463\n",
            "train loss:0.04082015848826217\n",
            "train loss:0.03139963171907235\n",
            "train loss:0.04798609254379265\n",
            "train loss:0.047303974495229315\n",
            "train loss:0.05620952067707855\n",
            "train loss:0.03372746743146558\n",
            "train loss:0.02160670272937026\n",
            "train loss:0.11638862828323576\n",
            "train loss:0.012459102791653454\n",
            "train loss:0.043463641302305345\n",
            "train loss:0.051613769420885164\n",
            "train loss:0.06553981316795705\n",
            "train loss:0.010273579564880066\n",
            "train loss:0.03851457653468606\n",
            "train loss:0.03854074073425349\n",
            "train loss:0.06291399973284137\n",
            "train loss:0.02469619748573307\n",
            "train loss:0.028057762877481643\n",
            "train loss:0.021703362138873635\n",
            "train loss:0.04196580302653128\n",
            "train loss:0.07258731341194495\n",
            "train loss:0.02387590609504891\n",
            "train loss:0.06775088363552197\n",
            "train loss:0.020578910288357323\n",
            "train loss:0.03944146412403025\n",
            "train loss:0.08629248235799812\n",
            "train loss:0.06595342627129758\n",
            "train loss:0.027835056778244112\n",
            "train loss:0.08398061811626877\n",
            "train loss:0.12477167874386438\n",
            "train loss:0.08356506796510349\n",
            "train loss:0.0468608120455003\n",
            "train loss:0.045468638363387194\n",
            "train loss:0.0604809569456515\n",
            "train loss:0.054244568858969805\n",
            "train loss:0.0139411455354088\n",
            "train loss:0.07165066299706847\n",
            "train loss:0.025778304394022036\n",
            "train loss:0.02563241855528069\n",
            "train loss:0.053471557912921276\n",
            "train loss:0.06553737132614466\n",
            "train loss:0.03390231742102969\n",
            "train loss:0.020840669229788663\n",
            "train loss:0.09579036393655016\n",
            "train loss:0.04179158075484971\n",
            "train loss:0.0936964708023464\n",
            "train loss:0.06922240676070306\n",
            "train loss:0.03629194295146826\n",
            "train loss:0.0331526048629831\n",
            "train loss:0.012840736566022849\n",
            "train loss:0.05724365618864265\n",
            "train loss:0.03869085922365599\n",
            "train loss:0.02743145851050594\n",
            "train loss:0.02783032593055828\n",
            "train loss:0.020813822879591486\n",
            "train loss:0.019482386968957873\n",
            "train loss:0.014747253024406482\n",
            "train loss:0.03699116491640423\n",
            "train loss:0.07748216641110577\n",
            "train loss:0.062371913471451046\n",
            "train loss:0.020030134803349225\n",
            "train loss:0.042477732856064175\n",
            "train loss:0.049413324518968474\n",
            "train loss:0.03592102716429157\n",
            "train loss:0.016561849716293188\n",
            "train loss:0.04573802341282693\n",
            "train loss:0.04955254881499457\n",
            "train loss:0.029352882127531083\n",
            "train loss:0.1395349698115522\n",
            "train loss:0.05361241199730937\n",
            "train loss:0.04974681133360541\n",
            "train loss:0.042921678335013765\n",
            "train loss:0.05840274730460183\n",
            "train loss:0.01975430301166128\n",
            "train loss:0.02505042157969523\n",
            "train loss:0.029628014532557017\n",
            "train loss:0.03626796799114165\n",
            "train loss:0.0555670149410815\n",
            "train loss:0.04549213589819103\n",
            "train loss:0.01847154181700372\n",
            "train loss:0.028192761617138032\n",
            "train loss:0.05796089819555844\n",
            "train loss:0.024172583752687826\n",
            "train loss:0.029250009437080723\n",
            "train loss:0.04461550900322873\n",
            "train loss:0.03745447752556206\n",
            "train loss:0.06871990509529154\n",
            "train loss:0.03493167461861056\n",
            "train loss:0.03247729230184981\n",
            "train loss:0.031560121083498684\n",
            "train loss:0.017285271799154206\n",
            "train loss:0.05165645980246532\n",
            "train loss:0.026223056560549175\n",
            "train loss:0.026575875931754436\n",
            "train loss:0.10460472285778989\n",
            "train loss:0.01649274728116966\n",
            "train loss:0.020178627974506345\n",
            "train loss:0.1308124800664788\n",
            "train loss:0.08564032980873498\n",
            "train loss:0.05076580555013628\n",
            "train loss:0.04535177085576197\n",
            "train loss:0.04203529449578666\n",
            "train loss:0.025597734804486197\n",
            "train loss:0.02098654705570098\n",
            "train loss:0.04129812494516884\n",
            "train loss:0.052436093943009396\n",
            "train loss:0.024503922484286244\n",
            "train loss:0.02427010653679061\n",
            "train loss:0.0328717330355844\n",
            "train loss:0.022396234764069312\n",
            "train loss:0.06278529215516691\n",
            "train loss:0.027646257900783656\n",
            "train loss:0.06580803607437344\n",
            "train loss:0.050507284872509176\n",
            "train loss:0.01983302536548717\n",
            "train loss:0.10064445111130427\n",
            "train loss:0.019405121509079713\n",
            "train loss:0.03317683739149956\n",
            "train loss:0.11370750714909375\n",
            "train loss:0.0688720872072715\n",
            "train loss:0.0474451083120594\n",
            "train loss:0.0888413169689966\n",
            "train loss:0.029155223924137802\n",
            "train loss:0.07028538998104839\n",
            "train loss:0.026168920004414287\n",
            "train loss:0.07276380395759417\n",
            "train loss:0.03902395794069509\n",
            "train loss:0.06936980176499218\n",
            "train loss:0.040608752367653045\n",
            "train loss:0.025804610401052565\n",
            "train loss:0.02996896411167611\n",
            "train loss:0.01405508355975287\n",
            "train loss:0.07292532223246066\n",
            "train loss:0.03711529666382655\n",
            "train loss:0.052333811719738256\n",
            "train loss:0.020539150715964127\n",
            "train loss:0.020523018627055092\n",
            "train loss:0.01837109268267409\n",
            "train loss:0.020765491396987718\n",
            "train loss:0.0944924443188836\n",
            "train loss:0.035765337560369974\n",
            "train loss:0.020127617270043006\n",
            "train loss:0.038109020851665\n",
            "train loss:0.05941286711447953\n",
            "train loss:0.03640316368438107\n",
            "train loss:0.058104921535865196\n",
            "train loss:0.020834483713467664\n",
            "train loss:0.03612468160982933\n",
            "train loss:0.037068016701046355\n",
            "train loss:0.030267673476709823\n",
            "train loss:0.03727062227784713\n",
            "train loss:0.06302378826909033\n",
            "train loss:0.04545706565848905\n",
            "train loss:0.06031299298449363\n",
            "train loss:0.056978001381492005\n",
            "train loss:0.016385222566647418\n",
            "train loss:0.07336947251215799\n",
            "train loss:0.03131958214906623\n",
            "train loss:0.03215754153934159\n",
            "train loss:0.026275683836261537\n",
            "train loss:0.060194262081141786\n",
            "train loss:0.15225480609349473\n",
            "train loss:0.08010405419653116\n",
            "train loss:0.04251225280597576\n",
            "train loss:0.03191121754597378\n",
            "train loss:0.024189321064703027\n",
            "train loss:0.024256781845678463\n",
            "train loss:0.03265495019319213\n",
            "train loss:0.0179778511485526\n",
            "train loss:0.024450062168095917\n",
            "train loss:0.07901539383682908\n",
            "train loss:0.04519746528660863\n",
            "train loss:0.0502733662057837\n",
            "train loss:0.030057129900906466\n",
            "train loss:0.04239643895223396\n",
            "train loss:0.02538640754001341\n",
            "train loss:0.03156472283188208\n",
            "train loss:0.07689754723864335\n",
            "=== epoch:16, train acc:0.981, test acc:0.9 ===\n",
            "train loss:0.030939249340063\n",
            "train loss:0.02162454655511036\n",
            "train loss:0.042347435123858544\n",
            "train loss:0.02693440696595809\n",
            "train loss:0.08052947901403396\n",
            "train loss:0.039380386627047397\n",
            "train loss:0.007325358778109677\n",
            "train loss:0.021950527339028598\n",
            "train loss:0.02307678638516827\n",
            "train loss:0.028911698043192605\n",
            "train loss:0.04546583717987119\n",
            "train loss:0.14235405849343727\n",
            "train loss:0.07358898099041306\n",
            "train loss:0.030120728587297797\n",
            "train loss:0.036726695692195416\n",
            "train loss:0.0372911900471338\n",
            "train loss:0.02327948428199557\n",
            "train loss:0.045741480667902824\n",
            "train loss:0.019847572940202486\n",
            "train loss:0.057164822425916556\n",
            "train loss:0.026257801312986072\n",
            "train loss:0.030511732032165794\n",
            "train loss:0.07258175404817155\n",
            "train loss:0.0402791737536702\n",
            "train loss:0.08449299176215677\n",
            "train loss:0.044563136719207336\n",
            "train loss:0.10510591562094168\n",
            "train loss:0.029943514129768308\n",
            "train loss:0.032364777699292545\n",
            "train loss:0.013952852438474151\n",
            "train loss:0.03877532924494312\n",
            "train loss:0.05682214837548266\n",
            "train loss:0.04411905012509879\n",
            "train loss:0.06388288591589539\n",
            "train loss:0.08272278902932494\n",
            "train loss:0.026589377094600613\n",
            "train loss:0.05264842654793038\n",
            "train loss:0.02205436929895835\n",
            "train loss:0.04615276097340682\n",
            "train loss:0.010627117874608925\n",
            "train loss:0.016134491373250532\n",
            "train loss:0.04312246095426717\n",
            "train loss:0.04355321093109162\n",
            "train loss:0.01804578840535171\n",
            "train loss:0.016379899717057826\n",
            "train loss:0.035283726284061075\n",
            "train loss:0.06082620540794493\n",
            "train loss:0.06456035599832381\n",
            "train loss:0.03865442852007768\n",
            "train loss:0.07532491557128738\n",
            "train loss:0.08913546727851378\n",
            "train loss:0.015710486454602914\n",
            "train loss:0.06311845728211989\n",
            "train loss:0.03914223750726623\n",
            "train loss:0.020458075124743857\n",
            "train loss:0.03821283065589716\n",
            "train loss:0.023780934930328054\n",
            "train loss:0.0515465170871713\n",
            "train loss:0.03598519678704408\n",
            "train loss:0.01975662160734941\n",
            "train loss:0.0554454066330774\n",
            "train loss:0.019880275724241597\n",
            "train loss:0.03516800398409848\n",
            "train loss:0.04790623222418415\n",
            "train loss:0.049133891113384856\n",
            "train loss:0.03554977420720627\n",
            "train loss:0.07375619281301908\n",
            "train loss:0.0292059104475914\n",
            "train loss:0.03958185391647088\n",
            "train loss:0.02641609794887888\n",
            "train loss:0.01246767522040558\n",
            "train loss:0.04642521275485379\n",
            "train loss:0.05542918160136779\n",
            "train loss:0.035552018154415346\n",
            "train loss:0.03306137884379836\n",
            "train loss:0.02592416512332604\n",
            "train loss:0.013924026241170668\n",
            "train loss:0.02457448288696292\n",
            "train loss:0.1009518257858945\n",
            "train loss:0.03640672480708122\n",
            "train loss:0.05629443790852475\n",
            "train loss:0.016778962900651187\n",
            "train loss:0.025230462056944812\n",
            "train loss:0.01940049873913713\n",
            "train loss:0.05137795535341767\n",
            "train loss:0.024641788745169677\n",
            "train loss:0.05004558002088943\n",
            "train loss:0.04063271640602705\n",
            "train loss:0.04263048195365594\n",
            "train loss:0.04862450021809729\n",
            "train loss:0.006398008825259874\n",
            "train loss:0.015064224676755542\n",
            "train loss:0.0180677198499681\n",
            "train loss:0.030623452331579558\n",
            "train loss:0.009360555432014937\n",
            "train loss:0.017606195661090642\n",
            "train loss:0.03309119439774032\n",
            "train loss:0.013318824192150207\n",
            "train loss:0.07244650831395064\n",
            "train loss:0.01862844013676529\n",
            "train loss:0.06834128866488527\n",
            "train loss:0.05040494742331236\n",
            "train loss:0.03692702650918806\n",
            "train loss:0.011808421603125906\n",
            "train loss:0.02496410950346853\n",
            "train loss:0.05341387868283614\n",
            "train loss:0.21895110874632015\n",
            "train loss:0.057531111370771784\n",
            "train loss:0.0182235389689474\n",
            "train loss:0.03365959903636334\n",
            "train loss:0.008731356065220323\n",
            "train loss:0.0334675293757806\n",
            "train loss:0.02467185491748306\n",
            "train loss:0.05490067061607153\n",
            "train loss:0.02942227321237544\n",
            "train loss:0.0699531618975956\n",
            "train loss:0.022698075517964565\n",
            "train loss:0.014806777978615326\n",
            "train loss:0.044901234002118554\n",
            "train loss:0.0651889049839654\n",
            "train loss:0.04888322050878499\n",
            "train loss:0.007176811893828147\n",
            "train loss:0.030255252493098957\n",
            "train loss:0.02954362313077757\n",
            "train loss:0.031305351275805894\n",
            "train loss:0.035921931269603385\n",
            "train loss:0.04423350354824225\n",
            "train loss:0.0146221205673044\n",
            "train loss:0.036993246742930336\n",
            "train loss:0.037232010817111444\n",
            "train loss:0.07730155733872128\n",
            "train loss:0.026698880247640368\n",
            "train loss:0.07392694249196437\n",
            "train loss:0.021443081611393796\n",
            "train loss:0.011639885822327\n",
            "train loss:0.06353965549672493\n",
            "train loss:0.04024739274601651\n",
            "train loss:0.007969265013650561\n",
            "train loss:0.07296128979165913\n",
            "train loss:0.05076517437813974\n",
            "train loss:0.0535732281298317\n",
            "train loss:0.027582938148850857\n",
            "train loss:0.03360458059416242\n",
            "train loss:0.040858852671892966\n",
            "train loss:0.04085660898422796\n",
            "train loss:0.05198170732843836\n",
            "train loss:0.09001657744388386\n",
            "train loss:0.024981540474200082\n",
            "train loss:0.030950036440398617\n",
            "train loss:0.06351271288858606\n",
            "train loss:0.0419991596292134\n",
            "train loss:0.027898332114878355\n",
            "train loss:0.02508580062389868\n",
            "train loss:0.021904881440219806\n",
            "train loss:0.02544304500684548\n",
            "train loss:0.0507799420296554\n",
            "train loss:0.01441293625305914\n",
            "train loss:0.0459095969353096\n",
            "train loss:0.027321175496718683\n",
            "train loss:0.026199654168842054\n",
            "train loss:0.0566654021987037\n",
            "train loss:0.04014045730834484\n",
            "train loss:0.010752174703126376\n",
            "train loss:0.013478315921564468\n",
            "train loss:0.0462625795673507\n",
            "train loss:0.018786834621644214\n",
            "train loss:0.0441683668178058\n",
            "train loss:0.04314443987378224\n",
            "train loss:0.049092319312705406\n",
            "train loss:0.03580917768199187\n",
            "train loss:0.035179243229759655\n",
            "train loss:0.04457483666043556\n",
            "train loss:0.022309243724258875\n",
            "train loss:0.033855125316126576\n",
            "train loss:0.056954221912695495\n",
            "train loss:0.03082481289677943\n",
            "train loss:0.05951846311118683\n",
            "train loss:0.014704227449838349\n",
            "train loss:0.03543344470057084\n",
            "train loss:0.05816315268367713\n",
            "train loss:0.038320101630552356\n",
            "train loss:0.07373703962706955\n",
            "train loss:0.030761511383444316\n",
            "train loss:0.06885183270474453\n",
            "train loss:0.016796691491195104\n",
            "train loss:0.0217105010973069\n",
            "train loss:0.017559582933731506\n",
            "train loss:0.022705696158547912\n",
            "train loss:0.03627744693417419\n",
            "train loss:0.024474334728282313\n",
            "train loss:0.02083048251358811\n",
            "train loss:0.026399817250078218\n",
            "train loss:0.026338182546839083\n",
            "train loss:0.04551528784196484\n",
            "train loss:0.041552911807711874\n",
            "train loss:0.023476496377943547\n",
            "train loss:0.029598811482127934\n",
            "train loss:0.0322309850922679\n",
            "train loss:0.028126762654009933\n",
            "train loss:0.021400387518875033\n",
            "train loss:0.028812330479669235\n",
            "train loss:0.04003381579324698\n",
            "train loss:0.011666013523957092\n",
            "train loss:0.06433867901105562\n",
            "train loss:0.027432047477966067\n",
            "train loss:0.06147521863290275\n",
            "train loss:0.033183279221554195\n",
            "train loss:0.022223501085822363\n",
            "train loss:0.08978125561888134\n",
            "train loss:0.007171567812943544\n",
            "train loss:0.025440556356331214\n",
            "train loss:0.04111190871002826\n",
            "train loss:0.008726403855103905\n",
            "train loss:0.02362262264419474\n",
            "train loss:0.02584364082022485\n",
            "train loss:0.037723037515235355\n",
            "train loss:0.023077521091282097\n",
            "train loss:0.032147787168279436\n",
            "train loss:0.04621759490141129\n",
            "train loss:0.06232036136342506\n",
            "train loss:0.009450177391054497\n",
            "train loss:0.02860343353205825\n",
            "train loss:0.01953707681867782\n",
            "train loss:0.04110756533977445\n",
            "train loss:0.020906159770159558\n",
            "train loss:0.02000356243549422\n",
            "train loss:0.020697797720025383\n",
            "train loss:0.023935254997976777\n",
            "train loss:0.034535782515428846\n",
            "train loss:0.021745339599534907\n",
            "train loss:0.015636938450523663\n",
            "train loss:0.06279727572127186\n",
            "train loss:0.020629046135826765\n",
            "train loss:0.04539198542149685\n",
            "train loss:0.04442267508056581\n",
            "train loss:0.030847160068038254\n",
            "train loss:0.014235167381649596\n",
            "train loss:0.016821877003267646\n",
            "train loss:0.027577856337026408\n",
            "train loss:0.06167719587399979\n",
            "train loss:0.06929245673613144\n",
            "train loss:0.12354386631644722\n",
            "train loss:0.01858773909649267\n",
            "train loss:0.01937985554227776\n",
            "train loss:0.016889873806450614\n",
            "train loss:0.02227148950692419\n",
            "train loss:0.033017722780346204\n",
            "train loss:0.005395080538031204\n",
            "train loss:0.05666490460629258\n",
            "train loss:0.027572937658003883\n",
            "train loss:0.0723325744213114\n",
            "train loss:0.05118797462724088\n",
            "train loss:0.02087754601844697\n",
            "train loss:0.02428090827783837\n",
            "train loss:0.017044813656474284\n",
            "train loss:0.07507717616160353\n",
            "train loss:0.01474673406922908\n",
            "train loss:0.02641073501801068\n",
            "train loss:0.03659825895383318\n",
            "train loss:0.013020598634791653\n",
            "train loss:0.013542766756117644\n",
            "train loss:0.013640254544384181\n",
            "train loss:0.02186005822351365\n",
            "train loss:0.05903773885675812\n",
            "train loss:0.03765389799504268\n",
            "train loss:0.0510804299819126\n",
            "train loss:0.043623311676238875\n",
            "train loss:0.05385016755071996\n",
            "train loss:0.040604984426354296\n",
            "train loss:0.02116287662546013\n",
            "train loss:0.034879788755930535\n",
            "train loss:0.06398392771019666\n",
            "train loss:0.06422717704316079\n",
            "train loss:0.02107528596682603\n",
            "train loss:0.0402120984250367\n",
            "train loss:0.031566849731013306\n",
            "train loss:0.021016862604841115\n",
            "train loss:0.030061522072959104\n",
            "train loss:0.02519591371938864\n",
            "train loss:0.05390785037867034\n",
            "train loss:0.009497632360034378\n",
            "train loss:0.055137238292890585\n",
            "train loss:0.05154721127085458\n",
            "train loss:0.018299459624044948\n",
            "train loss:0.03560319996733584\n",
            "train loss:0.022958529841281838\n",
            "train loss:0.022961479979623788\n",
            "train loss:0.015054522942870035\n",
            "train loss:0.028261319491133023\n",
            "train loss:0.07826426274686463\n",
            "train loss:0.04644912091420161\n",
            "train loss:0.015433761834700022\n",
            "train loss:0.034950075714381176\n",
            "train loss:0.08088777099208175\n",
            "train loss:0.011315009731354743\n",
            "train loss:0.018203322990792397\n",
            "train loss:0.05987016582459991\n",
            "train loss:0.008843652744953008\n",
            "train loss:0.03726379150032735\n",
            "train loss:0.040738775661732285\n",
            "train loss:0.059188935031101314\n",
            "train loss:0.03187471596413662\n",
            "train loss:0.02388323434548592\n",
            "train loss:0.015087636245283555\n",
            "train loss:0.036566321235410565\n",
            "train loss:0.01193048798578311\n",
            "train loss:0.035800893993949125\n",
            "train loss:0.026054885576733248\n",
            "train loss:0.027462865919506484\n",
            "train loss:0.0356842195351948\n",
            "train loss:0.010340152028227256\n",
            "train loss:0.07543058597030826\n",
            "train loss:0.07722765079958807\n",
            "train loss:0.04355138085690181\n",
            "train loss:0.07241717005040252\n",
            "train loss:0.051390386764658215\n",
            "train loss:0.04671151726256422\n",
            "train loss:0.015622744576450647\n",
            "train loss:0.034093807861846154\n",
            "train loss:0.04147959937558265\n",
            "train loss:0.057656106010778835\n",
            "train loss:0.06350225717536503\n",
            "train loss:0.03422249969812121\n",
            "train loss:0.03718702583477888\n",
            "train loss:0.020502118919274087\n",
            "train loss:0.04681892373152004\n",
            "train loss:0.04067278386042733\n",
            "train loss:0.02398793914662756\n",
            "train loss:0.06302443189744325\n",
            "train loss:0.058411430162012924\n",
            "train loss:0.06591112491990925\n",
            "train loss:0.03787140248938621\n",
            "train loss:0.043687705442505005\n",
            "train loss:0.02794447640190266\n",
            "train loss:0.03807876740282873\n",
            "train loss:0.042257036250123815\n",
            "train loss:0.023753562092180415\n",
            "train loss:0.0481007118176728\n",
            "train loss:0.034985662930323826\n",
            "train loss:0.058254365113284995\n",
            "train loss:0.03777751721471975\n",
            "train loss:0.038907732222892004\n",
            "train loss:0.013521073950120541\n",
            "train loss:0.029099310744671546\n",
            "train loss:0.010509446194645531\n",
            "train loss:0.011013807493381815\n",
            "train loss:0.037716958667695144\n",
            "train loss:0.01625358701792782\n",
            "train loss:0.03377545809447949\n",
            "train loss:0.030140523332482863\n",
            "train loss:0.023204107513669886\n",
            "train loss:0.06604789753362907\n",
            "train loss:0.024188629637067725\n",
            "train loss:0.007468166429460699\n",
            "train loss:0.021797171338707063\n",
            "train loss:0.049127336877245166\n",
            "train loss:0.017163520723165217\n",
            "train loss:0.013098019505758996\n",
            "train loss:0.023374089279210874\n",
            "train loss:0.04144622644355806\n",
            "train loss:0.047849969736749945\n",
            "train loss:0.011496781700047976\n",
            "train loss:0.05068264982923968\n",
            "train loss:0.007036476722821993\n",
            "train loss:0.06783558877097753\n",
            "train loss:0.04548699392044377\n",
            "train loss:0.06326004502431697\n",
            "train loss:0.020558868525341726\n",
            "train loss:0.015600204120393608\n",
            "train loss:0.04261927320786162\n",
            "train loss:0.012049588488183494\n",
            "train loss:0.025873187139608164\n",
            "train loss:0.023340430685374832\n",
            "train loss:0.01388251905025999\n",
            "train loss:0.044088088963698556\n",
            "train loss:0.0603764899859571\n",
            "train loss:0.006679222013801691\n",
            "train loss:0.019542968258246113\n",
            "train loss:0.0206530734730034\n",
            "train loss:0.014925461178262954\n",
            "train loss:0.023537010581227563\n",
            "train loss:0.012110656272570514\n",
            "train loss:0.010246238434956912\n",
            "train loss:0.029929208675314838\n",
            "train loss:0.014090659760678104\n",
            "train loss:0.0166056094069136\n",
            "train loss:0.03381416462607848\n",
            "train loss:0.023355775959331554\n",
            "train loss:0.019367703617126756\n",
            "train loss:0.0602442667488532\n",
            "train loss:0.024231892174915597\n",
            "train loss:0.040799027262781416\n",
            "train loss:0.017879633658588256\n",
            "train loss:0.007783675680761729\n",
            "train loss:0.02805600774141508\n",
            "train loss:0.027846811982338304\n",
            "train loss:0.05117563594257126\n",
            "train loss:0.07711103854178397\n",
            "train loss:0.00896283701398778\n",
            "train loss:0.03315909381734563\n",
            "train loss:0.022064858522731923\n",
            "train loss:0.03487459692707679\n",
            "train loss:0.02092260261589396\n",
            "train loss:0.007554156210839479\n",
            "train loss:0.038408007511830496\n",
            "train loss:0.015185441410350472\n",
            "train loss:0.0665169777911427\n",
            "train loss:0.05874536539547792\n",
            "train loss:0.016022547897391534\n",
            "train loss:0.00849103210302609\n",
            "train loss:0.018833803999931467\n",
            "train loss:0.01782445527408479\n",
            "train loss:0.016484342062055606\n",
            "train loss:0.026804523549580364\n",
            "train loss:0.019501650670364523\n",
            "train loss:0.010283996355319342\n",
            "train loss:0.06072938730825153\n",
            "train loss:0.03895003133115241\n",
            "train loss:0.01223794540763328\n",
            "train loss:0.02068414776666387\n",
            "train loss:0.04203754072464863\n",
            "train loss:0.030703470481806727\n",
            "train loss:0.022782196288392877\n",
            "train loss:0.021596316506355327\n",
            "train loss:0.037796097949193884\n",
            "train loss:0.08552741889323232\n",
            "train loss:0.03417464242302319\n",
            "train loss:0.0613965298507404\n",
            "train loss:0.025102772920821165\n",
            "train loss:0.03239970971211897\n",
            "train loss:0.0301181029950666\n",
            "train loss:0.026827088741916062\n",
            "train loss:0.0771719572861639\n",
            "train loss:0.02937425202764897\n",
            "train loss:0.009457303132004291\n",
            "train loss:0.021209832134247514\n",
            "train loss:0.10088922602503562\n",
            "train loss:0.027044224143991148\n",
            "train loss:0.07164794791835737\n",
            "train loss:0.022844764594968692\n",
            "train loss:0.013891114199169566\n",
            "train loss:0.04509707836870329\n",
            "train loss:0.058391604805035316\n",
            "train loss:0.12266291390597318\n",
            "train loss:0.02037829798979208\n",
            "train loss:0.022071675357661667\n",
            "train loss:0.035383803454767694\n",
            "train loss:0.032385157850442534\n",
            "train loss:0.012871336938837259\n",
            "train loss:0.032274941018034096\n",
            "train loss:0.022051074839447116\n",
            "train loss:0.017154600538371402\n",
            "train loss:0.05660716411236466\n",
            "train loss:0.012980175382433084\n",
            "train loss:0.036181120953059025\n",
            "train loss:0.022062120274627606\n",
            "train loss:0.01966990983541563\n",
            "train loss:0.030521068147135337\n",
            "train loss:0.04499980692538613\n",
            "train loss:0.023876723961066392\n",
            "train loss:0.025553112137922253\n",
            "train loss:0.07025977794584244\n",
            "train loss:0.03314578268392744\n",
            "train loss:0.03783314741705118\n",
            "train loss:0.031814563195146625\n",
            "train loss:0.029347332498425072\n",
            "train loss:0.013898350185041037\n",
            "train loss:0.03075970426430701\n",
            "train loss:0.06498479559647033\n",
            "train loss:0.026762628922502137\n",
            "train loss:0.013555257151785328\n",
            "train loss:0.06123440791784085\n",
            "train loss:0.020983372135922912\n",
            "train loss:0.04696444593194914\n",
            "train loss:0.019734117479318213\n",
            "train loss:0.041561418354534904\n",
            "train loss:0.014257999862332943\n",
            "train loss:0.021746772346476123\n",
            "train loss:0.012426564584257118\n",
            "train loss:0.026471133095291966\n",
            "train loss:0.05800315177768485\n",
            "train loss:0.024412006121014403\n",
            "train loss:0.021563516364691036\n",
            "train loss:0.026772702361138028\n",
            "train loss:0.010440053836940154\n",
            "train loss:0.023874525969933173\n",
            "train loss:0.013568451381034357\n",
            "train loss:0.011262856670846557\n",
            "train loss:0.025248769663984018\n",
            "train loss:0.10422799466119191\n",
            "train loss:0.013563999742411213\n",
            "train loss:0.035927320094843965\n",
            "train loss:0.12566151542325155\n",
            "train loss:0.02543292970780006\n",
            "train loss:0.01682190451578433\n",
            "train loss:0.028380751534711184\n",
            "train loss:0.021495956319300027\n",
            "train loss:0.016124806401989177\n",
            "train loss:0.034918476004307294\n",
            "train loss:0.056048808622593906\n",
            "train loss:0.024748117147720294\n",
            "train loss:0.0351593492336187\n",
            "train loss:0.01387129763841683\n",
            "train loss:0.020377729239492895\n",
            "train loss:0.02482837237500292\n",
            "train loss:0.006671841844324445\n",
            "train loss:0.0256765015991315\n",
            "train loss:0.054563860181181305\n",
            "train loss:0.019690155050697364\n",
            "train loss:0.06691746908467254\n",
            "train loss:0.03704135959053701\n",
            "train loss:0.021350762282348498\n",
            "train loss:0.03550873554163912\n",
            "train loss:0.007360404360041853\n",
            "train loss:0.01664370883938708\n",
            "train loss:0.022801356793763307\n",
            "train loss:0.029267582594496595\n",
            "train loss:0.015989834886648804\n",
            "train loss:0.01056413628048829\n",
            "train loss:0.03750095407614381\n",
            "train loss:0.046635474641857695\n",
            "train loss:0.0488114388943626\n",
            "train loss:0.010687345247787165\n",
            "train loss:0.019680221829167774\n",
            "train loss:0.049704419468633194\n",
            "train loss:0.02919909191646503\n",
            "train loss:0.01813973131543229\n",
            "train loss:0.021987444156398892\n",
            "train loss:0.028289483426638756\n",
            "train loss:0.01925198962244744\n",
            "train loss:0.014164381457490865\n",
            "train loss:0.03506932039346175\n",
            "train loss:0.03970992827851422\n",
            "train loss:0.02966881005617306\n",
            "train loss:0.043791110724000146\n",
            "train loss:0.014159184924039947\n",
            "train loss:0.03972128473323436\n",
            "train loss:0.017451277154402294\n",
            "train loss:0.04132423646841201\n",
            "train loss:0.07513235057434656\n",
            "train loss:0.012336312789491696\n",
            "train loss:0.011663031384513587\n",
            "train loss:0.0704476002193827\n",
            "train loss:0.07411514384598761\n",
            "train loss:0.01263454956276806\n",
            "train loss:0.06365472582684309\n",
            "train loss:0.030774007320102954\n",
            "train loss:0.035685781571085054\n",
            "train loss:0.04254831618925462\n",
            "train loss:0.01857813389824601\n",
            "train loss:0.030297824339855266\n",
            "train loss:0.04827928131111228\n",
            "train loss:0.00978185742973316\n",
            "train loss:0.04181778056995512\n",
            "train loss:0.059991077012474865\n",
            "train loss:0.026557964309310168\n",
            "train loss:0.012356093243235866\n",
            "train loss:0.04412569441744022\n",
            "train loss:0.030150134735851403\n",
            "train loss:0.07073501298651276\n",
            "train loss:0.023519085894129294\n",
            "train loss:0.016622333902072888\n",
            "train loss:0.015609673563621812\n",
            "train loss:0.018693425244102637\n",
            "train loss:0.011655332532781146\n",
            "train loss:0.007494944152001006\n",
            "train loss:0.029286061277615207\n",
            "train loss:0.015471319572982709\n",
            "train loss:0.05400738159006292\n",
            "train loss:0.009567790805230588\n",
            "train loss:0.012661216196497278\n",
            "train loss:0.022065180682815395\n",
            "train loss:0.04909477145801706\n",
            "train loss:0.10748283498747851\n",
            "train loss:0.02511584277977429\n",
            "train loss:0.031148025824789722\n",
            "train loss:0.021431622441913073\n",
            "train loss:0.046975198109103944\n",
            "train loss:0.08102154793044854\n",
            "train loss:0.0503420398021619\n",
            "train loss:0.016054944332862357\n",
            "train loss:0.012380829272229452\n",
            "train loss:0.0663060714326707\n",
            "train loss:0.03453104256505152\n",
            "train loss:0.03878033149808201\n",
            "train loss:0.035536067852683574\n",
            "train loss:0.04735223138609061\n",
            "train loss:0.05800691932888934\n",
            "train loss:0.015370829189823423\n",
            "train loss:0.022633160502688317\n",
            "train loss:0.029198341246067724\n",
            "train loss:0.10421272656498778\n",
            "train loss:0.01523178544760365\n",
            "train loss:0.009473079156100343\n",
            "train loss:0.03336778618696652\n",
            "train loss:0.00956535838267878\n",
            "train loss:0.012823851317936117\n",
            "train loss:0.06421044624686459\n",
            "train loss:0.057668330675444475\n",
            "train loss:0.012182464306087932\n",
            "=== epoch:17, train acc:0.984, test acc:0.899 ===\n",
            "train loss:0.016139201208793662\n",
            "train loss:0.03514985070992586\n",
            "train loss:0.013389221287030741\n",
            "train loss:0.029529246240814683\n",
            "train loss:0.03099437021683925\n",
            "train loss:0.016919614272274446\n",
            "train loss:0.04814442306984005\n",
            "train loss:0.015749902480805968\n",
            "train loss:0.012883119797274006\n",
            "train loss:0.03170495758313465\n",
            "train loss:0.03245486907887094\n",
            "train loss:0.0475231142831156\n",
            "train loss:0.03614088896949491\n",
            "train loss:0.024458315461309565\n",
            "train loss:0.02269538527159176\n",
            "train loss:0.0693206672529867\n",
            "train loss:0.03972679364566971\n",
            "train loss:0.03115633523474605\n",
            "train loss:0.033900744717668\n",
            "train loss:0.0815029342015211\n",
            "train loss:0.020343881885178817\n",
            "train loss:0.012798982852559487\n",
            "train loss:0.04156954067429701\n",
            "train loss:0.0161451920870899\n",
            "train loss:0.012898393337684177\n",
            "train loss:0.027969219031704393\n",
            "train loss:0.0489774331792418\n",
            "train loss:0.00962115855080211\n",
            "train loss:0.06522197063901848\n",
            "train loss:0.02623044844613582\n",
            "train loss:0.01633463009644741\n",
            "train loss:0.03594260481769143\n",
            "train loss:0.03394902489075146\n",
            "train loss:0.0189927452204909\n",
            "train loss:0.01950684791511276\n",
            "train loss:0.03879943968189816\n",
            "train loss:0.03305606963885189\n",
            "train loss:0.028911594579945632\n",
            "train loss:0.034860101249647384\n",
            "train loss:0.028911470129652974\n",
            "train loss:0.03591250286415976\n",
            "train loss:0.03885626758779462\n",
            "train loss:0.029186836210954838\n",
            "train loss:0.008411261618762527\n",
            "train loss:0.06741061328069654\n",
            "train loss:0.0826348580123471\n",
            "train loss:0.012365364509858206\n",
            "train loss:0.011250632870661142\n",
            "train loss:0.05325821457897946\n",
            "train loss:0.01711411944769953\n",
            "train loss:0.08774033500742322\n",
            "train loss:0.012449309705441544\n",
            "train loss:0.04750375799178504\n",
            "train loss:0.015204452848164171\n",
            "train loss:0.03001708884658738\n",
            "train loss:0.027399157635086865\n",
            "train loss:0.03968558553128584\n",
            "train loss:0.0966722826179141\n",
            "train loss:0.04550087984066129\n",
            "train loss:0.077922899363139\n",
            "train loss:0.03768265476815891\n",
            "train loss:0.02530097700030295\n",
            "train loss:0.07997812858615355\n",
            "train loss:0.023954268018975856\n",
            "train loss:0.008401714517606016\n",
            "train loss:0.01357473983469525\n",
            "train loss:0.012312873014068904\n",
            "train loss:0.016708832651542178\n",
            "train loss:0.04588759909371973\n",
            "train loss:0.03725303288332083\n",
            "train loss:0.014030614127223786\n",
            "train loss:0.02935649939653462\n",
            "train loss:0.01510829673254085\n",
            "train loss:0.11067363191363762\n",
            "train loss:0.03972600146420433\n",
            "train loss:0.1428624613792103\n",
            "train loss:0.04929762075636582\n",
            "train loss:0.03532047697222407\n",
            "train loss:0.045681214996919454\n",
            "train loss:0.057878140965920616\n",
            "train loss:0.012129865088026043\n",
            "train loss:0.013454993903070717\n",
            "train loss:0.0385514974945085\n",
            "train loss:0.03287423967038801\n",
            "train loss:0.06270767445846148\n",
            "train loss:0.035347178156073265\n",
            "train loss:0.016230414891861677\n",
            "train loss:0.022639273364014237\n",
            "train loss:0.058794786969006106\n",
            "train loss:0.039363840145806094\n",
            "train loss:0.015795439768119063\n",
            "train loss:0.08998551071370048\n",
            "train loss:0.011387227024356128\n",
            "train loss:0.022060668741590326\n",
            "train loss:0.011580189302765524\n",
            "train loss:0.012770562537936916\n",
            "train loss:0.04032936605942601\n",
            "train loss:0.02482832425562033\n",
            "train loss:0.03831005835553533\n",
            "train loss:0.018921797919307454\n",
            "train loss:0.014064435146752192\n",
            "train loss:0.008566773977622082\n",
            "train loss:0.04020146377842179\n",
            "train loss:0.01827615174913135\n",
            "train loss:0.024575729419399162\n",
            "train loss:0.04357727164092747\n",
            "train loss:0.056628135513707385\n",
            "train loss:0.03783422056692291\n",
            "train loss:0.01538862052085525\n",
            "train loss:0.02210651000328724\n",
            "train loss:0.01237082145026943\n",
            "train loss:0.014861340169490669\n",
            "train loss:0.02078047869489435\n",
            "train loss:0.03151492824968934\n",
            "train loss:0.010915756645017769\n",
            "train loss:0.025904788140077614\n",
            "train loss:0.05311447246470505\n",
            "train loss:0.0209783149338245\n",
            "train loss:0.018999007269764688\n",
            "train loss:0.008720846471137213\n",
            "train loss:0.011980375638037076\n",
            "train loss:0.03089993917150301\n",
            "train loss:0.030065940728026323\n",
            "train loss:0.01610671267726473\n",
            "train loss:0.014804363060538936\n",
            "train loss:0.01732860932035508\n",
            "train loss:0.04568393759119646\n",
            "train loss:0.03206982591804724\n",
            "train loss:0.017314938320409232\n",
            "train loss:0.06785093306257567\n",
            "train loss:0.025057970179434362\n",
            "train loss:0.05855470556139334\n",
            "train loss:0.025623889331707006\n",
            "train loss:0.021541006081241352\n",
            "train loss:0.04932169731197802\n",
            "train loss:0.04236235046067418\n",
            "train loss:0.013769589028814226\n",
            "train loss:0.019241771842316843\n",
            "train loss:0.013608832674630167\n",
            "train loss:0.01753057009130583\n",
            "train loss:0.03403928468945011\n",
            "train loss:0.03153123029220093\n",
            "train loss:0.023954097421923084\n",
            "train loss:0.06288345943877874\n",
            "train loss:0.034390446078283815\n",
            "train loss:0.020916658911995786\n",
            "train loss:0.028534231408378984\n",
            "train loss:0.07530642879165325\n",
            "train loss:0.02574743261470396\n",
            "train loss:0.024102908129103394\n",
            "train loss:0.01591907968921759\n",
            "train loss:0.024128388946453118\n",
            "train loss:0.03781720636979327\n",
            "train loss:0.07150203296503145\n",
            "train loss:0.02238513964231602\n",
            "train loss:0.00649515683873066\n",
            "train loss:0.016604546975640302\n",
            "train loss:0.05363635964315349\n",
            "train loss:0.030773082118086114\n",
            "train loss:0.036744827322509305\n",
            "train loss:0.020270818363107422\n",
            "train loss:0.015709877003871508\n",
            "train loss:0.07594526437355867\n",
            "train loss:0.05930107400618995\n",
            "train loss:0.046229200427728406\n",
            "train loss:0.003847872003740364\n",
            "train loss:0.021638206269203857\n",
            "train loss:0.05372466620508802\n",
            "train loss:0.0179073986171984\n",
            "train loss:0.022805690104349505\n",
            "train loss:0.03264694056910321\n",
            "train loss:0.03746090098745963\n",
            "train loss:0.043650684249718796\n",
            "train loss:0.03377198941937069\n",
            "train loss:0.014940097338874825\n",
            "train loss:0.03825098945531816\n",
            "train loss:0.029705947092425796\n",
            "train loss:0.04573232758219718\n",
            "train loss:0.026048566480407938\n",
            "train loss:0.0263134233249343\n",
            "train loss:0.027953757096158424\n",
            "train loss:0.039467581032508706\n",
            "train loss:0.026813411519810808\n",
            "train loss:0.03864776647486293\n",
            "train loss:0.029300804200373714\n",
            "train loss:0.08614429521360664\n",
            "train loss:0.017635170916085175\n",
            "train loss:0.0201545250034097\n",
            "train loss:0.023752765150575977\n",
            "train loss:0.012474283515983193\n",
            "train loss:0.027291214574233252\n",
            "train loss:0.03311939897936808\n",
            "train loss:0.02252723747914988\n",
            "train loss:0.08134923322377902\n",
            "train loss:0.012638670399329248\n",
            "train loss:0.025446854930555842\n",
            "train loss:0.029173407218496625\n",
            "train loss:0.01028346639832248\n",
            "train loss:0.08956305039801653\n",
            "train loss:0.029791565233203966\n",
            "train loss:0.02299733808560901\n",
            "train loss:0.01186128030201947\n",
            "train loss:0.053597867721352624\n",
            "train loss:0.04462944840388024\n",
            "train loss:0.02361217120395336\n",
            "train loss:0.04559556105985546\n",
            "train loss:0.042292526247954376\n",
            "train loss:0.039270707241886714\n",
            "train loss:0.008881883511743106\n",
            "train loss:0.05336270862303733\n",
            "train loss:0.04968832928565388\n",
            "train loss:0.045827013266671185\n",
            "train loss:0.02580808048729153\n",
            "train loss:0.0077865421812978765\n",
            "train loss:0.027017869092810776\n",
            "train loss:0.040788997447550185\n",
            "train loss:0.0214668575071154\n",
            "train loss:0.014438234159538616\n",
            "train loss:0.07508049242919117\n",
            "train loss:0.026050358512112962\n",
            "train loss:0.031833752248271355\n",
            "train loss:0.06036583164158567\n",
            "train loss:0.028931364259709453\n",
            "train loss:0.05199846969712107\n",
            "train loss:0.02149375779553915\n",
            "train loss:0.044468720491249746\n",
            "train loss:0.02061270320062387\n",
            "train loss:0.01770662619885955\n",
            "train loss:0.013582402640532162\n",
            "train loss:0.008671933285391452\n",
            "train loss:0.021851954322757473\n",
            "train loss:0.016114600868603196\n",
            "train loss:0.025769669637002775\n",
            "train loss:0.020449615382660716\n",
            "train loss:0.020822035111620493\n",
            "train loss:0.016348763420431\n",
            "train loss:0.01567642828585377\n",
            "train loss:0.020802069918645097\n",
            "train loss:0.02363671667129787\n",
            "train loss:0.010487221463285235\n",
            "train loss:0.013878123172011743\n",
            "train loss:0.0968329765487613\n",
            "train loss:0.0047438257903176136\n",
            "train loss:0.02600329540854189\n",
            "train loss:0.02509120416164228\n",
            "train loss:0.044533370275785036\n",
            "train loss:0.021753987191966636\n",
            "train loss:0.019115391666292992\n",
            "train loss:0.02289268402856876\n",
            "train loss:0.040613711043538\n",
            "train loss:0.06519531806647741\n",
            "train loss:0.024328064685752517\n",
            "train loss:0.011816860669791976\n",
            "train loss:0.023591648724252984\n",
            "train loss:0.05727105175113495\n",
            "train loss:0.01787682391192023\n",
            "train loss:0.039200505660223574\n",
            "train loss:0.02571785733729988\n",
            "train loss:0.03438343509519038\n",
            "train loss:0.013274104759053105\n",
            "train loss:0.03133591186456203\n",
            "train loss:0.03798021123320186\n",
            "train loss:0.013370518880867304\n",
            "train loss:0.03947346555041398\n",
            "train loss:0.02101352284029642\n",
            "train loss:0.028916247075815055\n",
            "train loss:0.048908403720310856\n",
            "train loss:0.038945155085309116\n",
            "train loss:0.031011552351973216\n",
            "train loss:0.033317642915168234\n",
            "train loss:0.009704651321613605\n",
            "train loss:0.012683847984490647\n",
            "train loss:0.01471279001207564\n",
            "train loss:0.01415761665029846\n",
            "train loss:0.041714614501448734\n",
            "train loss:0.03221100355957542\n",
            "train loss:0.01907192493171379\n",
            "train loss:0.01701377169183607\n",
            "train loss:0.04504746803075857\n",
            "train loss:0.024086561262386074\n",
            "train loss:0.04890497893465055\n",
            "train loss:0.033215649693236766\n",
            "train loss:0.026376266863310432\n",
            "train loss:0.02278419356520915\n",
            "train loss:0.04484374263920499\n",
            "train loss:0.016800584515019264\n",
            "train loss:0.03105289467392847\n",
            "train loss:0.009788623472747558\n",
            "train loss:0.013081283305776747\n",
            "train loss:0.043241719607490355\n",
            "train loss:0.056615867894769016\n",
            "train loss:0.02713048881414141\n",
            "train loss:0.023020687280545606\n",
            "train loss:0.02213853866239435\n",
            "train loss:0.014257818918969148\n",
            "train loss:0.016196661170582712\n",
            "train loss:0.030636921126050484\n",
            "train loss:0.039006508355683776\n",
            "train loss:0.02868806435044905\n",
            "train loss:0.05513625190216173\n",
            "train loss:0.023725098254222955\n",
            "train loss:0.016305706691405606\n",
            "train loss:0.04229549747591793\n",
            "train loss:0.02717381068970167\n",
            "train loss:0.0855671959045674\n",
            "train loss:0.06559028358304483\n",
            "train loss:0.030745586940401283\n",
            "train loss:0.023917576562737984\n",
            "train loss:0.0637061352699459\n",
            "train loss:0.0570584014495997\n",
            "train loss:0.023736158587084385\n",
            "train loss:0.021368665240961083\n",
            "train loss:0.04739695588781067\n",
            "train loss:0.0331983291626962\n",
            "train loss:0.030351438123182316\n",
            "train loss:0.061770414516462806\n",
            "train loss:0.052678579918353\n",
            "train loss:0.013224809237848174\n",
            "train loss:0.0445748972035203\n",
            "train loss:0.04897515227215953\n",
            "train loss:0.08642188210389005\n",
            "train loss:0.023908476793310107\n",
            "train loss:0.036148685710676615\n",
            "train loss:0.032546500292303396\n",
            "train loss:0.038070087441522314\n",
            "train loss:0.035323116254577\n",
            "train loss:0.022312407287905914\n",
            "train loss:0.020816780494457073\n",
            "train loss:0.020813457953951512\n",
            "train loss:0.013467290666417595\n",
            "train loss:0.058723919718267945\n",
            "train loss:0.024043205680955283\n",
            "train loss:0.02779051422710889\n",
            "train loss:0.09674738654129225\n",
            "train loss:0.024266990119383762\n",
            "train loss:0.011469332435924288\n",
            "train loss:0.014024811214348872\n",
            "train loss:0.02003051199322752\n",
            "train loss:0.012705489418625338\n",
            "train loss:0.022871460695141118\n",
            "train loss:0.00946247090771508\n",
            "train loss:0.06283823290374824\n",
            "train loss:0.05853170621623912\n",
            "train loss:0.0672250929609674\n",
            "train loss:0.06674068898018555\n",
            "train loss:0.012724831684484007\n",
            "train loss:0.031660235105479974\n",
            "train loss:0.01678313857260865\n",
            "train loss:0.0181558516539427\n",
            "train loss:0.016623077905169495\n",
            "train loss:0.015506707720338133\n",
            "train loss:0.020884133506429255\n",
            "train loss:0.012864582673776142\n",
            "train loss:0.040966624952736445\n",
            "train loss:0.007913146848672952\n",
            "train loss:0.005876619801194915\n",
            "train loss:0.11254134746470854\n",
            "train loss:0.01734578568825755\n",
            "train loss:0.013934126398604252\n",
            "train loss:0.07672339855451828\n",
            "train loss:0.01787989203375968\n",
            "train loss:0.030278737160499957\n",
            "train loss:0.05746766379124493\n",
            "train loss:0.01147476294823466\n",
            "train loss:0.03909644909133173\n",
            "train loss:0.03129132174994646\n",
            "train loss:0.05582101926196972\n",
            "train loss:0.017844526636039346\n",
            "train loss:0.011384422754772137\n",
            "train loss:0.05737644104505898\n",
            "train loss:0.042382044112188745\n",
            "train loss:0.06570345705543498\n",
            "train loss:0.01698961043256457\n",
            "train loss:0.017858558146546296\n",
            "train loss:0.025168776422618038\n",
            "train loss:0.02041712403855513\n",
            "train loss:0.015550981520096152\n",
            "train loss:0.0114881658353746\n",
            "train loss:0.02770171978542809\n",
            "train loss:0.019305647620596132\n",
            "train loss:0.01577603409210685\n",
            "train loss:0.027252066434656105\n",
            "train loss:0.06232003774545165\n",
            "train loss:0.01632330642458358\n",
            "train loss:0.01950417384006269\n",
            "train loss:0.0321622841773689\n",
            "train loss:0.07407774054990121\n",
            "train loss:0.04947446257456511\n",
            "train loss:0.029507817084871334\n",
            "train loss:0.02373199038099207\n",
            "train loss:0.018043608849730938\n",
            "train loss:0.018505808388874948\n",
            "train loss:0.031691873076903083\n",
            "train loss:0.015792813376322764\n",
            "train loss:0.008350402180310294\n",
            "train loss:0.09099974023450169\n",
            "train loss:0.08584858097500175\n",
            "train loss:0.030700150977709048\n",
            "train loss:0.0389367685671943\n",
            "train loss:0.032146550376785044\n",
            "train loss:0.05445202999304399\n",
            "train loss:0.05255250662835675\n",
            "train loss:0.02572362333093102\n",
            "train loss:0.011835489203965748\n",
            "train loss:0.025474384885689053\n",
            "train loss:0.06222181214293282\n",
            "train loss:0.049422887325553305\n",
            "train loss:0.030111439480874864\n",
            "train loss:0.025214683351793603\n",
            "train loss:0.04417735504477198\n",
            "train loss:0.018357937780171897\n",
            "train loss:0.013296332178478256\n",
            "train loss:0.024852254938590387\n",
            "train loss:0.04140037152605049\n",
            "train loss:0.04356292715923732\n",
            "train loss:0.04291149152311177\n",
            "train loss:0.0305752059470034\n",
            "train loss:0.012576932278779773\n",
            "train loss:0.008812379297466364\n",
            "train loss:0.0194528491574505\n",
            "train loss:0.03277821364838485\n",
            "train loss:0.018323353664958787\n",
            "train loss:0.024676523991014623\n",
            "train loss:0.023116536330131162\n",
            "train loss:0.06657211589679699\n",
            "train loss:0.01261857608041804\n",
            "train loss:0.022636614425500117\n",
            "train loss:0.021173914872341236\n",
            "train loss:0.01897241579569922\n",
            "train loss:0.03958009828486451\n",
            "train loss:0.012772884938478214\n",
            "train loss:0.017780560276581134\n",
            "train loss:0.045059651830927513\n",
            "train loss:0.023676561131750852\n",
            "train loss:0.02900452853864761\n",
            "train loss:0.010312297991292567\n",
            "train loss:0.022478263950497897\n",
            "train loss:0.04605608058873257\n",
            "train loss:0.009493250929022194\n",
            "train loss:0.019875864299828797\n",
            "train loss:0.009565792730530941\n",
            "train loss:0.009309222645711917\n",
            "train loss:0.017584065461681668\n",
            "train loss:0.03197322200824308\n",
            "train loss:0.02416294968062308\n",
            "train loss:0.014705845960043049\n",
            "train loss:0.01194522571605145\n",
            "train loss:0.022126408413905327\n",
            "train loss:0.01527048406748434\n",
            "train loss:0.010598459384625409\n",
            "train loss:0.007267404384542948\n",
            "train loss:0.013429933418137472\n",
            "train loss:0.005289245092044288\n",
            "train loss:0.046329566363572605\n",
            "train loss:0.08546463396556421\n",
            "train loss:0.013845504452495626\n",
            "train loss:0.008209394031284106\n",
            "train loss:0.006023797388402986\n",
            "train loss:0.04612508568165532\n",
            "train loss:0.005240902239792242\n",
            "train loss:0.013407012345252768\n",
            "train loss:0.00849041274033908\n",
            "train loss:0.052340906040324826\n",
            "train loss:0.01554359789489865\n",
            "train loss:0.010615645427913717\n",
            "train loss:0.03459341661173505\n",
            "train loss:0.023547306122820966\n",
            "train loss:0.06667868309395322\n",
            "train loss:0.021837801561060743\n",
            "train loss:0.01768241766236266\n",
            "train loss:0.023085600962741495\n",
            "train loss:0.049801602846755194\n",
            "train loss:0.010406826391534419\n",
            "train loss:0.03047430398860008\n",
            "train loss:0.02243928723898574\n",
            "train loss:0.04759202710229335\n",
            "train loss:0.011628131802390832\n",
            "train loss:0.02222872084646313\n",
            "train loss:0.0956168105231193\n",
            "train loss:0.033095752059669116\n",
            "train loss:0.003049214543399194\n",
            "train loss:0.03272354596981916\n",
            "train loss:0.0137148472642023\n",
            "train loss:0.02665953901976738\n",
            "train loss:0.02701611792887066\n",
            "train loss:0.05804270194729999\n",
            "train loss:0.05060406594777677\n",
            "train loss:0.009906607346029552\n",
            "train loss:0.008174668813109041\n",
            "train loss:0.014469212146896074\n",
            "train loss:0.01314407367463822\n",
            "train loss:0.010609493868112677\n",
            "train loss:0.019663438450813083\n",
            "train loss:0.012172144155849152\n",
            "train loss:0.03696792515997656\n",
            "train loss:0.037819434449244085\n",
            "train loss:0.0030695000947699187\n",
            "train loss:0.022810117992280343\n",
            "train loss:0.033784651243825146\n",
            "train loss:0.018188886907180495\n",
            "train loss:0.10749473587050366\n",
            "train loss:0.011491861015218808\n",
            "train loss:0.04610012394723076\n",
            "train loss:0.012922209691332716\n",
            "train loss:0.01615900323277961\n",
            "train loss:0.01159755476108033\n",
            "train loss:0.015924360352359883\n",
            "train loss:0.024777547708364742\n",
            "train loss:0.01831958106115463\n",
            "train loss:0.010669131534113654\n",
            "train loss:0.02439714663066427\n",
            "train loss:0.014425445995505723\n",
            "train loss:0.018211807396974422\n",
            "train loss:0.018704645567554055\n",
            "train loss:0.021102928751641225\n",
            "train loss:0.019168358922565014\n",
            "train loss:0.007632711396905867\n",
            "train loss:0.042478771567792305\n",
            "train loss:0.023579213138442436\n",
            "train loss:0.01639859689273878\n",
            "train loss:0.017530386942061243\n",
            "train loss:0.005532233527097656\n",
            "train loss:0.012557651089496464\n",
            "train loss:0.010161031498785325\n",
            "train loss:0.005374393667979531\n",
            "train loss:0.03024264694543149\n",
            "train loss:0.01837356659681023\n",
            "train loss:0.018092692268144993\n",
            "train loss:0.01380912737251884\n",
            "train loss:0.035098697313451284\n",
            "train loss:0.019395961858595007\n",
            "train loss:0.015225566487209742\n",
            "train loss:0.028227417712681856\n",
            "train loss:0.01357449447032167\n",
            "train loss:0.01829625905496784\n",
            "train loss:0.047145387250966786\n",
            "train loss:0.011915412663519451\n",
            "train loss:0.03532840834350628\n",
            "train loss:0.023952460574251485\n",
            "train loss:0.0150032535240856\n",
            "train loss:0.014263325213422173\n",
            "train loss:0.009159410313509636\n",
            "train loss:0.03633411070437841\n",
            "train loss:0.024727270072637433\n",
            "train loss:0.013626309401831571\n",
            "train loss:0.010917007546078588\n",
            "train loss:0.013270472419023217\n",
            "train loss:0.012157719374932825\n",
            "train loss:0.013149819403367326\n",
            "train loss:0.011598596534146816\n",
            "train loss:0.033213282728012404\n",
            "train loss:0.037558270150208656\n",
            "train loss:0.026090669521697515\n",
            "train loss:0.04618606694178516\n",
            "train loss:0.040834582040613514\n",
            "train loss:0.012156376914720381\n",
            "train loss:0.030314080972465022\n",
            "train loss:0.04671024883125551\n",
            "train loss:0.019989450585084206\n",
            "train loss:0.018411348447397273\n",
            "train loss:0.009961614064756095\n",
            "train loss:0.004359533680311455\n",
            "train loss:0.03364403108381123\n",
            "train loss:0.016982413457087075\n",
            "train loss:0.023834010152665272\n",
            "train loss:0.008022709599136943\n",
            "train loss:0.04279426269095075\n",
            "train loss:0.03307586655193182\n",
            "train loss:0.015182036225913671\n",
            "train loss:0.020583524984326886\n",
            "train loss:0.023146769819493214\n",
            "train loss:0.025485317922769122\n",
            "train loss:0.010768443888953972\n",
            "train loss:0.03175532262720642\n",
            "train loss:0.01244477387352762\n",
            "train loss:0.08610988707949582\n",
            "train loss:0.015977213025432105\n",
            "train loss:0.04362420566165125\n",
            "train loss:0.010329670924905514\n",
            "train loss:0.013580082208419329\n",
            "train loss:0.018188219654881033\n",
            "train loss:0.029846595920062023\n",
            "train loss:0.015573165729999679\n",
            "train loss:0.007789340157704317\n",
            "train loss:0.024875671321656183\n",
            "train loss:0.0062112822321319485\n",
            "train loss:0.020720345129538305\n",
            "train loss:0.023605851092161557\n",
            "train loss:0.010404616240940167\n",
            "train loss:0.012017295174882042\n",
            "train loss:0.01549417764953314\n",
            "train loss:0.02265129387625589\n",
            "train loss:0.009144131911151881\n",
            "train loss:0.006158096784910336\n",
            "train loss:0.07080624755283411\n",
            "train loss:0.023566935841978623\n",
            "train loss:0.013441944723754241\n",
            "train loss:0.015703390117111554\n",
            "train loss:0.01614340033046935\n",
            "train loss:0.0290995510063073\n",
            "=== epoch:18, train acc:0.992, test acc:0.907 ===\n",
            "train loss:0.02750847389040694\n",
            "train loss:0.00834134263090079\n",
            "train loss:0.014772312838857963\n",
            "train loss:0.01776172615748649\n",
            "train loss:0.013403572215321336\n",
            "train loss:0.012842174672505494\n",
            "train loss:0.0249715418823327\n",
            "train loss:0.006808295560474504\n",
            "train loss:0.01150676475805182\n",
            "train loss:0.007688361057375217\n",
            "train loss:0.00596295502849803\n",
            "train loss:0.018213868376933517\n",
            "train loss:0.016260029229990404\n",
            "train loss:0.02594098202734867\n",
            "train loss:0.017678907496144453\n",
            "train loss:0.02802864954653771\n",
            "train loss:0.009182469494954052\n",
            "train loss:0.015339637205812497\n",
            "train loss:0.0136409134069464\n",
            "train loss:0.010095915151478058\n",
            "train loss:0.034514159631247904\n",
            "train loss:0.04734450745219938\n",
            "train loss:0.012900813767774045\n",
            "train loss:0.011876247985306163\n",
            "train loss:0.09904995230850837\n",
            "train loss:0.049337621856596854\n",
            "train loss:0.0075511471382760905\n",
            "train loss:0.0185457575319675\n",
            "train loss:0.009034946782281292\n",
            "train loss:0.08390458204110736\n",
            "train loss:0.01139358854716732\n",
            "train loss:0.022010711361938076\n",
            "train loss:0.01316322104980417\n",
            "train loss:0.0062268336421910896\n",
            "train loss:0.02694277137458855\n",
            "train loss:0.005087395026044894\n",
            "train loss:0.007712156305743976\n",
            "train loss:0.0044910244470154105\n",
            "train loss:0.013004746979778476\n",
            "train loss:0.03321063965978824\n",
            "train loss:0.026416829683783977\n",
            "train loss:0.023112560898866708\n",
            "train loss:0.030944735879238114\n",
            "train loss:0.03445776305843767\n",
            "train loss:0.07853362388354491\n",
            "train loss:0.03236538597881626\n",
            "train loss:0.0289951647114861\n",
            "train loss:0.026325505924014708\n",
            "train loss:0.040193774301944346\n",
            "train loss:0.027694194187515016\n",
            "train loss:0.00862084392344273\n",
            "train loss:0.0631958805911142\n",
            "train loss:0.019327104571837263\n",
            "train loss:0.02171616327260302\n",
            "train loss:0.08296515505071228\n",
            "train loss:0.057140187783185074\n",
            "train loss:0.038986605898843486\n",
            "train loss:0.019860617067843617\n",
            "train loss:0.07214596509665645\n",
            "train loss:0.05314629213432794\n",
            "train loss:0.04819896381876691\n",
            "train loss:0.0275226286977601\n",
            "train loss:0.05327875252274322\n",
            "train loss:0.044043105532593704\n",
            "train loss:0.024039930687113156\n",
            "train loss:0.03833803200767322\n",
            "train loss:0.028172570580398005\n",
            "train loss:0.00615176806549012\n",
            "train loss:0.03382156081088316\n",
            "train loss:0.09258858516006079\n",
            "train loss:0.03764788050266888\n",
            "train loss:0.018653187080776804\n",
            "train loss:0.010467453944019796\n",
            "train loss:0.025477345509044205\n",
            "train loss:0.01932494040445916\n",
            "train loss:0.048491758804993436\n",
            "train loss:0.038303657602464715\n",
            "train loss:0.01888711495097658\n",
            "train loss:0.014310945748536818\n",
            "train loss:0.03821873803228924\n",
            "train loss:0.03349015842324758\n",
            "train loss:0.05521939796131198\n",
            "train loss:0.029369259498121806\n",
            "train loss:0.017885494908297137\n",
            "train loss:0.11562349524893088\n",
            "train loss:0.03515691266192113\n",
            "train loss:0.01476906352914156\n",
            "train loss:0.01778189969376598\n",
            "train loss:0.017427433907105064\n",
            "train loss:0.01047336475277214\n",
            "train loss:0.019790969699601743\n",
            "train loss:0.024357744598005566\n",
            "train loss:0.021228188715127345\n",
            "train loss:0.015960004818748732\n",
            "train loss:0.008469390425500484\n",
            "train loss:0.015251651373420045\n",
            "train loss:0.05912141389174923\n",
            "train loss:0.07648383529238303\n",
            "train loss:0.04514113538861258\n",
            "train loss:0.019405123147602395\n",
            "train loss:0.013447485297550396\n",
            "train loss:0.0063943820748363755\n",
            "train loss:0.013897585848666214\n",
            "train loss:0.018302048961566765\n",
            "train loss:0.02422155078351916\n",
            "train loss:0.03523537843979264\n",
            "train loss:0.015793132333027107\n",
            "train loss:0.030858305465740195\n",
            "train loss:0.02769999801856829\n",
            "train loss:0.015187762398481835\n",
            "train loss:0.03159584471450782\n",
            "train loss:0.006393636598572372\n",
            "train loss:0.03113951554122778\n",
            "train loss:0.05049716490963336\n",
            "train loss:0.013514511620514043\n",
            "train loss:0.010318074505348418\n",
            "train loss:0.042234335111937756\n",
            "train loss:0.031019907979106507\n",
            "train loss:0.023785206589420585\n",
            "train loss:0.011154965157974999\n",
            "train loss:0.011523889205524615\n",
            "train loss:0.02441071249356115\n",
            "train loss:0.014559702195750217\n",
            "train loss:0.04998922641407358\n",
            "train loss:0.009252411829921512\n",
            "train loss:0.01861630762908824\n",
            "train loss:0.038881582075290605\n",
            "train loss:0.036797693229393046\n",
            "train loss:0.016376101666620693\n",
            "train loss:0.026027728223276304\n",
            "train loss:0.01726153710308\n",
            "train loss:0.009178960868536501\n",
            "train loss:0.04931904606993796\n",
            "train loss:0.011138668962884925\n",
            "train loss:0.024387408441792036\n",
            "train loss:0.03152755191511297\n",
            "train loss:0.02379556804582255\n",
            "train loss:0.028243902384837517\n",
            "train loss:0.029237734726282903\n",
            "train loss:0.0410890336296242\n",
            "train loss:0.020323111662995205\n",
            "train loss:0.014468731916720967\n",
            "train loss:0.020719893570054434\n",
            "train loss:0.008722851445292766\n",
            "train loss:0.05849808120722903\n",
            "train loss:0.013517234827624245\n",
            "train loss:0.019056244346135228\n",
            "train loss:0.01746307949107777\n",
            "train loss:0.044266673665555596\n",
            "train loss:0.017184147096176553\n",
            "train loss:0.003586541567493099\n",
            "train loss:0.03154851998473206\n",
            "train loss:0.01610302658579848\n",
            "train loss:0.03349649724314033\n",
            "train loss:0.014779003571944243\n",
            "train loss:0.0246350278217245\n",
            "train loss:0.009279639425260005\n",
            "train loss:0.0387356704616994\n",
            "train loss:0.011035042181853865\n",
            "train loss:0.025285921321622565\n",
            "train loss:0.015081340215543651\n",
            "train loss:0.013505513635037679\n",
            "train loss:0.01881111173157028\n",
            "train loss:0.007469657979295363\n",
            "train loss:0.011597816875176952\n",
            "train loss:0.03756067077270082\n",
            "train loss:0.019434297772513886\n",
            "train loss:0.011650717121642437\n",
            "train loss:0.012983113925472104\n",
            "train loss:0.010119401948694769\n",
            "train loss:0.011944921864410804\n",
            "train loss:0.027669531727153907\n",
            "train loss:0.018391656006457056\n",
            "train loss:0.03949770659050123\n",
            "train loss:0.005801654029211475\n",
            "train loss:0.025943360073754446\n",
            "train loss:0.018023752137349986\n",
            "train loss:0.0061242952497962974\n",
            "train loss:0.008949733572453614\n",
            "train loss:0.021632753928331496\n",
            "train loss:0.05278036775414856\n",
            "train loss:0.006007970716754384\n",
            "train loss:0.04101351896093282\n",
            "train loss:0.009940890620946816\n",
            "train loss:0.016214615525285397\n",
            "train loss:0.032018387516472215\n",
            "train loss:0.018578138719582863\n",
            "train loss:0.024509523030028\n",
            "train loss:0.01867354706753459\n",
            "train loss:0.011804898549334128\n",
            "train loss:0.012280077728825962\n",
            "train loss:0.009083595178735604\n",
            "train loss:0.048656834095078\n",
            "train loss:0.00879212943164726\n",
            "train loss:0.01976759422811944\n",
            "train loss:0.0290236930716951\n",
            "train loss:0.047761043126818795\n",
            "train loss:0.00884777943695969\n",
            "train loss:0.013359654117065278\n",
            "train loss:0.02619260391859585\n",
            "train loss:0.016042724390298847\n",
            "train loss:0.013766300036632406\n",
            "train loss:0.023061030332290032\n",
            "train loss:0.013866654241560807\n",
            "train loss:0.012577682775756967\n",
            "train loss:0.026916034409021586\n",
            "train loss:0.02877900303640859\n",
            "train loss:0.03997482862629644\n",
            "train loss:0.010309194602033514\n",
            "train loss:0.0353325873398612\n",
            "train loss:0.08139599045665938\n",
            "train loss:0.01265961434450444\n",
            "train loss:0.04829983238223181\n",
            "train loss:0.025593820642125488\n",
            "train loss:0.013636236030557247\n",
            "train loss:0.055997134877022264\n",
            "train loss:0.01503053825771516\n",
            "train loss:0.009772107546611888\n",
            "train loss:0.04425806908259764\n",
            "train loss:0.03828944393792824\n",
            "train loss:0.020519650163447537\n",
            "train loss:0.021626705583080948\n",
            "train loss:0.026598477259511505\n",
            "train loss:0.030830588717977742\n",
            "train loss:0.04480075634821494\n",
            "train loss:0.013494716969519467\n",
            "train loss:0.034656154351772085\n",
            "train loss:0.012078940937771919\n",
            "train loss:0.018039593070214056\n",
            "train loss:0.02286697679144689\n",
            "train loss:0.0129335578425039\n",
            "train loss:0.024498617305961257\n",
            "train loss:0.006793230207995248\n",
            "train loss:0.03982170516260048\n",
            "train loss:0.1281383813819438\n",
            "train loss:0.0023516120459078317\n",
            "train loss:0.03142005954059588\n",
            "train loss:0.01914172478286255\n",
            "train loss:0.012429449335246003\n",
            "train loss:0.04182994408145427\n",
            "train loss:0.040832027856088325\n",
            "train loss:0.03246610789306974\n",
            "train loss:0.016626398122816483\n",
            "train loss:0.02077948271091883\n",
            "train loss:0.03147961639852069\n",
            "train loss:0.018968766191760363\n",
            "train loss:0.013018478079945456\n",
            "train loss:0.02636519295031053\n",
            "train loss:0.04652609965816131\n",
            "train loss:0.03734386434224153\n",
            "train loss:0.09872021044788226\n",
            "train loss:0.010854167948221844\n",
            "train loss:0.01054485687868289\n",
            "train loss:0.008179056128574507\n",
            "train loss:0.01881456621773949\n",
            "train loss:0.0063097903583210465\n",
            "train loss:0.017565990603403834\n",
            "train loss:0.0021796234806949126\n",
            "train loss:0.027514654635258297\n",
            "train loss:0.03592826877053942\n",
            "train loss:0.015485108404528691\n",
            "train loss:0.008768125591511293\n",
            "train loss:0.02631991583868866\n",
            "train loss:0.013276740057897292\n",
            "train loss:0.018177152469926693\n",
            "train loss:0.03210548175554329\n",
            "train loss:0.03448911806399409\n",
            "train loss:0.0443259902886838\n",
            "train loss:0.07103802654522026\n",
            "train loss:0.025009184419113355\n",
            "train loss:0.015010718015935566\n",
            "train loss:0.013084200954731801\n",
            "train loss:0.04962206448339763\n",
            "train loss:0.04091612476288987\n",
            "train loss:0.013311257998089054\n",
            "train loss:0.014668615323852479\n",
            "train loss:0.04348798905342703\n",
            "train loss:0.014484934862799139\n",
            "train loss:0.010684818533618388\n",
            "train loss:0.006649815350369684\n",
            "train loss:0.008770322519711412\n",
            "train loss:0.01183891265928115\n",
            "train loss:0.012049914051979511\n",
            "train loss:0.020598434356146528\n",
            "train loss:0.0412850376051952\n",
            "train loss:0.02779623472680873\n",
            "train loss:0.010918700623974827\n",
            "train loss:0.017529528313573003\n",
            "train loss:0.02673052993528186\n",
            "train loss:0.008577183865790873\n",
            "train loss:0.04215478048269945\n",
            "train loss:0.04674111166366655\n",
            "train loss:0.0260406861456935\n",
            "train loss:0.024228317475712306\n",
            "train loss:0.011984454123320627\n",
            "train loss:0.010037725984876282\n",
            "train loss:0.024103150052555332\n",
            "train loss:0.01761474504805392\n",
            "train loss:0.016094830130147203\n",
            "train loss:0.021062999771859617\n",
            "train loss:0.020223071679833594\n",
            "train loss:0.02991353108203938\n",
            "train loss:0.03031292759842771\n",
            "train loss:0.013197966444344171\n",
            "train loss:0.019202371244921537\n",
            "train loss:0.006373454581378817\n",
            "train loss:0.016673586075893285\n",
            "train loss:0.005802993024707211\n",
            "train loss:0.022724492691852102\n",
            "train loss:0.018100411907257908\n",
            "train loss:0.010226910655360943\n",
            "train loss:0.008275783356771663\n",
            "train loss:0.00941622382489726\n",
            "train loss:0.010860733776946714\n",
            "train loss:0.011854913639796199\n",
            "train loss:0.028261677013847176\n",
            "train loss:0.004273548747852848\n",
            "train loss:0.013261237049933903\n",
            "train loss:0.03689115288112044\n",
            "train loss:0.012659315102198053\n",
            "train loss:0.016053319902446128\n",
            "train loss:0.03338373930588372\n",
            "train loss:0.011847419622819851\n",
            "train loss:0.04514932394815539\n",
            "train loss:0.006474174761425102\n",
            "train loss:0.021687048214584975\n",
            "train loss:0.005773172467509212\n",
            "train loss:0.01774462963839871\n",
            "train loss:0.01041199383807165\n",
            "train loss:0.013181081227861166\n",
            "train loss:0.02150425481213365\n",
            "train loss:0.029921178427047685\n",
            "train loss:0.007504618410561747\n",
            "train loss:0.00467927592515386\n",
            "train loss:0.018797331223883152\n",
            "train loss:0.008546972693440786\n",
            "train loss:0.03683626739551231\n",
            "train loss:0.028463598108088607\n",
            "train loss:0.013629027532922977\n",
            "train loss:0.0360234598978427\n",
            "train loss:0.010948000831761404\n",
            "train loss:0.006895513146151499\n",
            "train loss:0.008992424141995782\n",
            "train loss:0.010706869077205116\n",
            "train loss:0.003761012194754209\n",
            "train loss:0.01076240294064315\n",
            "train loss:0.017610001708273416\n",
            "train loss:0.0068856160535450016\n",
            "train loss:0.010646713360675955\n",
            "train loss:0.018849233232246354\n",
            "train loss:0.011002771001974342\n",
            "train loss:0.005951993543755378\n",
            "train loss:0.009258430730599456\n",
            "train loss:0.01943880570194021\n",
            "train loss:0.009098229506649454\n",
            "train loss:0.028323267604414525\n",
            "train loss:0.016738206245546106\n",
            "train loss:0.020643392410815305\n",
            "train loss:0.01602153664198107\n",
            "train loss:0.01903523240941605\n",
            "train loss:0.012010027538297155\n",
            "train loss:0.0076727084132188475\n",
            "train loss:0.018498852101228626\n",
            "train loss:0.015884534509260882\n",
            "train loss:0.008848210949960321\n",
            "train loss:0.008833766218399701\n",
            "train loss:0.01621625839753482\n",
            "train loss:0.005596062324967435\n",
            "train loss:0.027869693231655567\n",
            "train loss:0.02677919467692026\n",
            "train loss:0.033199700631160345\n",
            "train loss:0.031202552010994714\n",
            "train loss:0.007708100627888883\n",
            "train loss:0.013944629856830391\n",
            "train loss:0.025047109513456123\n",
            "train loss:0.027420216463035878\n",
            "train loss:0.011522391430893626\n",
            "train loss:0.036437462920531384\n",
            "train loss:0.013192622585252669\n",
            "train loss:0.026765007068864418\n",
            "train loss:0.010830780934074202\n",
            "train loss:0.008527881654030754\n",
            "train loss:0.014034200879566189\n",
            "train loss:0.01790096003929165\n",
            "train loss:0.01755962957687337\n",
            "train loss:0.01811708689125745\n",
            "train loss:0.014089476363013623\n",
            "train loss:0.07214142873854434\n",
            "train loss:0.017254003649557294\n",
            "train loss:0.0115443745863448\n",
            "train loss:0.01737613145372576\n",
            "train loss:0.0072574482590170245\n",
            "train loss:0.007012644735209136\n",
            "train loss:0.012075454367756748\n",
            "train loss:0.01336889919724483\n",
            "train loss:0.027374073275545176\n",
            "train loss:0.03515509734933345\n",
            "train loss:0.02155736541132934\n",
            "train loss:0.023035475418503912\n",
            "train loss:0.008018484003570275\n",
            "train loss:0.007189203317949719\n",
            "train loss:0.0064573907447412655\n",
            "train loss:0.0127170570148181\n",
            "train loss:0.03420487133948556\n",
            "train loss:0.004249513957321512\n",
            "train loss:0.014333186129176199\n",
            "train loss:0.00800371075966776\n",
            "train loss:0.034640560890256474\n",
            "train loss:0.0070024287600022285\n",
            "train loss:0.017642114163954465\n",
            "train loss:0.01297976132572226\n",
            "train loss:0.02835592810583051\n",
            "train loss:0.015544564702142272\n",
            "train loss:0.007038207401725234\n",
            "train loss:0.015452760396024763\n",
            "train loss:0.019075983806224106\n",
            "train loss:0.013183695633819992\n",
            "train loss:0.013721288664009918\n",
            "train loss:0.0309584470812042\n",
            "train loss:0.017453336166896687\n",
            "train loss:0.014055654141138876\n",
            "train loss:0.06885795910660332\n",
            "train loss:0.01934498290321395\n",
            "train loss:0.034833197592103946\n",
            "train loss:0.006378157449591673\n",
            "train loss:0.014998092026753057\n",
            "train loss:0.02608231570049608\n",
            "train loss:0.024614301569150513\n",
            "train loss:0.024220976607970016\n",
            "train loss:0.01926507096845487\n",
            "train loss:0.021352181091232717\n",
            "train loss:0.019698069771852428\n",
            "train loss:0.01105873476205345\n",
            "train loss:0.006629330692753557\n",
            "train loss:0.026682602441992335\n",
            "train loss:0.016192357127875912\n",
            "train loss:0.010738663232344793\n",
            "train loss:0.009642853890981788\n",
            "train loss:0.041232757061890064\n",
            "train loss:0.004647262606490669\n",
            "train loss:0.06571429886058255\n",
            "train loss:0.009156810171611554\n",
            "train loss:0.07184644147251937\n",
            "train loss:0.003072190078152862\n",
            "train loss:0.015071020852026046\n",
            "train loss:0.014850048092440385\n",
            "train loss:0.006506835608516841\n",
            "train loss:0.00661987793321166\n",
            "train loss:0.014443932408175063\n",
            "train loss:0.018428244693623055\n",
            "train loss:0.02209102592639143\n",
            "train loss:0.011057275439070112\n",
            "train loss:0.015496662347101876\n",
            "train loss:0.024349383489008277\n",
            "train loss:0.023960506928350438\n",
            "train loss:0.014989500893001956\n",
            "train loss:0.022569025891299402\n",
            "train loss:0.02143394401866642\n",
            "train loss:0.011223725688637842\n",
            "train loss:0.03155730785705436\n",
            "train loss:0.04229369182796053\n",
            "train loss:0.03420225821000145\n",
            "train loss:0.024066405223597355\n",
            "train loss:0.07596372388798155\n",
            "train loss:0.008025089040348354\n",
            "train loss:0.01742502189818203\n",
            "train loss:0.006280800147092026\n",
            "train loss:0.015054688957170366\n",
            "train loss:0.0022811470352029507\n",
            "train loss:0.010207417362787581\n",
            "train loss:0.006882651989413887\n",
            "train loss:0.013510678848476163\n",
            "train loss:0.02409802971848788\n",
            "train loss:0.02065633227928781\n",
            "train loss:0.029986163327562473\n",
            "train loss:0.01089168511241429\n",
            "train loss:0.012346725095584727\n",
            "train loss:0.005319588805398821\n",
            "train loss:0.025083811730027935\n",
            "train loss:0.012168258609816051\n",
            "train loss:0.007972819222272836\n",
            "train loss:0.020173325440724824\n",
            "train loss:0.025344498719836118\n",
            "train loss:0.009254486964800097\n",
            "train loss:0.02566435532238052\n",
            "train loss:0.027634881656671585\n",
            "train loss:0.02586607184551937\n",
            "train loss:0.028972247450928208\n",
            "train loss:0.012683644157325993\n",
            "train loss:0.01816884550739714\n",
            "train loss:0.01422417763948922\n",
            "train loss:0.022122775258084565\n",
            "train loss:0.022389193535976612\n",
            "train loss:0.013651926338648292\n",
            "train loss:0.056188301587880914\n",
            "train loss:0.005114944954817137\n",
            "train loss:0.016861511605413427\n",
            "train loss:0.013188346326370426\n",
            "train loss:0.025669436327697456\n",
            "train loss:0.004535921435153294\n",
            "train loss:0.012045070355214776\n",
            "train loss:0.030414408325893722\n",
            "train loss:0.013726134997192168\n",
            "train loss:0.022401060074941317\n",
            "train loss:0.02868033002395225\n",
            "train loss:0.025539373848644943\n",
            "train loss:0.035610913162902574\n",
            "train loss:0.01292234096095339\n",
            "train loss:0.006403016765088251\n",
            "train loss:0.026331311818053864\n",
            "train loss:0.048493448936023704\n",
            "train loss:0.015508104563576302\n",
            "train loss:0.017798933367959933\n",
            "train loss:0.01957616017386063\n",
            "train loss:0.030684547594147772\n",
            "train loss:0.0034552048099572045\n",
            "train loss:0.026748170960118993\n",
            "train loss:0.010320544380867427\n",
            "train loss:0.0559152606006311\n",
            "train loss:0.012382896198148119\n",
            "train loss:0.02083161244999228\n",
            "train loss:0.04322930574932994\n",
            "train loss:0.027290876265320754\n",
            "train loss:0.0040076263540466985\n",
            "train loss:0.012068101128749464\n",
            "train loss:0.031544669981481\n",
            "train loss:0.047173542001202655\n",
            "train loss:0.02028199120569977\n",
            "train loss:0.010507786846627381\n",
            "train loss:0.01868330200024777\n",
            "train loss:0.007398433447785938\n",
            "train loss:0.04115769213141538\n",
            "train loss:0.038579293439904835\n",
            "train loss:0.011754929209737032\n",
            "train loss:0.022835308673539143\n",
            "train loss:0.01308606570859525\n",
            "train loss:0.04988158307623117\n",
            "train loss:0.012132868992256653\n",
            "train loss:0.020011714765595174\n",
            "train loss:0.014168631254960595\n",
            "train loss:0.015255589647016888\n",
            "train loss:0.03445046866050442\n",
            "train loss:0.01727492007743243\n",
            "train loss:0.03179058089761042\n",
            "train loss:0.01178555265156243\n",
            "train loss:0.005005121274935808\n",
            "train loss:0.022032439630463335\n",
            "train loss:0.01695407638061678\n",
            "train loss:0.01720429965208865\n",
            "train loss:0.018270409432311355\n",
            "train loss:0.01591887151250834\n",
            "train loss:0.007618291689063027\n",
            "train loss:0.02783706357934398\n",
            "train loss:0.008543466630507618\n",
            "train loss:0.013618512397202527\n",
            "train loss:0.017339915629028688\n",
            "train loss:0.01207858310215958\n",
            "train loss:0.006351213015644751\n",
            "train loss:0.007301940264904545\n",
            "train loss:0.0072389190350753896\n",
            "train loss:0.004275873315536032\n",
            "train loss:0.05033333374052883\n",
            "train loss:0.031372553861457715\n",
            "train loss:0.010232817732271264\n",
            "train loss:0.03491818399617607\n",
            "train loss:0.02570371072302484\n",
            "train loss:0.009956660306650506\n",
            "train loss:0.012199011077193987\n",
            "train loss:0.03539507481020842\n",
            "train loss:0.030533204223785332\n",
            "train loss:0.07905040220564373\n",
            "train loss:0.024375196510847137\n",
            "train loss:0.017330232077780172\n",
            "train loss:0.01393721512967504\n",
            "train loss:0.017228035365199415\n",
            "train loss:0.02659697764897268\n",
            "train loss:0.01150436611802561\n",
            "train loss:0.04205161657487035\n",
            "train loss:0.02529536895528421\n",
            "train loss:0.007429813004670736\n",
            "train loss:0.02679185042186062\n",
            "train loss:0.04160699680876236\n",
            "train loss:0.00972331277428505\n",
            "train loss:0.015524802105893576\n",
            "train loss:0.021541165541561044\n",
            "train loss:0.018583478456557622\n",
            "train loss:0.024834403737575207\n",
            "train loss:0.023462213592221243\n",
            "train loss:0.02385579027362367\n",
            "train loss:0.055860822066639046\n",
            "train loss:0.021315943443842578\n",
            "train loss:0.04143832407001872\n",
            "train loss:0.01731242542706115\n",
            "train loss:0.014289406307288494\n",
            "train loss:0.04122960652181239\n",
            "train loss:0.01906091587321956\n",
            "train loss:0.009928124185466004\n",
            "train loss:0.006442933759139208\n",
            "train loss:0.0306870095582879\n",
            "train loss:0.01790979292126119\n",
            "=== epoch:19, train acc:0.987, test acc:0.908 ===\n",
            "train loss:0.024726058362181568\n",
            "train loss:0.005208543585635986\n",
            "train loss:0.01593961231349814\n",
            "train loss:0.00959776665444826\n",
            "train loss:0.021362819319950387\n",
            "train loss:0.016125358189604047\n",
            "train loss:0.009997980140931036\n",
            "train loss:0.015933657580109443\n",
            "train loss:0.026062673074207118\n",
            "train loss:0.006066704396689433\n",
            "train loss:0.024619131778200694\n",
            "train loss:0.011919388294039816\n",
            "train loss:0.04330546417746849\n",
            "train loss:0.016434608976275147\n",
            "train loss:0.046229903609963194\n",
            "train loss:0.01954527419567118\n",
            "train loss:0.026535654607342214\n",
            "train loss:0.019738187279945637\n",
            "train loss:0.055235552131785146\n",
            "train loss:0.012531349766807073\n",
            "train loss:0.019218160330023035\n",
            "train loss:0.013007351378679796\n",
            "train loss:0.006796919490043965\n",
            "train loss:0.00357416177043049\n",
            "train loss:0.01930301597915685\n",
            "train loss:0.011077473300319796\n",
            "train loss:0.020185374151911874\n",
            "train loss:0.04248853082134514\n",
            "train loss:0.03178023679605507\n",
            "train loss:0.01793840985760307\n",
            "train loss:0.05320485285050184\n",
            "train loss:0.009448229815809294\n",
            "train loss:0.020439799393309813\n",
            "train loss:0.02106124739220825\n",
            "train loss:0.017701393561033663\n",
            "train loss:0.013802849273517348\n",
            "train loss:0.02518887184455536\n",
            "train loss:0.055415824757141595\n",
            "train loss:0.03436984893483171\n",
            "train loss:0.021201100810980325\n",
            "train loss:0.018311077949410403\n",
            "train loss:0.02100315180260086\n",
            "train loss:0.028190463817819214\n",
            "train loss:0.013425984095367152\n",
            "train loss:0.06120962971373214\n",
            "train loss:0.025045797472427606\n",
            "train loss:0.02985943121232355\n",
            "train loss:0.01593059958076438\n",
            "train loss:0.021656548356487247\n",
            "train loss:0.006365611939423026\n",
            "train loss:0.020714189870470454\n",
            "train loss:0.02005343073918873\n",
            "train loss:0.013499026040542211\n",
            "train loss:0.028155739753177093\n",
            "train loss:0.03469551339150027\n",
            "train loss:0.027574016181024633\n",
            "train loss:0.008988012231996799\n",
            "train loss:0.013577676370447971\n",
            "train loss:0.01590389019446039\n",
            "train loss:0.02592881117157475\n",
            "train loss:0.022323030209940087\n",
            "train loss:0.013096665136183074\n",
            "train loss:0.013891514993263979\n",
            "train loss:0.01173602151607791\n",
            "train loss:0.028920576366272207\n",
            "train loss:0.030107070429458278\n",
            "train loss:0.0033421145101612294\n",
            "train loss:0.011018898732803522\n",
            "train loss:0.010847360707563933\n",
            "train loss:0.03430463626285393\n",
            "train loss:0.007651602901538125\n",
            "train loss:0.01322106311225661\n",
            "train loss:0.014844178183706973\n",
            "train loss:0.013164813442206243\n",
            "train loss:0.0038846357015159347\n",
            "train loss:0.019350975601789337\n",
            "train loss:0.00894566165753048\n",
            "train loss:0.0769815722823185\n",
            "train loss:0.0026403332555341237\n",
            "train loss:0.011018375867702244\n",
            "train loss:0.04582502061927161\n",
            "train loss:0.026234856388461843\n",
            "train loss:0.014394338245131179\n",
            "train loss:0.012693097596389908\n",
            "train loss:0.018703808947437194\n",
            "train loss:0.009943292990552565\n",
            "train loss:0.003814005013129857\n",
            "train loss:0.029065573412363387\n",
            "train loss:0.014993878836421504\n",
            "train loss:0.006611730387377267\n",
            "train loss:0.03058133388754412\n",
            "train loss:0.008431592022087432\n",
            "train loss:0.05313127623504549\n",
            "train loss:0.01541172585597473\n",
            "train loss:0.010661107412590673\n",
            "train loss:0.011092869724027088\n",
            "train loss:0.011895445569746135\n",
            "train loss:0.008855888726760132\n",
            "train loss:0.012759700196392403\n",
            "train loss:0.02202090882113442\n",
            "train loss:0.02028487294096497\n",
            "train loss:0.015254779399724161\n",
            "train loss:0.026689904300665184\n",
            "train loss:0.012550006355442376\n",
            "train loss:0.0067601508623228\n",
            "train loss:0.012006483563563097\n",
            "train loss:0.05025338487508527\n",
            "train loss:0.011799464975158705\n",
            "train loss:0.010228413944592153\n",
            "train loss:0.016308889556709557\n",
            "train loss:0.0028096211191365435\n",
            "train loss:0.017042652791749834\n",
            "train loss:0.07764504773463617\n",
            "train loss:0.03817621415370183\n",
            "train loss:0.0252125620789841\n",
            "train loss:0.016520830268622836\n",
            "train loss:0.04558637235686873\n",
            "train loss:0.04217240549439248\n",
            "train loss:0.020594014914817594\n",
            "train loss:0.009184955240839243\n",
            "train loss:0.04551619623413219\n",
            "train loss:0.01908939848962359\n",
            "train loss:0.04472092824947626\n",
            "train loss:0.03608163859677578\n",
            "train loss:0.005467189504968288\n",
            "train loss:0.0262263266117582\n",
            "train loss:0.042775600504908716\n",
            "train loss:0.012707841985634875\n",
            "train loss:0.02202506709501346\n",
            "train loss:0.03716134720499338\n",
            "train loss:0.029491392834817852\n",
            "train loss:0.009045714620699245\n",
            "train loss:0.008264539644611834\n",
            "train loss:0.016621116082426737\n",
            "train loss:0.029885617781600256\n",
            "train loss:0.02088843180778286\n",
            "train loss:0.011610504280544783\n",
            "train loss:0.005823037888035038\n",
            "train loss:0.03615883401865773\n",
            "train loss:0.021969106368176363\n",
            "train loss:0.007150672045905058\n",
            "train loss:0.013723041312298106\n",
            "train loss:0.015827275227131645\n",
            "train loss:0.014159479249723378\n",
            "train loss:0.021803769890690338\n",
            "train loss:0.030103938803512677\n",
            "train loss:0.015375480634206731\n",
            "train loss:0.02538544928862879\n",
            "train loss:0.01347742764097768\n",
            "train loss:0.013784723432131158\n",
            "train loss:0.009745350389716613\n",
            "train loss:0.052199853254889204\n",
            "train loss:0.009733433586315968\n",
            "train loss:0.03278521290313824\n",
            "train loss:0.04816555705270475\n",
            "train loss:0.00556590367864197\n",
            "train loss:0.012918898527729457\n",
            "train loss:0.007731438078383027\n",
            "train loss:0.0024343331666257574\n",
            "train loss:0.016017730585146103\n",
            "train loss:0.007414436840359632\n",
            "train loss:0.03159792203145456\n",
            "train loss:0.022491571050557248\n",
            "train loss:0.021623635453950137\n",
            "train loss:0.015057890162459375\n",
            "train loss:0.0045685820128790276\n",
            "train loss:0.024618427356456186\n",
            "train loss:0.015401109246881373\n",
            "train loss:0.010002304288425412\n",
            "train loss:0.0081325424956387\n",
            "train loss:0.01625318694137322\n",
            "train loss:0.0057593095518823755\n",
            "train loss:0.014491518121785379\n",
            "train loss:0.015730984827554098\n",
            "train loss:0.01085446634954129\n",
            "train loss:0.023471476327732325\n",
            "train loss:0.008769373700459585\n",
            "train loss:0.010238133424227969\n",
            "train loss:0.0031254463468163114\n",
            "train loss:0.014344242385605716\n",
            "train loss:0.04387985267372449\n",
            "train loss:0.003811613036955339\n",
            "train loss:0.01525962649670982\n",
            "train loss:0.00926521435197412\n",
            "train loss:0.011999069356216287\n",
            "train loss:0.010762337600193274\n",
            "train loss:0.012037853788597059\n",
            "train loss:0.010575978920546975\n",
            "train loss:0.011944204298717995\n",
            "train loss:0.020245001839691544\n",
            "train loss:0.015270198267030148\n",
            "train loss:0.014690523193710638\n",
            "train loss:0.009385598486080764\n",
            "train loss:0.011624081127568502\n",
            "train loss:0.013973966329517673\n",
            "train loss:0.004938388975387732\n",
            "train loss:0.04042930478349027\n",
            "train loss:0.010789407740876621\n",
            "train loss:0.01957264215235418\n",
            "train loss:0.008136764289511404\n",
            "train loss:0.0110941671578334\n",
            "train loss:0.00424033693868937\n",
            "train loss:0.030873994926541113\n",
            "train loss:0.013089552772776607\n",
            "train loss:0.008394812781823208\n",
            "train loss:0.00090218415800011\n",
            "train loss:0.03347174660745859\n",
            "train loss:0.009698220251298\n",
            "train loss:0.008670174942616887\n",
            "train loss:0.008613947049889265\n",
            "train loss:0.05321779854935995\n",
            "train loss:0.0130004426171869\n",
            "train loss:0.003863878571474311\n",
            "train loss:0.04467397921678822\n",
            "train loss:0.011365933060539447\n",
            "train loss:0.041343122436190385\n",
            "train loss:0.05051324063803118\n",
            "train loss:0.035394795344343116\n",
            "train loss:0.017450264169508303\n",
            "train loss:0.011746882057878445\n",
            "train loss:0.02763804621676329\n",
            "train loss:0.01831277296076124\n",
            "train loss:0.017950041872120487\n",
            "train loss:0.03480887748698026\n",
            "train loss:0.01137512647881269\n",
            "train loss:0.02549347232529545\n",
            "train loss:0.01650068722192367\n",
            "train loss:0.005605923812762962\n",
            "train loss:0.011117350696390128\n",
            "train loss:0.02339396416676983\n",
            "train loss:0.011321440339208157\n",
            "train loss:0.014890241609902443\n",
            "train loss:0.02230064305539182\n",
            "train loss:0.031093495043514698\n",
            "train loss:0.034203849547953954\n",
            "train loss:0.011358529031693473\n",
            "train loss:0.00969551726706606\n",
            "train loss:0.0588800228511016\n",
            "train loss:0.029049720624582664\n",
            "train loss:0.007277754639048562\n",
            "train loss:0.00546360977697652\n",
            "train loss:0.009744655395353446\n",
            "train loss:0.022520404606833563\n",
            "train loss:0.03547456060497606\n",
            "train loss:0.03440672160594832\n",
            "train loss:0.009521527198443437\n",
            "train loss:0.01504622732071617\n",
            "train loss:0.016581258706242746\n",
            "train loss:0.004971804313447107\n",
            "train loss:0.008811539321668928\n",
            "train loss:0.021926582220284187\n",
            "train loss:0.03533029785004534\n",
            "train loss:0.020572556983710345\n",
            "train loss:0.012143869443105722\n",
            "train loss:0.013036103117460406\n",
            "train loss:0.009916304555893039\n",
            "train loss:0.019808815569835398\n",
            "train loss:0.033252247796126184\n",
            "train loss:0.028962275309813816\n",
            "train loss:0.01506816807475883\n",
            "train loss:0.005782623295488756\n",
            "train loss:0.010375318232355515\n",
            "train loss:0.011289481222837088\n",
            "train loss:0.019177298342737764\n",
            "train loss:0.007638266831561836\n",
            "train loss:0.01561960915252355\n",
            "train loss:0.009569048180038911\n",
            "train loss:0.008189148514392439\n",
            "train loss:0.015627917547143225\n",
            "train loss:0.0248823178951465\n",
            "train loss:0.028174863716361364\n",
            "train loss:0.008419222115289261\n",
            "train loss:0.019069174285164883\n",
            "train loss:0.003543739321415016\n",
            "train loss:0.00712566937540001\n",
            "train loss:0.0088214905898913\n",
            "train loss:0.012941504584507707\n",
            "train loss:0.0668636521856481\n",
            "train loss:0.00702298450315559\n",
            "train loss:0.015129198548457965\n",
            "train loss:0.010448084714728703\n",
            "train loss:0.02192616557734615\n",
            "train loss:0.024240446969272036\n",
            "train loss:0.007037670873884655\n",
            "train loss:0.014316536225696643\n",
            "train loss:0.008733771765121625\n",
            "train loss:0.008107420836860516\n",
            "train loss:0.009979525191203964\n",
            "train loss:0.011608606512071195\n",
            "train loss:0.005590486540372304\n",
            "train loss:0.007370128826640709\n",
            "train loss:0.03646347367713163\n",
            "train loss:0.03861003621405445\n",
            "train loss:0.00845037365816745\n",
            "train loss:0.015389144009371055\n",
            "train loss:0.012765434034404093\n",
            "train loss:0.014393453754678463\n",
            "train loss:0.010847035483349707\n",
            "train loss:0.014725465357616934\n",
            "train loss:0.017744354514152655\n",
            "train loss:0.01869222883482865\n",
            "train loss:0.019488969487171732\n",
            "train loss:0.006114131134682375\n",
            "train loss:0.012930808770394693\n",
            "train loss:0.02050985080620224\n",
            "train loss:0.030050433063635733\n",
            "train loss:0.0530948401598141\n",
            "train loss:0.011650287806253525\n",
            "train loss:0.0451778488842907\n",
            "train loss:0.017534813298207295\n",
            "train loss:0.0029369490406047105\n",
            "train loss:0.03229307776835302\n",
            "train loss:0.004287409014259707\n",
            "train loss:0.03221827743299417\n",
            "train loss:0.01939528933063601\n",
            "train loss:0.01234038834962269\n",
            "train loss:0.0084310927766633\n",
            "train loss:0.0312570130194764\n",
            "train loss:0.008181875783506208\n",
            "train loss:0.002713099602190055\n",
            "train loss:0.012711732568509555\n",
            "train loss:0.013786226715331027\n",
            "train loss:0.0355319910883006\n",
            "train loss:0.013072921848478278\n",
            "train loss:0.04028472844941322\n",
            "train loss:0.004829639826143441\n",
            "train loss:0.019959449611449864\n",
            "train loss:0.008570083479614105\n",
            "train loss:0.02886542488202739\n",
            "train loss:0.01995920932133736\n",
            "train loss:0.020069542356150213\n",
            "train loss:0.001588074599967768\n",
            "train loss:0.01606525836991398\n",
            "train loss:0.041592775450388225\n",
            "train loss:0.03303747493168428\n",
            "train loss:0.01086478717535329\n",
            "train loss:0.06937296025407731\n",
            "train loss:0.008271205753329161\n",
            "train loss:0.022934086996825784\n",
            "train loss:0.011654854161357187\n",
            "train loss:0.02361128351304751\n",
            "train loss:0.06241940765561886\n",
            "train loss:0.0028791149949557917\n",
            "train loss:0.020200514299817295\n",
            "train loss:0.035260187386302405\n",
            "train loss:0.02641503576231381\n",
            "train loss:0.011862563849700879\n",
            "train loss:0.00701172769840213\n",
            "train loss:0.006203018203311099\n",
            "train loss:0.021284230421974873\n",
            "train loss:0.014378584734757928\n",
            "train loss:0.017449620006945415\n",
            "train loss:0.014183258838600832\n",
            "train loss:0.009889535470357475\n",
            "train loss:0.01048156061786952\n",
            "train loss:0.006087199449083206\n",
            "train loss:0.019392074433911007\n",
            "train loss:0.009775102489573848\n",
            "train loss:0.047653298195672995\n",
            "train loss:0.01713827622023644\n",
            "train loss:0.0037073343920685353\n",
            "train loss:0.006065086110891711\n",
            "train loss:0.01835902817935894\n",
            "train loss:0.03070478351720749\n",
            "train loss:0.00444908857233758\n",
            "train loss:0.028925503166678423\n",
            "train loss:0.005066561630302941\n",
            "train loss:0.04722684770884658\n",
            "train loss:0.007505569184094175\n",
            "train loss:0.005064183955273325\n",
            "train loss:0.021697911102869675\n",
            "train loss:0.02163227482110628\n",
            "train loss:0.016866915897071524\n",
            "train loss:0.009715682711476398\n",
            "train loss:0.015101238138272624\n",
            "train loss:0.10059357822941012\n",
            "train loss:0.007288569906191652\n",
            "train loss:0.018245997807283382\n",
            "train loss:0.01379054078756278\n",
            "train loss:0.009131306372792455\n",
            "train loss:0.02711930947887628\n",
            "train loss:0.008525980891711802\n",
            "train loss:0.02204855973616645\n",
            "train loss:0.02014780965480529\n",
            "train loss:0.02973077702251018\n",
            "train loss:0.026294087948262664\n",
            "train loss:0.010790940561865539\n",
            "train loss:0.02259585876523895\n",
            "train loss:0.006478081213299092\n",
            "train loss:0.024117437136825313\n",
            "train loss:0.013172960085245544\n",
            "train loss:0.03416014848398459\n",
            "train loss:0.032441887589518596\n",
            "train loss:0.00578684685506221\n",
            "train loss:0.010100145952010552\n",
            "train loss:0.014233219484285629\n",
            "train loss:0.007944524591186513\n",
            "train loss:0.015183918571991261\n",
            "train loss:0.12298267239660453\n",
            "train loss:0.028903144196265797\n",
            "train loss:0.010685235252753788\n",
            "train loss:0.07194913013839824\n",
            "train loss:0.022144984049388573\n",
            "train loss:0.01864786766804613\n",
            "train loss:0.06108463639067286\n",
            "train loss:0.03347377985056358\n",
            "train loss:0.026056378564092108\n",
            "train loss:0.01870045917164526\n",
            "train loss:0.01048517883513926\n",
            "train loss:0.008552903060872152\n",
            "train loss:0.005179969837668544\n",
            "train loss:0.02729049213183936\n",
            "train loss:0.003219015625357079\n",
            "train loss:0.009882155943396847\n",
            "train loss:0.021245131634867458\n",
            "train loss:0.013107207331236515\n",
            "train loss:0.026347338847419587\n",
            "train loss:0.04625044416589552\n",
            "train loss:0.006122845873517568\n",
            "train loss:0.007944005158126183\n",
            "train loss:0.011168913489241223\n",
            "train loss:0.0036426359088359305\n",
            "train loss:0.015088321180810447\n",
            "train loss:0.01717060850364694\n",
            "train loss:0.011663557552549324\n",
            "train loss:0.011870334061790815\n",
            "train loss:0.0547222268703829\n",
            "train loss:0.008520454500399656\n",
            "train loss:0.02417937632977073\n",
            "train loss:0.011203911992951808\n",
            "train loss:0.02329524398586556\n",
            "train loss:0.005141023370945782\n",
            "train loss:0.005236972519486499\n",
            "train loss:0.007845513911029743\n",
            "train loss:0.01022633015106975\n",
            "train loss:0.012276083904483457\n",
            "train loss:0.02517804057875374\n",
            "train loss:0.018700206582083465\n",
            "train loss:0.014250088471216848\n",
            "train loss:0.018825401965986734\n",
            "train loss:0.022738848245614292\n",
            "train loss:0.013577532188402153\n",
            "train loss:0.01694238582703455\n",
            "train loss:0.014048862033859457\n",
            "train loss:0.009422260634487633\n",
            "train loss:0.005608299917146996\n",
            "train loss:0.00441675928510932\n",
            "train loss:0.02607175326598137\n",
            "train loss:0.00598896081717638\n",
            "train loss:0.0064162860604299096\n",
            "train loss:0.010209948070067223\n",
            "train loss:0.009848823284763159\n",
            "train loss:0.011564142931967286\n",
            "train loss:0.007827569251156572\n",
            "train loss:0.04655164632321208\n",
            "train loss:0.020210245306478292\n",
            "train loss:0.015345807819081382\n",
            "train loss:0.011506061246811304\n",
            "train loss:0.006170017167626417\n",
            "train loss:0.03188549451602109\n",
            "train loss:0.035380149245896365\n",
            "train loss:0.010349744347663357\n",
            "train loss:0.015314625694458931\n",
            "train loss:0.004104432480473105\n",
            "train loss:0.037778880434031624\n",
            "train loss:0.008066981204778206\n",
            "train loss:0.028460182303259612\n",
            "train loss:0.01615630515308254\n",
            "train loss:0.012342221955043373\n",
            "train loss:0.010619104292828847\n",
            "train loss:0.022090520484204824\n",
            "train loss:0.028344438382143573\n",
            "train loss:0.006839817598210347\n",
            "train loss:0.02159273788254958\n",
            "train loss:0.0070211149703732385\n",
            "train loss:0.015405880953905018\n",
            "train loss:0.011767376317031791\n",
            "train loss:0.013301794237571931\n",
            "train loss:0.00970093258944788\n",
            "train loss:0.02505315753765351\n",
            "train loss:0.04037078755592498\n",
            "train loss:0.010904205148716468\n",
            "train loss:0.010056162582443184\n",
            "train loss:0.07810016742321861\n",
            "train loss:0.027772653405396888\n",
            "train loss:0.020140650290863565\n",
            "train loss:0.012008518839408712\n",
            "train loss:0.013061077100452347\n",
            "train loss:0.06444106998330114\n",
            "train loss:0.04172249774909935\n",
            "train loss:0.009917073497284027\n",
            "train loss:0.028724253953010922\n",
            "train loss:0.017974948257855186\n",
            "train loss:0.009151316658754002\n",
            "train loss:0.0153791370619771\n",
            "train loss:0.006236342524637653\n",
            "train loss:0.007666584719578191\n",
            "train loss:0.04578796181001983\n",
            "train loss:0.029279282409003026\n",
            "train loss:0.031892413879425895\n",
            "train loss:0.016648538791635966\n",
            "train loss:0.006399902192817336\n",
            "train loss:0.020398200156938526\n",
            "train loss:0.011307037028331206\n",
            "train loss:0.0066979724710379595\n",
            "train loss:0.011098577582403384\n",
            "train loss:0.0438038361943557\n",
            "train loss:0.009479672878994077\n",
            "train loss:0.028350386619560117\n",
            "train loss:0.006065597790753464\n",
            "train loss:0.015131464231398335\n",
            "train loss:0.010038518742864924\n",
            "train loss:0.008821429189168704\n",
            "train loss:0.011829594852511068\n",
            "train loss:0.019815612117325514\n",
            "train loss:0.025696174038342356\n",
            "train loss:0.012078054793871296\n",
            "train loss:0.01221020530772454\n",
            "train loss:0.012118791654482455\n",
            "train loss:0.009547255602233105\n",
            "train loss:0.011819204203305284\n",
            "train loss:0.012593682039825558\n",
            "train loss:0.01062792191859406\n",
            "train loss:0.021161304348644737\n",
            "train loss:0.02018735773407673\n",
            "train loss:0.04740912172208525\n",
            "train loss:0.017549503178604983\n",
            "train loss:0.037055013973285686\n",
            "train loss:0.010767396070071125\n",
            "train loss:0.0076609573227993215\n",
            "train loss:0.014229805180124639\n",
            "train loss:0.00846183917478024\n",
            "train loss:0.006520019662274784\n",
            "train loss:0.016177296482265555\n",
            "train loss:0.021464419322840548\n",
            "train loss:0.015801928686953914\n",
            "train loss:0.02074970577063042\n",
            "train loss:0.012903905970160778\n",
            "train loss:0.009020045102185435\n",
            "train loss:0.006035246813464608\n",
            "train loss:0.00464344997187629\n",
            "train loss:0.015296447253325676\n",
            "train loss:0.004574335338257388\n",
            "train loss:0.021615455300464995\n",
            "train loss:0.033457126021243615\n",
            "train loss:0.003203738355354458\n",
            "train loss:0.022969893495011568\n",
            "train loss:0.035514896367457534\n",
            "train loss:0.013127581146515476\n",
            "train loss:0.09388201788654424\n",
            "train loss:0.012934264430761809\n",
            "train loss:0.013846762297962884\n",
            "train loss:0.007192052731669725\n",
            "train loss:0.011307112527472091\n",
            "train loss:0.016780591385513174\n",
            "train loss:0.03830719651430961\n",
            "train loss:0.010454352870655095\n",
            "train loss:0.009075087792936764\n",
            "train loss:0.026545762048683873\n",
            "train loss:0.010261244894702789\n",
            "train loss:0.014703427031450967\n",
            "train loss:0.02301787372102334\n",
            "train loss:0.009551886791220366\n",
            "train loss:0.0039754339169152915\n",
            "train loss:0.03241996249158083\n",
            "train loss:0.024008486629883686\n",
            "train loss:0.006379400681888639\n",
            "train loss:0.006994246640959462\n",
            "train loss:0.02109717053336448\n",
            "train loss:0.004021107528743444\n",
            "train loss:0.05047323910415297\n",
            "train loss:0.007373205425956832\n",
            "train loss:0.016452735067881714\n",
            "train loss:0.011418854910231025\n",
            "train loss:0.007800142494901254\n",
            "train loss:0.012851625671527468\n",
            "train loss:0.012859995409423014\n",
            "train loss:0.011097824642747563\n",
            "train loss:0.00709750775477498\n",
            "train loss:0.007905900585123738\n",
            "train loss:0.00900766641277726\n",
            "train loss:0.0052344012690476805\n",
            "train loss:0.009525763905541678\n",
            "train loss:0.021802208831372724\n",
            "train loss:0.017089909187453903\n",
            "train loss:0.015753802695306764\n",
            "train loss:0.013703402977741223\n",
            "train loss:0.018868218279281868\n",
            "train loss:0.024726730233294497\n",
            "train loss:0.017443590319802096\n",
            "train loss:0.05015591279463364\n",
            "train loss:0.0242989658542188\n",
            "train loss:0.04089487235294348\n",
            "train loss:0.06993174724873985\n",
            "train loss:0.011976776827491953\n",
            "train loss:0.013258865072059542\n",
            "train loss:0.010332093425440054\n",
            "train loss:0.03228062844735825\n",
            "train loss:0.008518010417892513\n",
            "train loss:0.009020775935294005\n",
            "=== epoch:20, train acc:0.994, test acc:0.906 ===\n",
            "train loss:0.04083390478658367\n",
            "train loss:0.023945969877619006\n",
            "train loss:0.005522312939895039\n",
            "train loss:0.005943174406038877\n",
            "train loss:0.017929460459619823\n",
            "train loss:0.006327930631737697\n",
            "train loss:0.014454738996940709\n",
            "train loss:0.03965037220592599\n",
            "train loss:0.006027764494870553\n",
            "train loss:0.01264354845842853\n",
            "train loss:0.007190004645413991\n",
            "train loss:0.01743505976262811\n",
            "train loss:0.007263700126231941\n",
            "train loss:0.020969541273984045\n",
            "train loss:0.023250750122602724\n",
            "train loss:0.009483884639428503\n",
            "train loss:0.017801560280730634\n",
            "train loss:0.003298912677021077\n",
            "train loss:0.03509297229757094\n",
            "train loss:0.007692184033802158\n",
            "train loss:0.02782815691921378\n",
            "train loss:0.0075289361101855135\n",
            "train loss:0.015918375057719123\n",
            "train loss:0.026283534029873313\n",
            "train loss:0.010121625513735282\n",
            "train loss:0.0038514866473140404\n",
            "train loss:0.04599700520371111\n",
            "train loss:0.012441002182509913\n",
            "train loss:0.022067863475603914\n",
            "train loss:0.033771538968253634\n",
            "train loss:0.011515846636784928\n",
            "train loss:0.016536357659603925\n",
            "train loss:0.01907202726514585\n",
            "train loss:0.02405761063447641\n",
            "train loss:0.014534673050490408\n",
            "train loss:0.028810517649801037\n",
            "train loss:0.05682101540471827\n",
            "train loss:0.01131503288655391\n",
            "train loss:0.01963094887374193\n",
            "train loss:0.025199003440122224\n",
            "train loss:0.009126900671221565\n",
            "train loss:0.033364077323226514\n",
            "train loss:0.025093027926335325\n",
            "train loss:0.012310569970647525\n",
            "train loss:0.011049021515252165\n",
            "train loss:0.01160161008019069\n",
            "train loss:0.013567877414695347\n",
            "train loss:0.01168655151181756\n",
            "train loss:0.004396983061158773\n",
            "train loss:0.012614010354052128\n",
            "train loss:0.024115253215210544\n",
            "train loss:0.0655421660685585\n",
            "train loss:0.0384295574448988\n",
            "train loss:0.007240315743964225\n",
            "train loss:0.014775047710203194\n",
            "train loss:0.015347902112920595\n",
            "train loss:0.016955940733839835\n",
            "train loss:0.023403205761863593\n",
            "train loss:0.01892919943854556\n",
            "train loss:0.006879279494978143\n",
            "train loss:0.01675960252985897\n",
            "train loss:0.012672071647065273\n",
            "train loss:0.016210076462355796\n",
            "train loss:0.045232834730237176\n",
            "train loss:0.01990432259679451\n",
            "train loss:0.02721818639303442\n",
            "train loss:0.023608944954964416\n",
            "train loss:0.008389500026711485\n",
            "train loss:0.028385056183959513\n",
            "train loss:0.004195304042478491\n",
            "train loss:0.01175926400927751\n",
            "train loss:0.04939668363785091\n",
            "train loss:0.020472516539624293\n",
            "train loss:0.016183115298703813\n",
            "train loss:0.008298012333310237\n",
            "train loss:0.01381325857565759\n",
            "train loss:0.03596388803456893\n",
            "train loss:0.044380676483693586\n",
            "train loss:0.022953881448235916\n",
            "train loss:0.02087683760275028\n",
            "train loss:0.022617633914036067\n",
            "train loss:0.017438053960609184\n",
            "train loss:0.013943598027822378\n",
            "train loss:0.04276648390244678\n",
            "train loss:0.020504953471339474\n",
            "train loss:0.010729230326061347\n",
            "train loss:0.011887885527236417\n",
            "train loss:0.027143113166602464\n",
            "train loss:0.01712957852981697\n",
            "train loss:0.05561643026395904\n",
            "train loss:0.01723927581836525\n",
            "train loss:0.028058034972420834\n",
            "train loss:0.026957838848638612\n",
            "train loss:0.012792333038428845\n",
            "train loss:0.039129655764166174\n",
            "train loss:0.006847942618456594\n",
            "train loss:0.033698802803811365\n",
            "train loss:0.02469776231605228\n",
            "train loss:0.011935040783085432\n",
            "train loss:0.010815737937934784\n",
            "train loss:0.006021588211436937\n",
            "train loss:0.010933500943499068\n",
            "train loss:0.09259349855590764\n",
            "train loss:0.02646868655424389\n",
            "train loss:0.05423757992757681\n",
            "train loss:0.013641218154313762\n",
            "train loss:0.019964073673249677\n",
            "train loss:0.04880656117326826\n",
            "train loss:0.009769619570766623\n",
            "train loss:0.00616994866637642\n",
            "train loss:0.013066988875745844\n",
            "train loss:0.03582393929854606\n",
            "train loss:0.015362107281847406\n",
            "train loss:0.018713593219220177\n",
            "train loss:0.024105090369834315\n",
            "train loss:0.012934459173154323\n",
            "train loss:0.00868729503346771\n",
            "train loss:0.008636756919883021\n",
            "train loss:0.018342964385291708\n",
            "train loss:0.01598330524771657\n",
            "train loss:0.00842787929461547\n",
            "train loss:0.002100141373659434\n",
            "train loss:0.012534241886880302\n",
            "train loss:0.00891080932189583\n",
            "train loss:0.015738626669258963\n",
            "train loss:0.023788077052199673\n",
            "train loss:0.016080171892478897\n",
            "train loss:0.01518583809344023\n",
            "train loss:0.010632340748096198\n",
            "train loss:0.007354105927911404\n",
            "train loss:0.011560118874717622\n",
            "train loss:0.015339581469347332\n",
            "train loss:0.007206389641866534\n",
            "train loss:0.00738600462070241\n",
            "train loss:0.013094575545641191\n",
            "train loss:0.04037225380061242\n",
            "train loss:0.007196564309160582\n",
            "train loss:0.022792602558801386\n",
            "train loss:0.0043483527569380806\n",
            "train loss:0.021244600905310725\n",
            "train loss:0.020534168812926235\n",
            "train loss:0.005740051887161989\n",
            "train loss:0.015361384290508467\n",
            "train loss:0.018402490376152315\n",
            "train loss:0.004895735052938294\n",
            "train loss:0.00937471509291571\n",
            "train loss:0.01459211297038532\n",
            "train loss:0.006909595459298892\n",
            "train loss:0.007542584667937625\n",
            "train loss:0.010752041281996019\n",
            "train loss:0.005713245560565091\n",
            "train loss:0.02632497491250039\n",
            "train loss:0.0033761376137037167\n",
            "train loss:0.024929241961007997\n",
            "train loss:0.007969808721277373\n",
            "train loss:0.006046288118374114\n",
            "train loss:0.006116407146499858\n",
            "train loss:0.00885575478492033\n",
            "train loss:0.006409985406452981\n",
            "train loss:0.0017712245774448355\n",
            "train loss:0.013871673068862162\n",
            "train loss:0.003916436442715963\n",
            "train loss:0.01525726596569093\n",
            "train loss:0.08003221893487415\n",
            "train loss:0.012751227187026131\n",
            "train loss:0.012698034943508857\n",
            "train loss:0.03226405279474077\n",
            "train loss:0.01107479779384129\n",
            "train loss:0.021042223445629407\n",
            "train loss:0.01329322073565945\n",
            "train loss:0.03418192352849935\n",
            "train loss:0.005105104787698703\n",
            "train loss:0.004160378962962546\n",
            "train loss:0.011935878789605236\n",
            "train loss:0.003534792221096168\n",
            "train loss:0.006550872100230114\n",
            "train loss:0.021964288969324924\n",
            "train loss:0.004391062374016326\n",
            "train loss:0.0074226590162640525\n",
            "train loss:0.004779559728690288\n",
            "train loss:0.01898578970050461\n",
            "train loss:0.010166845655379834\n",
            "train loss:0.015375480925455192\n",
            "train loss:0.005227405056285827\n",
            "train loss:0.0053496711207911855\n",
            "train loss:0.0008044913024134917\n",
            "train loss:0.023832538957529698\n",
            "train loss:0.008034616904241808\n",
            "train loss:0.008269431891993518\n",
            "train loss:0.011990847616417964\n",
            "train loss:0.012521440497046123\n",
            "train loss:0.008929802750104278\n",
            "train loss:0.00562502169476547\n",
            "train loss:0.005783338121897962\n",
            "train loss:0.015492560646651126\n",
            "train loss:0.0037781254984230405\n",
            "train loss:0.013771456102423266\n",
            "train loss:0.010887440174635605\n",
            "train loss:0.010623783362681896\n",
            "train loss:0.019801677350618038\n",
            "train loss:0.012026122296237483\n",
            "train loss:0.00978651964242539\n",
            "train loss:0.013749934787971221\n",
            "train loss:0.0026296006875023313\n",
            "train loss:0.0041656335283480625\n",
            "train loss:0.014170369932370298\n",
            "train loss:0.00868376271751553\n",
            "train loss:0.009689351253128453\n",
            "train loss:0.029647272900629348\n",
            "train loss:0.02752115674720817\n",
            "train loss:0.005452724467195192\n",
            "train loss:0.04399516923255017\n",
            "train loss:0.005109039936322145\n",
            "train loss:0.015110758486297527\n",
            "train loss:0.01572822294258428\n",
            "train loss:0.011398181548888425\n",
            "train loss:0.01973403209874019\n",
            "train loss:0.039519662860312284\n",
            "train loss:0.015376399499985727\n",
            "train loss:0.006306342277785966\n",
            "train loss:0.014303861871115004\n",
            "train loss:0.05750515303399947\n",
            "train loss:0.005243263011606298\n",
            "train loss:0.013683629829127917\n",
            "train loss:0.008140006133729916\n",
            "train loss:0.00897847056309706\n",
            "train loss:0.02452923341941333\n",
            "train loss:0.011184460114513182\n",
            "train loss:0.012801454725602745\n",
            "train loss:0.01765802466061091\n",
            "train loss:0.049670769649101346\n",
            "train loss:0.004949591021656888\n",
            "train loss:0.02039832096534633\n",
            "train loss:0.011564306899064411\n",
            "train loss:0.004569752109679707\n",
            "train loss:0.007641068426906463\n",
            "train loss:0.007704478882629084\n",
            "train loss:0.009909126389514003\n",
            "train loss:0.03958727953884377\n",
            "train loss:0.015314618952365704\n",
            "train loss:0.004754045478650512\n",
            "train loss:0.020010571329377447\n",
            "train loss:0.0051189753248514925\n",
            "train loss:0.0382828780984191\n",
            "train loss:0.01866741427014821\n",
            "train loss:0.01133088153703586\n",
            "train loss:0.012920706969779684\n",
            "train loss:0.004055176153967452\n",
            "train loss:0.01103646335758574\n",
            "train loss:0.005731623678830377\n",
            "train loss:0.008201801837784062\n",
            "train loss:0.04652054063970218\n",
            "train loss:0.04258897686106509\n",
            "train loss:0.01658072482079666\n",
            "train loss:0.052133626655814516\n",
            "train loss:0.009572307905034452\n",
            "train loss:0.007444517483536105\n",
            "train loss:0.007560246896739804\n",
            "train loss:0.0014636221803690803\n",
            "train loss:0.012893926924809491\n",
            "train loss:0.006682829390868725\n",
            "train loss:0.01738393893693597\n",
            "train loss:0.01038199222346404\n",
            "train loss:0.028491918101406232\n",
            "train loss:0.017047097707372873\n",
            "train loss:0.024218657230840948\n",
            "train loss:0.001269106508193492\n",
            "train loss:0.010252203824493147\n",
            "train loss:0.015030916954209479\n",
            "train loss:0.005789801288280731\n",
            "train loss:0.021083241889881008\n",
            "train loss:0.011356131725105956\n",
            "train loss:0.011011340289324892\n",
            "train loss:0.04145628154630482\n",
            "train loss:0.006157642562798728\n",
            "train loss:0.007744794700146834\n",
            "train loss:0.009312957313410374\n",
            "train loss:0.004037815067061187\n",
            "train loss:0.008010848720319182\n",
            "train loss:0.01221735130645175\n",
            "train loss:0.013642984430356067\n",
            "train loss:0.010169719188101754\n",
            "train loss:0.05128988529776326\n",
            "train loss:0.00940584432981797\n",
            "train loss:0.02574961785562209\n",
            "train loss:0.04088479909184943\n",
            "train loss:0.017068490247789142\n",
            "train loss:0.0077396134518338735\n",
            "train loss:0.024933890036723815\n",
            "train loss:0.007152190047126283\n",
            "train loss:0.014278064187989388\n",
            "train loss:0.038020432704871245\n",
            "train loss:0.022990250783390755\n",
            "train loss:0.00994072375490227\n",
            "train loss:0.012673528804129326\n",
            "train loss:0.020745407133731818\n",
            "train loss:0.03443863706939603\n",
            "train loss:0.009044332809248249\n",
            "train loss:0.01802692761673418\n",
            "train loss:0.014819190067079985\n",
            "train loss:0.008169039859031899\n",
            "train loss:0.01758790183440412\n",
            "train loss:0.02753368103034405\n",
            "train loss:0.01645095223647541\n",
            "train loss:0.0019844074477050997\n",
            "train loss:0.012187619405241\n",
            "train loss:0.015916463311223487\n",
            "train loss:0.03263985753997835\n",
            "train loss:0.018092037436203887\n",
            "train loss:0.015652615522458196\n",
            "train loss:0.013543800376902978\n",
            "train loss:0.009011097922044046\n",
            "train loss:0.017754701429306265\n",
            "train loss:0.010076710293254658\n",
            "train loss:0.014294728920142125\n",
            "train loss:0.026230852500623035\n",
            "train loss:0.012879570811590269\n",
            "train loss:0.01807874881590093\n",
            "train loss:0.012165781045282735\n",
            "train loss:0.007944489224677588\n",
            "train loss:0.03022875923879114\n",
            "train loss:0.007879430986284961\n",
            "train loss:0.003075044654110707\n",
            "train loss:0.01916446323708251\n",
            "train loss:0.011419389316283987\n",
            "train loss:0.01175686470076869\n",
            "train loss:0.059603469125154934\n",
            "train loss:0.012358619562955005\n",
            "train loss:0.0200526727298588\n",
            "train loss:0.04754109818110822\n",
            "train loss:0.006106536642051554\n",
            "train loss:0.04056013255435242\n",
            "train loss:0.018540355301613097\n",
            "train loss:0.012947976270313395\n",
            "train loss:0.012310404468672466\n",
            "train loss:0.014827813809781449\n",
            "train loss:0.027775574660687696\n",
            "train loss:0.006046922482286686\n",
            "train loss:0.013448558691275174\n",
            "train loss:0.01854562477620382\n",
            "train loss:0.0344737977683922\n",
            "train loss:0.004812781098001831\n",
            "train loss:0.04201282718109727\n",
            "train loss:0.01363462078906371\n",
            "train loss:0.01664707539143731\n",
            "train loss:0.0199158303199908\n",
            "train loss:0.021287549926038457\n",
            "train loss:0.0048653164391795735\n",
            "train loss:0.008015133137990271\n",
            "train loss:0.017297590035683274\n",
            "train loss:0.025099514902655443\n",
            "train loss:0.015334684845908069\n",
            "train loss:0.024144078724560685\n",
            "train loss:0.020940202499263018\n",
            "train loss:0.015666897640427294\n",
            "train loss:0.011599352633190386\n",
            "train loss:0.005811241641997889\n",
            "train loss:0.008003024394151008\n",
            "train loss:0.012575163491103793\n",
            "train loss:0.011591585378999705\n",
            "train loss:0.014500073136166114\n",
            "train loss:0.006004093564554929\n",
            "train loss:0.009552479963606477\n",
            "train loss:0.027310595234145217\n",
            "train loss:0.009794295023179364\n",
            "train loss:0.0024520253025068677\n",
            "train loss:0.004186572688866349\n",
            "train loss:0.009593855983726144\n",
            "train loss:0.023547084217591187\n",
            "train loss:0.008401359012877389\n",
            "train loss:0.003310236238167286\n",
            "train loss:0.003641640301024687\n",
            "train loss:0.02064360072970564\n",
            "train loss:0.03839330144827062\n",
            "train loss:0.003490426046124619\n",
            "train loss:0.028450306489950575\n",
            "train loss:0.017126245523042427\n",
            "train loss:0.05521776796222696\n",
            "train loss:0.015223262434536686\n",
            "train loss:0.01858154383693124\n",
            "train loss:0.039504464440461386\n",
            "train loss:0.014936463678053749\n",
            "train loss:0.01499094796111671\n",
            "train loss:0.02008865554826183\n",
            "train loss:0.014622280064952514\n",
            "train loss:0.024192712649501843\n",
            "train loss:0.014693141775550906\n",
            "train loss:0.00816794952841165\n",
            "train loss:0.010434480780900082\n",
            "train loss:0.018425446298046298\n",
            "train loss:0.01874187972094661\n",
            "train loss:0.014974583305202842\n",
            "train loss:0.020894764877501845\n",
            "train loss:0.039003172358184963\n",
            "train loss:0.020848742838610795\n",
            "train loss:0.009588524167484099\n",
            "train loss:0.05773221007729133\n",
            "train loss:0.029813413613994925\n",
            "train loss:0.03377950026193665\n",
            "train loss:0.011168972204501757\n",
            "train loss:0.013715599795620405\n",
            "train loss:0.012216518638485989\n",
            "train loss:0.012214382002068798\n",
            "train loss:0.02088369768862068\n",
            "train loss:0.023069934129199155\n",
            "train loss:0.013970287713068331\n",
            "train loss:0.02532323244015286\n",
            "train loss:0.006437149107116621\n",
            "train loss:0.062315327854186406\n",
            "train loss:0.01742954156560566\n",
            "train loss:0.01577639230926146\n",
            "train loss:0.012695414259048925\n",
            "train loss:0.03111318104329821\n",
            "train loss:0.027465875329403117\n",
            "train loss:0.0031021942875346657\n",
            "train loss:0.03666430520701571\n",
            "train loss:0.04186143089144181\n",
            "train loss:0.017660205346240108\n",
            "train loss:0.014532943632063384\n",
            "train loss:0.010955147079861569\n",
            "train loss:0.021606425310664746\n",
            "train loss:0.005006533492127688\n",
            "train loss:0.020547490982291253\n",
            "train loss:0.006859277126407798\n",
            "train loss:0.019506383250482158\n",
            "train loss:0.010688141543137823\n",
            "train loss:0.057973392169515775\n",
            "train loss:0.02077933595991274\n",
            "train loss:0.023316190869720633\n",
            "train loss:0.02182819422236179\n",
            "train loss:0.010232186393256792\n",
            "train loss:0.023673266576037813\n",
            "train loss:0.007985266779891604\n",
            "train loss:0.010592825749069296\n",
            "train loss:0.011329350644773046\n",
            "train loss:0.008916757374412793\n",
            "train loss:0.011464497314147806\n",
            "train loss:0.005682767664964669\n",
            "train loss:0.006647476364620239\n",
            "train loss:0.014579537917917995\n",
            "train loss:0.04325655865260955\n",
            "train loss:0.014948277002549828\n",
            "train loss:0.005155892434849751\n",
            "train loss:0.024491234665037595\n",
            "train loss:0.003958876062663292\n",
            "train loss:0.006823412944371032\n",
            "train loss:0.005252335458452172\n",
            "train loss:0.021541187233822784\n",
            "train loss:0.003727025418566568\n",
            "train loss:0.014502120933923756\n",
            "train loss:0.011655650958157536\n",
            "train loss:0.025955033293362236\n",
            "train loss:0.008625352051257543\n",
            "train loss:0.016618073307932427\n",
            "train loss:0.019878653453872193\n",
            "train loss:0.04450648171922438\n",
            "train loss:0.007385751476491402\n",
            "train loss:0.013138953862714377\n",
            "train loss:0.015757131877087948\n",
            "train loss:0.007573224231873038\n",
            "train loss:0.052105722884485076\n",
            "train loss:0.043579857527238385\n",
            "train loss:0.0076807937316962106\n",
            "train loss:0.019350892126143614\n",
            "train loss:0.0020956009620930805\n",
            "train loss:0.010383049443617353\n",
            "train loss:0.007292270586294283\n",
            "train loss:0.007768320674587582\n",
            "train loss:0.008008847478258278\n",
            "train loss:0.012586018514132678\n",
            "train loss:0.010159521008716537\n",
            "train loss:0.02475751020321984\n",
            "train loss:0.011899474183804265\n",
            "train loss:0.0069090291512357615\n",
            "train loss:0.0070142289461681264\n",
            "train loss:0.014006149295376817\n",
            "train loss:0.016722797176268626\n",
            "train loss:0.024791224224224206\n",
            "train loss:0.0022296633864275135\n",
            "train loss:0.005886082956289066\n",
            "train loss:0.010105119034689598\n",
            "train loss:0.011147223594938149\n",
            "train loss:0.009943343106980246\n",
            "train loss:0.025854647654396993\n",
            "train loss:0.014169941020026604\n",
            "train loss:0.032419906524412075\n",
            "train loss:0.008956207634293067\n",
            "train loss:0.00624104596118974\n",
            "train loss:0.022859107939246982\n",
            "train loss:0.012386352285122455\n",
            "train loss:0.005881205327301884\n",
            "train loss:0.006786831737873643\n",
            "train loss:0.011227967049200543\n",
            "train loss:0.012242402201979322\n",
            "train loss:0.006574340522389353\n",
            "train loss:0.02627967320389637\n",
            "train loss:0.014360367392605062\n",
            "train loss:0.015894198640392373\n",
            "train loss:0.008672262083107563\n",
            "train loss:0.005695646361009299\n",
            "train loss:0.010486593885803967\n",
            "train loss:0.03077632238253629\n",
            "train loss:0.04270589347666899\n",
            "train loss:0.00874625880174938\n",
            "train loss:0.02964657885914011\n",
            "train loss:0.017548614460343035\n",
            "train loss:0.02767127818746397\n",
            "train loss:0.007211457377349518\n",
            "train loss:0.033706801543785775\n",
            "train loss:0.0117139025359117\n",
            "train loss:0.01056631236038737\n",
            "train loss:0.00628021259968839\n",
            "train loss:0.017732450379449895\n",
            "train loss:0.006809922519082694\n",
            "train loss:0.007427600832911142\n",
            "train loss:0.006828330776178303\n",
            "train loss:0.017026950771531657\n",
            "train loss:0.00822479469479398\n",
            "train loss:0.006581044327919397\n",
            "train loss:0.006203300171119103\n",
            "train loss:0.009962822409487553\n",
            "train loss:0.01945206116748391\n",
            "train loss:0.016035924588374436\n",
            "train loss:0.012211545319827852\n",
            "train loss:0.008925304003954837\n",
            "train loss:0.00296551127764066\n",
            "train loss:0.01211957967423742\n",
            "train loss:0.01223920479202505\n",
            "train loss:0.007047019673218916\n",
            "train loss:0.02451486225056157\n",
            "train loss:0.00664465532981233\n",
            "train loss:0.005828746964914549\n",
            "train loss:0.010361678638975094\n",
            "train loss:0.007572603529720678\n",
            "train loss:0.007577567010649453\n",
            "train loss:0.026797370447332845\n",
            "train loss:0.0059172238212409165\n",
            "train loss:0.011755638561359132\n",
            "train loss:0.00431389721142405\n",
            "train loss:0.0095896859890201\n",
            "train loss:0.00784172151141375\n",
            "train loss:0.00570118616694604\n",
            "train loss:0.004070068379550518\n",
            "train loss:0.0034539239746446504\n",
            "train loss:0.003121535936422105\n",
            "train loss:0.004356106735890945\n",
            "train loss:0.01068318220106824\n",
            "train loss:0.002622761302879948\n",
            "train loss:0.03339517891415433\n",
            "train loss:0.006973988458028245\n",
            "train loss:0.008508061348274434\n",
            "train loss:0.006431949219242669\n",
            "train loss:0.013884289298537002\n",
            "train loss:0.010291597171689714\n",
            "train loss:0.0045812898056221\n",
            "train loss:0.019437235304842836\n",
            "train loss:0.006565376191968727\n",
            "train loss:0.0034737676863807005\n",
            "train loss:0.0120330794252071\n",
            "train loss:0.017872502738598556\n",
            "train loss:0.020096622690612184\n",
            "train loss:0.003709966617058581\n",
            "train loss:0.00776437399705494\n",
            "train loss:0.013535958588916229\n",
            "train loss:0.006348230414242956\n",
            "train loss:0.01015091363724489\n",
            "train loss:0.008347054253019865\n",
            "train loss:0.019821052261279223\n",
            "train loss:0.024182507484070963\n",
            "train loss:0.016004021824392856\n",
            "train loss:0.009611107448847184\n",
            "train loss:0.007684637666504036\n",
            "train loss:0.002848554961916422\n",
            "train loss:0.03262700910466925\n",
            "train loss:0.006208677989177141\n",
            "train loss:0.00985059245584507\n",
            "train loss:0.005953675332115776\n",
            "train loss:0.018046272829742357\n",
            "train loss:0.0053268730023851175\n",
            "train loss:0.014261011833776358\n",
            "train loss:0.03188232817863132\n",
            "train loss:0.005940067953966618\n",
            "train loss:0.005018965916122086\n",
            "train loss:0.02574634109546451\n",
            "train loss:0.008052041393714685\n",
            "train loss:0.030519748738452427\n",
            "train loss:0.033690271234598466\n",
            "train loss:0.015385686904953509\n",
            "train loss:0.012357222375231607\n",
            "train loss:0.01236887717712003\n",
            "train loss:0.012479890067335712\n",
            "train loss:0.008079149730825592\n",
            "train loss:0.008756070254614685\n",
            "train loss:0.0048500701594449865\n",
            "train loss:0.010394038417473486\n",
            "train loss:0.022304446509257357\n",
            "train loss:0.027534633000161745\n",
            "train loss:0.008958840565058896\n",
            "train loss:0.005729022569766751\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9027\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}